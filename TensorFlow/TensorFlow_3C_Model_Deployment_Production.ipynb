{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìô FILE 3-C: MODEL DEPLOYMENT & PRODUCTION\n",
    "\n",
    "**Ph·∫ßn:** ADVANCED & PROFESSIONAL (Production-Ready) - FINAL\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- ‚úÖ Inference Pipeline chuy√™n nghi·ªáp\n",
    "- ‚úÖ Save & Load Models ƒë√∫ng c√°ch\n",
    "- ‚úÖ Export SavedModel format\n",
    "- ‚úÖ Performance Optimization\n",
    "- ‚úÖ Production Best Practices\n",
    "- ‚úÖ Common Anti-patterns\n",
    "\n",
    "**Th·ªùi l∆∞·ª£ng:** 2-3 tu·∫ßn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö M·ª•c L·ª•c\n",
    "\n",
    "### PH·∫¶N 1: SAVE & LOAD MODELS\n",
    "1. Model Formats trong TensorFlow\n",
    "2. Keras Format (.keras)\n",
    "3. SavedModel Format\n",
    "4. Checkpoints\n",
    "5. Weights Only\n",
    "6. Best Practices\n",
    "\n",
    "### PH·∫¶N 2: INFERENCE PIPELINE\n",
    "1. Inference Pipeline l√† g√¨?\n",
    "2. Preprocessing for Inference\n",
    "3. Batch Inference\n",
    "4. Real-time Inference\n",
    "5. Post-processing\n",
    "6. Error Handling\n",
    "\n",
    "### PH·∫¶N 3: PERFORMANCE OPTIMIZATION\n",
    "1. Model Optimization Techniques\n",
    "2. Quantization\n",
    "3. Pruning\n",
    "4. TensorFlow Lite\n",
    "5. ONNX Export\n",
    "\n",
    "### PH·∫¶N 4: PRODUCTION BEST PRACTICES\n",
    "1. Model Versioning\n",
    "2. Monitoring & Logging\n",
    "3. A/B Testing\n",
    "4. Rollback Strategy\n",
    "5. Common Anti-patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ Keras version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 1: SAVE & LOAD MODELS\n",
    "\n",
    "## 1.1 Model Formats trong TensorFlow\n",
    "\n",
    "### C√°c format ch√≠nh\n",
    "\n",
    "| Format | Extension | Khi n√†o d√πng | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |\n",
    "|--------|-----------|--------------|---------|-------------|\n",
    "| **Keras Format** | .keras | Training, evaluation | ƒê∆°n gi·∫£n, complete | Ch·ªâ cho Keras |\n",
    "| **SavedModel** | / (directory) | Production, serving | Universal, TF Serving | Ph·ª©c t·∫°p h∆°n |\n",
    "| **Checkpoint** | .ckpt | Training checkpoints | Ch·ªâ weights | C·∫ßn architecture |\n",
    "| **HDF5** | .h5 | Legacy (TF 1.x) | Backward compatible | Deprecated |\n",
    "\n",
    "### Khuy·∫øn ngh·ªã\n",
    "\n",
    "- üéØ **Training/Development**: Keras format (.keras)\n",
    "- üöÄ **Production/Serving**: SavedModel format\n",
    "- üíæ **Checkpoints**: During training\n",
    "- ‚ùå **Tr√°nh**: HDF5 (.h5) trong TF 2.x\n",
    "\n",
    "### So s√°nh SavedModel vs Keras\n",
    "\n",
    "#### Keras Format (.keras)\n",
    "```python\n",
    "model.save('my_model.keras')      # Save\n",
    "model = keras.models.load_model('my_model.keras')  # Load\n",
    "```\n",
    "‚úÖ ƒê∆°n gi·∫£n nh·∫•t\n",
    "‚úÖ L∆∞u: architecture + weights + optimizer state\n",
    "‚ùå Ch·ªâ cho Keras\n",
    "\n",
    "#### SavedModel Format\n",
    "```python\n",
    "model.save('my_model')            # Save (directory)\n",
    "model = keras.models.load_model('my_model')  # Load\n",
    "```\n",
    "‚úÖ Universal (TF Serving, TF Lite, TF.js)\n",
    "‚úÖ Language-agnostic\n",
    "‚úÖ Include signatures for serving\n",
    "‚ùå Ph·ª©c t·∫°p h∆°n m·ªôt ch√∫t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Keras Format - C√°ch ƒë∆°n gi·∫£n nh·∫•t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o m·ªôt model ƒë∆°n gi·∫£n ƒë·ªÉ demo\n",
    "def create_simple_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "model = create_simple_model()\n",
    "\n",
    "# Fake training data\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 3, 100)\n",
    "\n",
    "# Train\n",
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - Keras Format\n",
    "model_path = 'saved_models/my_model.keras'\n",
    "Path('saved_models').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving model...\")\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Check file size\n",
    "file_size = Path(model_path).stat().st_size / (1024 * 1024)  # MB\n",
    "print(f\"   File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - Keras Format\n",
    "print(\"üìÇ Loading model...\")\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nüîç Verification:\")\n",
    "\n",
    "# Test prediction\n",
    "X_test = np.random.rand(5, 10)\n",
    "original_pred = model.predict(X_test, verbose=0)\n",
    "loaded_pred = loaded_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Check if predictions are identical\n",
    "are_equal = np.allclose(original_pred, loaded_pred)\n",
    "print(f\"   Predictions match: {are_equal}\")\n",
    "\n",
    "if are_equal:\n",
    "    print(\"   ‚úÖ Model loaded correctly!\")\n",
    "else:\n",
    "    print(\"   ‚ùå Something wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 SavedModel Format - Cho Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE - SavedModel Format\n",
    "savedmodel_path = 'saved_models/my_savedmodel'\n",
    "\n",
    "print(\"üíæ Saving as SavedModel...\")\n",
    "model.save(savedmodel_path)  # Kh√¥ng c√≥ extension!\n",
    "print(f\"‚úÖ SavedModel saved to {savedmodel_path}\")\n",
    "\n",
    "# Check directory structure\n",
    "print(\"\\nüìÅ SavedModel directory structure:\")\n",
    "for path in Path(savedmodel_path).rglob('*'):\n",
    "    if path.is_file():\n",
    "        size = path.stat().st_size / 1024  # KB\n",
    "        print(f\"   {path.relative_to(savedmodel_path)}: {size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD - SavedModel Format\n",
    "print(\"üìÇ Loading SavedModel...\")\n",
    "loaded_savedmodel = keras.models.load_model(savedmodel_path)\n",
    "print(\"‚úÖ SavedModel loaded successfully!\")\n",
    "\n",
    "# Verify\n",
    "savedmodel_pred = loaded_savedmodel.predict(X_test, verbose=0)\n",
    "are_equal = np.allclose(original_pred, savedmodel_pred)\n",
    "print(f\"\\nüîç Predictions match: {are_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Checkpoints - Save during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint callback\n",
    "checkpoint_dir = 'checkpoints'\n",
    "Path(checkpoint_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f'{checkpoint_dir}/model_epoch_{{epoch:02d}}_val_acc_{{val_accuracy:.4f}}.keras',\n",
    "    save_best_only=False,  # Save m·ªói epoch\n",
    "    save_weights_only=False,  # Save full model\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train v·ªõi checkpoints\n",
    "print(\"üöÄ Training with checkpoints...\\n\")\n",
    "\n",
    "model_new = create_simple_model()\n",
    "history = model_new.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# List checkpoints\n",
    "print(\"\\nüìÅ Saved checkpoints:\")\n",
    "for ckpt in sorted(Path(checkpoint_dir).glob('*.keras')):\n",
    "    print(f\"   {ckpt.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load t·ª´ checkpoint\n",
    "checkpoints = sorted(Path(checkpoint_dir).glob('*.keras'))\n",
    "if checkpoints:\n",
    "    latest_checkpoint = checkpoints[-1]\n",
    "    print(f\"üìÇ Loading checkpoint: {latest_checkpoint.name}\")\n",
    "    \n",
    "    restored_model = keras.models.load_model(latest_checkpoint)\n",
    "    print(\"‚úÖ Model restored from checkpoint!\")\n",
    "    \n",
    "    # Continue training\n",
    "    print(\"\\nüîÑ Continue training...\")\n",
    "    restored_model.fit(X_train, y_train, epochs=2, verbose=0)\n",
    "    print(\"‚úÖ Training continued!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Save Weights Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights only (nh·ªè h∆°n, nhanh h∆°n)\n",
    "weights_path = 'saved_models/my_weights.weights.h5'\n",
    "\n",
    "print(\"üíæ Saving weights only...\")\n",
    "model.save_weights(weights_path)\n",
    "print(f\"‚úÖ Weights saved to {weights_path}\")\n",
    "\n",
    "# Compare sizes\n",
    "full_model_size = Path(model_path).stat().st_size / (1024 * 1024)\n",
    "weights_size = Path(weights_path).stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüìä Size comparison:\")\n",
    "print(f\"   Full model (.keras): {full_model_size:.2f} MB\")\n",
    "print(f\"   Weights only (.weights.h5): {weights_size:.2f} MB\")\n",
    "print(f\"   Savings: {(1 - weights_size/full_model_size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights only\n",
    "# ‚ö†Ô∏è PH·∫¢I t·∫°o model v·ªõi c√πng architecture tr∆∞·ªõc!\n",
    "\n",
    "print(\"üìÇ Loading weights...\")\n",
    "\n",
    "# Create model v·ªõi same architecture\n",
    "new_model = create_simple_model()\n",
    "\n",
    "# Load weights\n",
    "new_model.load_weights(weights_path)\n",
    "print(\"‚úÖ Weights loaded!\")\n",
    "\n",
    "# Verify\n",
    "new_pred = new_model.predict(X_test, verbose=0)\n",
    "are_equal = np.allclose(original_pred, new_pred)\n",
    "print(f\"\\nüîç Predictions match: {are_equal}\")\n",
    "\n",
    "print(\"\\nüí° L∆∞u √Ω:\")\n",
    "print(\"   - Weights only: Nh·ªè h∆°n, nhanh h∆°n\")\n",
    "print(\"   - Nh∆∞ng PH·∫¢I c√≥ architecture code\")\n",
    "print(\"   - Full model: L·ªõn h∆°n nh∆∞ng self-contained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Best Practices cho Save/Load\n",
    "\n",
    "### ‚úÖ DO (N√äN L√ÄM)\n",
    "\n",
    "#### 1. Versioning\n",
    "```python\n",
    "# ‚úÖ GOOD: Version trong t√™n file\n",
    "model.save(f'models/model_v{version}_{timestamp}.keras')\n",
    "\n",
    "# ‚úÖ GOOD: Structured directory\n",
    "models/\n",
    "‚îú‚îÄ‚îÄ v1.0/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model.keras\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metrics.json\n",
    "‚îî‚îÄ‚îÄ v1.1/\n",
    "```\n",
    "\n",
    "#### 2. Save metadata\n",
    "```python\n",
    "# ‚úÖ GOOD: L∆∞u metadata c√πng model\n",
    "metadata = {\n",
    "    'version': '1.0',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'accuracy': 0.95,\n",
    "    'config': config_dict\n",
    "}\n",
    "with open('models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "```\n",
    "\n",
    "#### 3. Test sau khi load\n",
    "```python\n",
    "# ‚úÖ GOOD: Always verify\n",
    "loaded_model = keras.models.load_model('model.keras')\n",
    "pred = loaded_model.predict(X_test)\n",
    "assert pred.shape == expected_shape\n",
    "```\n",
    "\n",
    "### ‚ùå DON'T (KH√îNG N√äN)\n",
    "\n",
    "#### 1. Overwrite models\n",
    "```python\n",
    "# ‚ùå BAD: Overwrite model.keras m·ªói l·∫ßn\n",
    "model.save('model.keras')  # M·∫•t model c≈©!\n",
    "\n",
    "# ‚úÖ GOOD: Version or timestamp\n",
    "model.save(f'model_{timestamp}.keras')\n",
    "```\n",
    "\n",
    "#### 2. Kh√¥ng save config\n",
    "```python\n",
    "# ‚ùå BAD: Ch·ªâ save model\n",
    "model.save('model.keras')\n",
    "\n",
    "# ‚úÖ GOOD: Save config c√πng\n",
    "model.save('model.keras')\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f)\n",
    "```\n",
    "\n",
    "#### 3. D√πng HDF5 trong TF 2.x\n",
    "```python\n",
    "# ‚ùå BAD: Legacy format\n",
    "model.save('model.h5')\n",
    "\n",
    "# ‚úÖ GOOD: Keras format\n",
    "model.save('model.keras')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete save with metadata\n",
    "\n",
    "def save_model_with_metadata(model, model_dir, version, config, metrics):\n",
    "    \"\"\"\n",
    "    Save model v·ªõi metadata ƒë·∫ßy ƒë·ªß\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        model_dir: Directory to save\n",
    "        version: Model version\n",
    "        config: Configuration dict\n",
    "        metrics: Metrics dict\n",
    "    \"\"\"\n",
    "    # Create directory\n",
    "    model_path = Path(model_dir) / f'v{version}'\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(model_path / 'model.keras')\n",
    "    \n",
    "    # Save config\n",
    "    with open(model_path / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(model_path / 'metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'version': version,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'tensorflow_version': tf.__version__,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    with open(model_path / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to {model_path}\")\n",
    "    print(\"   Files:\")\n",
    "    for file in model_path.iterdir():\n",
    "        print(f\"     - {file.name}\")\n",
    "\n",
    "# Save\n",
    "save_model_with_metadata(\n",
    "    model=model,\n",
    "    model_dir='production_models',\n",
    "    version='1.0',\n",
    "    config={'batch_size': 32, 'learning_rate': 0.001},\n",
    "    metrics={'accuracy': 0.95, 'loss': 0.15}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 2: INFERENCE PIPELINE\n",
    "\n",
    "## 2.1 Inference Pipeline l√† g√¨?\n",
    "\n",
    "### ƒê·ªãnh nghƒ©a\n",
    "\n",
    "**Inference Pipeline** = Quy tr√¨nh t·ª´ raw input ‚Üí predictions\n",
    "\n",
    "```\n",
    "Raw Input ‚Üí Preprocessing ‚Üí Model Inference ‚Üí Post-processing ‚Üí Final Output\n",
    "```\n",
    "\n",
    "### Components\n",
    "\n",
    "1. **Preprocessing**: Chu·∫©n b·ªã input (resize, normalize, augment)\n",
    "2. **Model Inference**: Ch·∫°y model\n",
    "3. **Post-processing**: X·ª≠ l√Ω output (threshold, NMS, format)\n",
    "4. **Error Handling**: Handle edge cases\n",
    "\n",
    "### Batch vs Real-time Inference\n",
    "\n",
    "| Batch Inference | Real-time Inference |\n",
    "|-----------------|---------------------|\n",
    "| X·ª≠ l√Ω nhi·ªÅu samples c√πng l√∫c | X·ª≠ l√Ω t·ª´ng sample |\n",
    "| Throughput cao | Latency th·∫•p |\n",
    "| Offline processing | Online serving |\n",
    "| V√≠ d·ª•: X·ª≠ l√Ω 1M ·∫£nh | V√≠ d·ª•: API endpoint |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inference Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    \"\"\"Production-ready inference pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, config=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_path: Path to saved model\n",
    "            config: Configuration dict\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.config = config or {}\n",
    "        self.model = None\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # Load model and metadata\n",
    "        self._load_model()\n",
    "        self._load_metadata()\n",
    "        \n",
    "        print(\"‚úÖ InferencePipeline initialized\")\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        try:\n",
    "            self.model = keras.models.load_model(self.model_path)\n",
    "            print(f\"   Model loaded from {self.model_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load metadata if available\"\"\"\n",
    "        model_dir = Path(self.model_path).parent\n",
    "        metadata_path = model_dir / 'metadata.json'\n",
    "        \n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "            print(f\"   Metadata loaded\")\n",
    "    \n",
    "    def preprocess(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocess inputs\n",
    "        \n",
    "        Args:\n",
    "            inputs: Raw inputs\n",
    "        \n",
    "        Returns:\n",
    "            Preprocessed inputs\n",
    "        \"\"\"\n",
    "        # Example: Normalize\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            inputs = inputs.astype('float32')\n",
    "            # Add normalization logic here\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def predict(self, inputs, batch_size=32):\n",
    "        \"\"\"\n",
    "        Batch inference\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input data\n",
    "            batch_size: Batch size for inference\n",
    "        \n",
    "        Returns:\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        inputs = self.preprocess(inputs)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(inputs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Post-process\n",
    "        predictions = self.postprocess(predictions)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_single(self, input_sample):\n",
    "        \"\"\"\n",
    "        Single sample inference (for real-time)\n",
    "        \n",
    "        Args:\n",
    "            input_sample: Single input sample\n",
    "        \n",
    "        Returns:\n",
    "            Prediction\n",
    "        \"\"\"\n",
    "        # Add batch dimension\n",
    "        if len(input_sample.shape) == len(self.model.input_shape) - 1:\n",
    "            input_sample = np.expand_dims(input_sample, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.predict(input_sample, batch_size=1)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        return prediction[0]\n",
    "    \n",
    "    def postprocess(self, predictions):\n",
    "        \"\"\"\n",
    "        Post-process predictions\n",
    "        \n",
    "        Args:\n",
    "            predictions: Raw predictions\n",
    "        \n",
    "        Returns:\n",
    "            Post-processed predictions\n",
    "        \"\"\"\n",
    "        # Example: Apply threshold for binary classification\n",
    "        # Or get class with highest probability\n",
    "        return predictions\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Get pipeline info\"\"\"\n",
    "        return {\n",
    "            'model_path': str(self.model_path),\n",
    "            'input_shape': self.model.input_shape,\n",
    "            'output_shape': self.model.output_shape,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ InferencePipeline class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "pipeline = InferencePipeline('production_models/v1.0/model.keras')\n",
    "\n",
    "# Pipeline info\n",
    "print(\"\\nüìã Pipeline Info:\")\n",
    "info = pipeline.get_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Batch inference\n",
    "X_batch = np.random.rand(10, 10)\n",
    "print(\"\\nüîÆ Batch inference...\")\n",
    "predictions = pipeline.predict(X_batch)\n",
    "print(f\"   Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Single inference\n",
    "X_single = np.random.rand(10)\n",
    "print(\"\\nüîÆ Single inference...\")\n",
    "prediction = pipeline.predict_single(X_single)\n",
    "print(f\"   Prediction shape: {prediction.shape}\")\n",
    "print(f\"   Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(pipeline, input_shape, num_samples=1000, batch_sizes=[1, 8, 32, 64]):\n",
    "    \"\"\"\n",
    "    Benchmark inference performance\n",
    "    \n",
    "    Args:\n",
    "        pipeline: InferencePipeline instance\n",
    "        input_shape: Input shape (without batch dim)\n",
    "        num_samples: Number of samples to test\n",
    "        batch_sizes: List of batch sizes to test\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary of benchmark results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Generate test data\n",
    "    X_test = np.random.rand(num_samples, *input_shape)\n",
    "    \n",
    "    print(f\"üìä Benchmarking inference with {num_samples} samples...\\n\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        \n",
    "        # Warmup\n",
    "        _ = pipeline.predict(X_test[:batch_size], batch_size=batch_size)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        predictions = pipeline.predict(X_test, batch_size=batch_size)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        throughput = num_samples / elapsed_time  # samples/second\n",
    "        latency = (elapsed_time / num_samples) * 1000  # ms per sample\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'elapsed_time': elapsed_time,\n",
    "            'throughput': throughput,\n",
    "            'latency': latency\n",
    "        }\n",
    "        \n",
    "        print(f\"  Elapsed time: {elapsed_time:.2f}s\")\n",
    "        print(f\"  Throughput: {throughput:.2f} samples/sec\")\n",
    "        print(f\"  Latency: {latency:.2f} ms/sample\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark\n",
    "results = benchmark_inference(\n",
    "    pipeline=pipeline,\n",
    "    input_shape=(10,),\n",
    "    num_samples=1000,\n",
    "    batch_sizes=[1, 8, 32, 64]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "batch_sizes = list(results.keys())\n",
    "throughputs = [results[bs]['throughput'] for bs in batch_sizes]\n",
    "latencies = [results[bs]['latency'] for bs in batch_sizes]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Throughput\n",
    "ax1.plot(batch_sizes, throughputs, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size', fontsize=12)\n",
    "ax1.set_ylabel('Throughput (samples/sec)', fontsize=12)\n",
    "ax1.set_title('Throughput vs Batch Size', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log', base=2)\n",
    "\n",
    "# Latency\n",
    "ax2.plot(batch_sizes, latencies, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "ax2.set_ylabel('Latency (ms/sample)', fontsize=12)\n",
    "ax2.set_title('Latency vs Batch Size', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Insights:\")\n",
    "print(\"   - Batch size l·ªõn ‚Üí Throughput cao (hi·ªáu qu·∫£ h∆°n)\")\n",
    "print(\"   - Batch size nh·ªè ‚Üí Latency th·∫•p (ph·∫£n h·ªìi nhanh)\")\n",
    "print(\"   - Trade-off gi·ªØa throughput v√† latency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 3: PERFORMANCE OPTIMIZATION\n",
    "\n",
    "## 3.1 Model Optimization Techniques\n",
    "\n",
    "### C√°c k·ªπ thu·∫≠t optimization\n",
    "\n",
    "| Technique | Gi·∫£m size | TƒÉng t·ªëc | Gi·∫£m accuracy | Khi n√†o d√πng |\n",
    "|-----------|-----------|----------|---------------|---------------|\n",
    "| **Quantization** | ‚úÖ‚úÖ‚úÖ‚úÖ (4x) | ‚úÖ‚úÖ‚úÖ | Minimal | Production, mobile |\n",
    "| **Pruning** | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ | Minimal | Model compression |\n",
    "| **Knowledge Distillation** | ‚úÖ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ | Small | Mobile, edge |\n",
    "| **TensorFlow Lite** | ‚úÖ‚úÖ‚úÖ | ‚úÖ‚úÖ‚úÖ‚úÖ | Minimal | Mobile deployment |\n",
    "| **ONNX** | ‚úÖ‚úÖ | ‚úÖ‚úÖ | None | Cross-platform |\n",
    "\n",
    "### Quantization l√† g√¨?\n",
    "\n",
    "**Quantization** = Gi·∫£m precision c·ªßa weights v√† activations\n",
    "\n",
    "- **Float32** ‚Üí **Int8**: 4x smaller, faster\n",
    "- **Post-training quantization**: Kh√¥ng c·∫ßn retrain\n",
    "- **Quantization-aware training**: Train v·ªõi quantization (accuracy t·ªët h∆°n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 TensorFlow Lite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Lite\n",
    "\n",
    "def convert_to_tflite(model, optimization='default'):\n",
    "    \"\"\"\n",
    "    Convert Keras model to TensorFlow Lite\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        optimization: 'default', 'float16', 'int8'\n",
    "    \n",
    "    Returns:\n",
    "        tflite_model: Converted model (bytes)\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimization == 'default':\n",
    "        # No optimization\n",
    "        pass\n",
    "    \n",
    "    elif optimization == 'float16':\n",
    "        # Float16 quantization\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    elif optimization == 'int8':\n",
    "        # Int8 quantization (c·∫ßn representative dataset)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        # Representative dataset for quantization\n",
    "        def representative_dataset():\n",
    "            for _ in range(100):\n",
    "                yield [np.random.rand(1, *model.input_shape[1:]).astype(np.float32)]\n",
    "        \n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "    \n",
    "    # Convert\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "# Convert v·ªõi c√°c optimization levels\n",
    "print(\"üîÑ Converting to TensorFlow Lite...\\n\")\n",
    "\n",
    "optimizations = ['default', 'float16', 'int8']\n",
    "tflite_models = {}\n",
    "\n",
    "for opt in optimizations:\n",
    "    print(f\"Converting with {opt} optimization...\")\n",
    "    try:\n",
    "        tflite_model = convert_to_tflite(model, optimization=opt)\n",
    "        tflite_models[opt] = tflite_model\n",
    "        \n",
    "        # Save\n",
    "        tflite_path = f'saved_models/model_{opt}.tflite'\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        size_mb = len(tflite_model) / (1024 * 1024)\n",
    "        print(f\"  ‚úÖ Saved to {tflite_path} ({size_mb:.2f} MB)\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed: {e}\\n\")\n",
    "\n",
    "print(\"‚úÖ Conversion completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes\n",
    "import os\n",
    "\n",
    "print(\"üìä MODEL SIZE COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Original model\n",
    "original_size = Path('production_models/v1.0/model.keras').stat().st_size / (1024 * 1024)\n",
    "print(f\"Original (.keras):      {original_size:>8.2f} MB (100%)\")\n",
    "\n",
    "# TFLite models\n",
    "for opt in optimizations:\n",
    "    tflite_path = f'saved_models/model_{opt}.tflite'\n",
    "    if Path(tflite_path).exists():\n",
    "        size = Path(tflite_path).stat().st_size / (1024 * 1024)\n",
    "        reduction = (1 - size / original_size) * 100\n",
    "        print(f\"TFLite ({opt:8s}): {size:>8.2f} MB ({100-reduction:.1f}%, -{reduction:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° Int8 quantization c√≥ th·ªÉ gi·∫£m size 4x v·ªõi accuracy loss minimal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ONNX format\n",
    "# ONNX = Open Neural Network Exchange (cross-platform format)\n",
    "\n",
    "try:\n",
    "    import tf2onnx\n",
    "    \n",
    "    print(\"üîÑ Converting to ONNX...\")\n",
    "    \n",
    "    # Convert\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model)\n",
    "    \n",
    "    # Save\n",
    "    onnx_path = 'saved_models/model.onnx'\n",
    "    with open(onnx_path, 'wb') as f:\n",
    "        f.write(onnx_model.SerializeToString())\n",
    "    \n",
    "    size_mb = Path(onnx_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ ONNX model saved to {onnx_path} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(\"\\nüí° ONNX format cho ph√©p:\")\n",
    "    print(\"   - Deploy tr√™n nhi·ªÅu platforms (PyTorch, ONNX Runtime, etc.)\")\n",
    "    print(\"   - Optimize v·ªõi ONNX Runtime\")\n",
    "    print(\"   - Cross-framework compatibility\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  tf2onnx not installed\")\n",
    "    print(\"   Install: pip install tf2onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 4: PRODUCTION BEST PRACTICES\n",
    "\n",
    "## 4.1 Model Versioning Strategy\n",
    "\n",
    "### Semantic Versioning\n",
    "\n",
    "```\n",
    "v{MAJOR}.{MINOR}.{PATCH}\n",
    "```\n",
    "\n",
    "- **MAJOR**: Breaking changes (architecture change, incompatible API)\n",
    "- **MINOR**: New features (backward compatible)\n",
    "- **PATCH**: Bug fixes, small improvements\n",
    "\n",
    "Examples:\n",
    "- `v1.0.0` ‚Üí Initial release\n",
    "- `v1.1.0` ‚Üí Th√™m features m·ªõi\n",
    "- `v1.1.1` ‚Üí Bug fix\n",
    "- `v2.0.0` ‚Üí Architecture change\n",
    "\n",
    "### Model Registry Structure\n",
    "\n",
    "```\n",
    "models/\n",
    "‚îú‚îÄ‚îÄ production/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ v1.2.1/          # Current production model\n",
    "‚îú‚îÄ‚îÄ staging/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ v1.3.0/          # Testing before production\n",
    "‚îú‚îÄ‚îÄ archive/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ v1.0.0/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ v1.1.0/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ v1.2.0/\n",
    "‚îî‚îÄ‚îÄ experiments/\n",
    "    ‚îî‚îÄ‚îÄ exp_001/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"Simple model registry for versioning\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir='model_registry'):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        for subdir in ['production', 'staging', 'archive', 'experiments']:\n",
    "            (self.base_dir / subdir).mkdir(exist_ok=True)\n",
    "    \n",
    "    def register_model(self, model, version, stage='staging', metadata=None):\n",
    "        \"\"\"\n",
    "        Register a model\n",
    "        \n",
    "        Args:\n",
    "            model: Keras model\n",
    "            version: Version string (e.g., '1.2.0')\n",
    "            stage: 'staging', 'production', or 'archive'\n",
    "            metadata: Optional metadata dict\n",
    "        \"\"\"\n",
    "        # Create version directory\n",
    "        model_dir = self.base_dir / stage / f'v{version}'\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model.save(model_dir / 'model.keras')\n",
    "        \n",
    "        # Save metadata\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        \n",
    "        metadata.update({\n",
    "            'version': version,\n",
    "            'stage': stage,\n",
    "            'registered_at': datetime.now().isoformat(),\n",
    "            'tensorflow_version': tf.__version__\n",
    "        })\n",
    "        \n",
    "        with open(model_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Model v{version} registered to {stage}\")\n",
    "        return model_dir\n",
    "    \n",
    "    def promote_to_production(self, version):\n",
    "        \"\"\"\n",
    "        Promote staging model to production\n",
    "        \n",
    "        Args:\n",
    "            version: Version to promote\n",
    "        \"\"\"\n",
    "        staging_path = self.base_dir / 'staging' / f'v{version}'\n",
    "        production_path = self.base_dir / 'production' / f'v{version}'\n",
    "        \n",
    "        if not staging_path.exists():\n",
    "            raise ValueError(f\"Version {version} not found in staging\")\n",
    "        \n",
    "        # Archive current production model if exists\n",
    "        current_prod = list((self.base_dir / 'production').iterdir())\n",
    "        for old_model in current_prod:\n",
    "            archive_path = self.base_dir / 'archive' / old_model.name\n",
    "            shutil.move(str(old_model), str(archive_path))\n",
    "            print(f\"   Archived {old_model.name}\")\n",
    "        \n",
    "        # Copy to production\n",
    "        shutil.copytree(staging_path, production_path)\n",
    "        \n",
    "        print(f\"‚úÖ Model v{version} promoted to production!\")\n",
    "    \n",
    "    def load_model(self, version=None, stage='production'):\n",
    "        \"\"\"\n",
    "        Load model from registry\n",
    "        \n",
    "        Args:\n",
    "            version: Version to load (None = latest)\n",
    "            stage: Stage to load from\n",
    "        \n",
    "        Returns:\n",
    "            model, metadata\n",
    "        \"\"\"\n",
    "        stage_dir = self.base_dir / stage\n",
    "        \n",
    "        if version is None:\n",
    "            # Load latest\n",
    "            versions = sorted(stage_dir.iterdir())\n",
    "            if not versions:\n",
    "                raise ValueError(f\"No models found in {stage}\")\n",
    "            model_dir = versions[-1]\n",
    "        else:\n",
    "            model_dir = stage_dir / f'v{version}'\n",
    "        \n",
    "        # Load model\n",
    "        model = keras.models.load_model(model_dir / 'model.keras')\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(model_dir / 'metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded model v{metadata['version']} from {stage}\")\n",
    "        return model, metadata\n",
    "    \n",
    "    def list_models(self, stage=None):\n",
    "        \"\"\"List all models\"\"\"\n",
    "        if stage:\n",
    "            stages = [stage]\n",
    "        else:\n",
    "            stages = ['production', 'staging', 'archive']\n",
    "        \n",
    "        for stage in stages:\n",
    "            stage_dir = self.base_dir / stage\n",
    "            versions = sorted(stage_dir.iterdir())\n",
    "            \n",
    "            print(f\"\\n{stage.upper()}:\")\n",
    "            if not versions:\n",
    "                print(\"  (empty)\")\n",
    "            else:\n",
    "                for v in versions:\n",
    "                    print(f\"  - {v.name}\")\n",
    "\n",
    "print(\"‚úÖ ModelRegistry class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register model to staging\n",
    "registry.register_model(\n",
    "    model=model,\n",
    "    version='1.0.0',\n",
    "    stage='staging',\n",
    "    metadata={'accuracy': 0.95, 'description': 'Initial release'}\n",
    ")\n",
    "\n",
    "# Promote to production\n",
    "print(\"\\nüöÄ Promoting to production...\")\n",
    "registry.promote_to_production('1.0.0')\n",
    "\n",
    "# List models\n",
    "registry.list_models()\n",
    "\n",
    "# Load from production\n",
    "print(\"\\nüìÇ Loading from production...\")\n",
    "prod_model, metadata = registry.load_model(stage='production')\n",
    "print(f\"   Metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Monitoring & Logging\n",
    "\n",
    "### Metrics c·∫ßn monitor\n",
    "\n",
    "#### Model Performance Metrics\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- Distribution of predictions\n",
    "- Confidence scores\n",
    "\n",
    "#### System Metrics\n",
    "- Latency (p50, p95, p99)\n",
    "- Throughput (requests/sec)\n",
    "- Error rate\n",
    "- CPU/Memory usage\n",
    "\n",
    "#### Data Quality Metrics\n",
    "- Input distribution shift\n",
    "- Missing values\n",
    "- Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"Monitor model performance in production\"\"\"\n",
    "    \n",
    "    def __init__(self, log_file='model_monitor.log'):\n",
    "        self.log_file = log_file\n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def log_prediction(self, input_data, prediction, confidence, latency):\n",
    "        \"\"\"\n",
    "        Log a prediction\n",
    "        \n",
    "        Args:\n",
    "            input_data: Input features\n",
    "            prediction: Model prediction\n",
    "            confidence: Confidence score\n",
    "            latency: Inference latency (ms)\n",
    "        \"\"\"\n",
    "        # Store metrics\n",
    "        self.metrics['latency'].append(latency)\n",
    "        self.metrics['confidence'].append(confidence)\n",
    "        \n",
    "        # Log\n",
    "        self.logger.info(f\"Prediction: {prediction}, Confidence: {confidence:.4f}, Latency: {latency:.2f}ms\")\n",
    "    \n",
    "    def log_error(self, error_type, error_message):\n",
    "        \"\"\"Log an error\"\"\"\n",
    "        self.logger.error(f\"{error_type}: {error_message}\")\n",
    "        self.metrics['errors'].append(error_type)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get monitoring statistics\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        if self.metrics['latency']:\n",
    "            latencies = np.array(self.metrics['latency'])\n",
    "            stats['latency'] = {\n",
    "                'mean': float(np.mean(latencies)),\n",
    "                'p50': float(np.percentile(latencies, 50)),\n",
    "                'p95': float(np.percentile(latencies, 95)),\n",
    "                'p99': float(np.percentile(latencies, 99)),\n",
    "                'max': float(np.max(latencies))\n",
    "            }\n",
    "        \n",
    "        if self.metrics['confidence']:\n",
    "            confidences = np.array(self.metrics['confidence'])\n",
    "            stats['confidence'] = {\n",
    "                'mean': float(np.mean(confidences)),\n",
    "                'min': float(np.min(confidences)),\n",
    "                'max': float(np.max(confidences))\n",
    "            }\n",
    "        \n",
    "        stats['total_predictions'] = len(self.metrics['latency'])\n",
    "        stats['total_errors'] = len(self.metrics['errors'])\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print monitoring report\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(\"üìä MONITORING REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total Predictions: {stats['total_predictions']}\")\n",
    "        print(f\"Total Errors: {stats['total_errors']}\")\n",
    "        \n",
    "        if 'latency' in stats:\n",
    "            print(\"\\nLatency Statistics (ms):\")\n",
    "            for key, value in stats['latency'].items():\n",
    "                print(f\"  {key}: {value:.2f}\")\n",
    "        \n",
    "        if 'confidence' in stats:\n",
    "            print(\"\\nConfidence Statistics:\")\n",
    "            for key, value in stats['confidence'].items():\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Example usage\n",
    "monitor = ModelMonitor()\n",
    "\n",
    "# Simulate predictions\n",
    "for i in range(100):\n",
    "    input_data = np.random.rand(10)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    prediction = pipeline.predict_single(input_data)\n",
    "    latency = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    confidence = float(np.max(prediction))\n",
    "    \n",
    "    monitor.log_prediction(input_data, prediction, confidence, latency)\n",
    "\n",
    "# Print report\n",
    "monitor.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Common Anti-patterns\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 1: Kh√¥ng version models\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Overwrite model.keras\n",
    "model.save('model.keras')  # M·∫•t track c·ªßa models c≈©!\n",
    "\n",
    "# ‚úÖ GOOD: Version models\n",
    "model.save(f'models/v{version}/model.keras')\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 2: Training/Inference preprocessing kh√°c nhau\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Preprocessing kh√°c nhau\n",
    "# Training\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "# Inference\n",
    "X_test = (X_test - mean) / std  # KH√ÅC!\n",
    "\n",
    "# ‚úÖ GOOD: Same preprocessing\n",
    "def preprocess(X):\n",
    "    return X / 255.0\n",
    "\n",
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 3: Kh√¥ng handle errors\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: No error handling\n",
    "prediction = model.predict(input_data)\n",
    "\n",
    "# ‚úÖ GOOD: Handle errors\n",
    "try:\n",
    "    prediction = model.predict(input_data)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Prediction failed: {e}\")\n",
    "    return default_prediction\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 4: Kh√¥ng monitor production\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Deploy and forget\n",
    "model.predict(X)\n",
    "\n",
    "# ‚úÖ GOOD: Monitor everything\n",
    "start = time.time()\n",
    "prediction = model.predict(X)\n",
    "latency = time.time() - start\n",
    "monitor.log(prediction, latency)\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 5: Hardcode config trong code\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Magic numbers\n",
    "model = Model(hidden_size=128, dropout=0.2, lr=0.001)\n",
    "\n",
    "# ‚úÖ GOOD: Config file\n",
    "config = load_config('config.yaml')\n",
    "model = Model(**config['model'])\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 6: Kh√¥ng test model sau khi load\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Load and use\n",
    "model = load_model('model.keras')\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# ‚úÖ GOOD: Verify after load\n",
    "model = load_model('model.keras')\n",
    "test_input = np.random.rand(1, *input_shape)\n",
    "test_output = model.predict(test_input)\n",
    "assert test_output.shape == expected_shape\n",
    "```\n",
    "\n",
    "### ‚ùå ANTI-PATTERN 7: Qu√° optimize s·ªõm\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Optimize ngay t·ª´ ƒë·∫ßu\n",
    "# - Quantize model\n",
    "# - Prune layers\n",
    "# - Complex serving setup\n",
    "# ‚Üí Ch∆∞a bi·∫øt bottleneck ·ªü ƒë√¢u!\n",
    "\n",
    "# ‚úÖ GOOD: Optimize khi c·∫ßn\n",
    "# 1. Deploy simple version\n",
    "# 2. Measure performance\n",
    "# 3. Identify bottlenecks\n",
    "# 4. Optimize targeted areas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Production Checklist\n",
    "\n",
    "### ‚úÖ Pre-deployment\n",
    "\n",
    "- [ ] Model achieves target metrics\n",
    "- [ ] Model versioned properly\n",
    "- [ ] Config file created\n",
    "- [ ] Preprocessing code tested\n",
    "- [ ] Inference pipeline tested\n",
    "- [ ] Error handling implemented\n",
    "- [ ] Logging configured\n",
    "- [ ] Documentation written\n",
    "\n",
    "### ‚úÖ Deployment\n",
    "\n",
    "- [ ] Model saved in production format\n",
    "- [ ] A/B testing setup (if needed)\n",
    "- [ ] Monitoring enabled\n",
    "- [ ] Alerts configured\n",
    "- [ ] Rollback plan ready\n",
    "- [ ] Load testing completed\n",
    "- [ ] Gradual rollout plan\n",
    "\n",
    "### ‚úÖ Post-deployment\n",
    "\n",
    "- [ ] Monitor metrics daily\n",
    "- [ ] Check for data drift\n",
    "- [ ] Review error logs\n",
    "- [ ] Analyze prediction distribution\n",
    "- [ ] Gather user feedback\n",
    "- [ ] Plan next iteration\n",
    "\n",
    "### üö® Red Flags\n",
    "\n",
    "- ‚ö†Ô∏è  Accuracy drop > 5%\n",
    "- ‚ö†Ô∏è  Latency increase > 50%\n",
    "- ‚ö†Ô∏è  Error rate > 1%\n",
    "- ‚ö†Ô∏è  Memory leak\n",
    "- ‚ö†Ô∏è  Input distribution shift\n",
    "- ‚ö†Ô∏è  Unusual prediction patterns\n",
    "\n",
    "‚Üí **ROLLBACK IMMEDIATELY!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì T·ªïng k·∫øt FILE 3-C & TO√ÄN B·ªò SERIES\n",
    "\n",
    "## ‚úÖ FILE 3-C: Model Deployment & Production\n",
    "\n",
    "### 1. Save & Load Models\n",
    "- **Formats**: Keras (.keras), SavedModel, Checkpoints\n",
    "- **Best practices**: Versioning, metadata, verification\n",
    "- **Recommendation**: .keras cho development, SavedModel cho production\n",
    "\n",
    "### 2. Inference Pipeline\n",
    "- **Components**: Preprocessing ‚Üí Inference ‚Üí Post-processing\n",
    "- **Batch vs Real-time**: Trade-off throughput vs latency\n",
    "- **Performance**: Benchmark v√† optimize\n",
    "\n",
    "### 3. Performance Optimization\n",
    "- **Quantization**: Float32 ‚Üí Int8 (4x smaller)\n",
    "- **TensorFlow Lite**: Mobile deployment\n",
    "- **ONNX**: Cross-platform compatibility\n",
    "\n",
    "### 4. Production Best Practices\n",
    "- **Versioning**: Semantic versioning (v1.2.3)\n",
    "- **Monitoring**: Metrics, logs, alerts\n",
    "- **Anti-patterns**: Common mistakes to avoid\n",
    "- **Checklist**: Pre/during/post deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò SERIES!\n",
    "\n",
    "### FILE 3-A: Transfer Learning & Mixed Precision\n",
    "- ‚úÖ Transfer Learning (MobileNetV2, ResNet50)\n",
    "- ‚úÖ Feature Extraction vs Fine-tuning\n",
    "- ‚úÖ Mixed Precision Training (2-3x speedup)\n",
    "\n",
    "### FILE 3-B: Clean ML Pipeline & Evaluation\n",
    "- ‚úÖ Clean ML Pipeline (config-driven, modular)\n",
    "- ‚úÖ Reproducibility (seeds, versioning)\n",
    "- ‚úÖ Model Evaluation (metrics, cross-validation)\n",
    "\n",
    "### FILE 3-C: Model Deployment & Production\n",
    "- ‚úÖ Save/Load strategies\n",
    "- ‚úÖ Inference pipeline\n",
    "- ‚úÖ Performance optimization\n",
    "- ‚úÖ Production best practices\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps - B·∫°n ƒë√£ s·∫µn s√†ng cho:\n",
    "\n",
    "### 1. Production ML Projects\n",
    "- Build end-to-end ML pipelines\n",
    "- Deploy models to production\n",
    "- Monitor and maintain models\n",
    "\n",
    "### 2. Advanced Topics\n",
    "- TensorFlow Serving\n",
    "- MLOps with MLflow/Kubeflow\n",
    "- Distributed training\n",
    "- Model compression techniques\n",
    "\n",
    "### 3. Specialized Domains\n",
    "- Computer Vision (object detection, segmentation)\n",
    "- NLP (transformers, BERT)\n",
    "- Time Series forecasting\n",
    "- Recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways - Top 10\n",
    "\n",
    "1. **Transfer Learning** l√† must-have cho CV\n",
    "2. **Mixed Precision** = free 2-3x speedup\n",
    "3. **Clean pipeline** = d·ªÖ maintain v√† scale\n",
    "4. **Reproducibility** = set seeds + version everything\n",
    "5. **Right metrics** > high accuracy\n",
    "6. **SavedModel** for production\n",
    "7. **Monitor everything** in production\n",
    "8. **Version models** properly (semantic versioning)\n",
    "9. **Optimize when needed**, not prematurely\n",
    "10. **Production checklist** before deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìö T√†i li·ªáu tham kh·∫£o\n",
    "\n",
    "- **TensorFlow Official Docs**: https://www.tensorflow.org/guide\n",
    "- **TensorFlow Model Optimization**: https://www.tensorflow.org/model_optimization\n",
    "- **MLOps Best Practices**: https://ml-ops.org/\n",
    "- **Production ML Systems**: https://developers.google.com/machine-learning/crash-course/production-ml-systems\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Ch√∫c m·ª´ng b·∫°n ƒë√£ ho√†n th√†nh to√†n b·ªô course TensorFlow t·ª´ Beginner ƒë·∫øn Professional! üéâ**\n",
    "\n",
    "**B·∫°n gi·ªù ƒë√£ s·∫µn s√†ng build v√† deploy production ML systems! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
