{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìô FILE 3-B: CLEAN ML PIPELINE & MODEL EVALUATION\n",
    "\n",
    "**Ph·∫ßn:** ADVANCED & PROFESSIONAL (Production-Ready)\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- ‚úÖ X√¢y d·ª±ng Clean ML Pipeline\n",
    "- ‚úÖ ƒê·∫£m b·∫£o Reproducibility\n",
    "- ‚úÖ Model Evaluation chuy√™n nghi·ªáp\n",
    "- ‚úÖ Metrics cho c√°c b√†i to√°n kh√°c nhau\n",
    "- ‚úÖ Best practices cho production\n",
    "\n",
    "**Th·ªùi l∆∞·ª£ng:** 2-3 tu·∫ßn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö M·ª•c L·ª•c\n",
    "\n",
    "### PH·∫¶N 1: CLEAN ML PIPELINE\n",
    "1. ML Pipeline l√† g√¨?\n",
    "2. Pipeline Components\n",
    "3. Data Pipeline\n",
    "4. Training Pipeline\n",
    "5. Config Management\n",
    "6. Experiment Tracking\n",
    "\n",
    "### PH·∫¶N 2: REPRODUCIBILITY\n",
    "1. Reproducibility l√† g√¨?\n",
    "2. Random Seeds\n",
    "3. Version Control\n",
    "4. Environment Management\n",
    "5. Data Versioning\n",
    "\n",
    "### PH·∫¶N 3: MODEL EVALUATION & METRICS\n",
    "1. Classification Metrics\n",
    "2. Regression Metrics\n",
    "3. Confusion Matrix Analysis\n",
    "4. ROC & AUC\n",
    "5. Cross-validation\n",
    "6. Model Comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 1: CLEAN ML PIPELINE\n",
    "\n",
    "## 1.1 ML Pipeline l√† g√¨?\n",
    "\n",
    "### ƒê·ªãnh nghƒ©a\n",
    "\n",
    "**ML Pipeline** = Quy tr√¨nh t·ª± ƒë·ªông h√≥a c√°c b∆∞·ªõc trong ML workflow\n",
    "\n",
    "### T·∫°i sao c·∫ßn ML Pipeline?\n",
    "\n",
    "| V·∫•n ƒë·ªÅ | Gi·∫£i ph√°p v·ªõi Pipeline |\n",
    "|--------|------------------------|\n",
    "| üîÑ **Reproducibility** | Quy tr√¨nh c·ªë ƒë·ªãnh, d·ªÖ l·∫∑p l·∫°i |\n",
    "| üêõ **Debugging** | D·ªÖ t√¨m l·ªói, test t·ª´ng component |\n",
    "| üìä **Scaling** | D·ªÖ scale l√™n production |\n",
    "| üë• **Collaboration** | Code r√µ r√†ng, d·ªÖ l√†m vi·ªác nh√≥m |\n",
    "| üîß **Maintenance** | D·ªÖ update, maintain |\n",
    "\n",
    "### Components c·ªßa ML Pipeline\n",
    "\n",
    "```\n",
    "Data Loading ‚Üí Preprocessing ‚Üí Augmentation ‚Üí Training ‚Üí Evaluation ‚Üí Deployment\n",
    "```\n",
    "\n",
    "### Pipeline t·ªët vs Pipeline x·∫•u\n",
    "\n",
    "#### ‚ùå Pipeline X·∫§U\n",
    "```python\n",
    "# Notebook messy, code r·∫£i r√°c\n",
    "# Magic numbers everywhere\n",
    "# Kh√¥ng c√≥ config\n",
    "# Kh√¥ng track experiments\n",
    "# Kh√¥ng reproducible\n",
    "```\n",
    "\n",
    "#### ‚úÖ Pipeline T·ªêT\n",
    "```python\n",
    "# Modular, organized\n",
    "# Config-driven\n",
    "# Logging & tracking\n",
    "# Version control\n",
    "# Reproducible\n",
    "# Documented\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Config Management\n",
    "\n",
    "### T·∫°i sao c·∫ßn Config?\n",
    "\n",
    "- ‚úÖ T√°ch code v√† parameters\n",
    "- ‚úÖ D·ªÖ thay ƒë·ªïi hyperparameters\n",
    "- ‚úÖ D·ªÖ share experiments\n",
    "- ‚úÖ Version control cho configs\n",
    "\n",
    "### Config File Format\n",
    "\n",
    "C√≥ th·ªÉ d√πng:\n",
    "- **JSON**: ƒê∆°n gi·∫£n, d·ªÖ ƒë·ªçc\n",
    "- **YAML**: D·ªÖ ƒë·ªçc h∆°n JSON, support comments\n",
    "- **Python dict**: Linh ho·∫°t nh·∫•t\n",
    "\n",
    "Khuy·∫øn ngh·ªã: **YAML** cho d·ªÖ ƒë·ªçc v√† comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Config v·ªõi Python dict\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for ML pipeline\"\"\"\n",
    "    \n",
    "    # Data\n",
    "    DATA_DIR = './data'\n",
    "    IMG_SIZE = 224\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = 'MobileNetV2'\n",
    "    NUM_CLASSES = 2\n",
    "    DROPOUT_RATE = 0.2\n",
    "    \n",
    "    # Training\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    OPTIMIZER = 'adam'\n",
    "    LOSS = 'binary_crossentropy'\n",
    "    \n",
    "    # Callbacks\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    REDUCE_LR_PATIENCE = 5\n",
    "    REDUCE_LR_FACTOR = 0.5\n",
    "    \n",
    "    # Paths\n",
    "    MODEL_DIR = './models'\n",
    "    LOG_DIR = './logs'\n",
    "    CHECKPOINT_DIR = './checkpoints'\n",
    "    \n",
    "    # Random seed\n",
    "    SEED = 42\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return {k: v for k, v in cls.__dict__.items() \n",
    "                if not k.startswith('_') and not callable(v)}\n",
    "    \n",
    "    @classmethod\n",
    "    def save(cls, path):\n",
    "        \"\"\"Save config to JSON file\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(cls.to_dict(), f, indent=2)\n",
    "        print(f\"‚úÖ Config saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load config from JSON file\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        for key, value in config_dict.items():\n",
    "            setattr(cls, key, value)\n",
    "        \n",
    "        print(f\"‚úÖ Config loaded from {path}\")\n",
    "\n",
    "# Test\n",
    "config = Config()\n",
    "print(\"üìã Current Config:\")\n",
    "print(json.dumps(config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Config v·ªõi YAML (khuy·∫øn ngh·ªã)\n",
    "\n",
    "config_yaml = \"\"\"\n",
    "# ML Pipeline Configuration\n",
    "\n",
    "# Data Configuration\n",
    "data:\n",
    "  data_dir: ./data\n",
    "  img_size: 224\n",
    "  batch_size: 32\n",
    "  validation_split: 0.2\n",
    "  \n",
    "# Model Configuration  \n",
    "model:\n",
    "  name: MobileNetV2\n",
    "  num_classes: 2\n",
    "  dropout_rate: 0.2\n",
    "  pretrained: true\n",
    "  \n",
    "# Training Configuration\n",
    "training:\n",
    "  epochs: 50\n",
    "  learning_rate: 0.001\n",
    "  optimizer: adam\n",
    "  loss: binary_crossentropy\n",
    "  metrics:\n",
    "    - accuracy\n",
    "    - precision\n",
    "    - recall\n",
    "  \n",
    "# Callbacks\n",
    "callbacks:\n",
    "  early_stopping:\n",
    "    patience: 10\n",
    "    restore_best_weights: true\n",
    "  reduce_lr:\n",
    "    patience: 5\n",
    "    factor: 0.5\n",
    "    min_lr: 1.0e-7\n",
    "  model_checkpoint:\n",
    "    save_best_only: true\n",
    "    \n",
    "# Paths\n",
    "paths:\n",
    "  model_dir: ./models\n",
    "  log_dir: ./logs\n",
    "  checkpoint_dir: ./checkpoints\n",
    "  \n",
    "# Reproducibility\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "# Save config\n",
    "with open('config.yaml', 'w') as f:\n",
    "    f.write(config_yaml)\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚úÖ Config loaded from YAML:\")\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "print(\"\\nüí° ∆Øu ƒëi·ªÉm c·ªßa YAML:\")\n",
    "print(\"   ‚úÖ D·ªÖ ƒë·ªçc h∆°n JSON\")\n",
    "print(\"   ‚úÖ Support comments\")\n",
    "print(\"   ‚úÖ Hierarchical structure\")\n",
    "print(\"   ‚úÖ D·ªÖ edit b·∫±ng text editor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Pipeline\n",
    "\n",
    "### Clean Data Pipeline Structure\n",
    "\n",
    "```python\n",
    "class DataPipeline:\n",
    "    def __init__(self, config)\n",
    "    def load_data()\n",
    "    def preprocess()\n",
    "    def augment()\n",
    "    def create_dataset()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Clean data pipeline for image classification\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.img_size = config['data']['img_size']\n",
    "        self.batch_size = config['data']['batch_size']\n",
    "        self.data_dir = config['data']['data_dir']\n",
    "        \n",
    "    def create_preprocessing_fn(self):\n",
    "        \"\"\"Create preprocessing function\"\"\"\n",
    "        def preprocess(image, label):\n",
    "            # Resize\n",
    "            image = tf.image.resize(image, (self.img_size, self.img_size))\n",
    "            # Normalize to [0, 1]\n",
    "            image = image / 255.0\n",
    "            return image, label\n",
    "        return preprocess\n",
    "    \n",
    "    def create_augmentation_fn(self):\n",
    "        \"\"\"Create augmentation function for training\"\"\"\n",
    "        def augment(image, label):\n",
    "            # Random flip\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            # Random brightness\n",
    "            image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "            # Random contrast\n",
    "            image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "            # Clip values\n",
    "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "            return image, label\n",
    "        return augment\n",
    "    \n",
    "    def create_dataset(self, file_pattern, is_training=True):\n",
    "        \"\"\"\n",
    "        Create tf.data.Dataset\n",
    "        \n",
    "        Args:\n",
    "            file_pattern: Pattern for files (e.g., 'train/*.jpg')\n",
    "            is_training: Whether this is training data\n",
    "        \n",
    "        Returns:\n",
    "            tf.data.Dataset\n",
    "        \"\"\"\n",
    "        # Load dataset (example v·ªõi image_dataset_from_directory)\n",
    "        dataset = keras.utils.image_dataset_from_directory(\n",
    "            self.data_dir,\n",
    "            image_size=(self.img_size, self.img_size),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=is_training\n",
    "        )\n",
    "        \n",
    "        # Preprocessing\n",
    "        preprocess_fn = self.create_preprocessing_fn()\n",
    "        dataset = dataset.map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Augmentation (ch·ªâ cho training)\n",
    "        if is_training:\n",
    "            augment_fn = self.create_augmentation_fn()\n",
    "            dataset = dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Performance optimization\n",
    "        dataset = dataset.cache()  # Cache sau khi augment\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def get_train_val_datasets(self, validation_split=0.2):\n",
    "        \"\"\"\n",
    "        Get training and validation datasets\n",
    "        \n",
    "        Args:\n",
    "            validation_split: Fraction of data for validation\n",
    "        \n",
    "        Returns:\n",
    "            train_dataset, val_dataset\n",
    "        \"\"\"\n",
    "        # Implementation depends on data structure\n",
    "        pass\n",
    "\n",
    "print(\"‚úÖ DataPipeline class defined!\")\n",
    "print(\"\\nüìö Features:\")\n",
    "print(\"   ‚úÖ Config-driven\")\n",
    "print(\"   ‚úÖ Modular (preprocess, augment separate)\")\n",
    "print(\"   ‚úÖ Performance optimized (cache, prefetch)\")\n",
    "print(\"   ‚úÖ Easy to test v√† maintain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training Pipeline\n",
    "\n",
    "### Clean Training Pipeline Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"Clean training pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Create directories\n",
    "        self._create_directories()\n",
    "        \n",
    "        # Set seeds for reproducibility\n",
    "        self._set_seeds(config['seed'])\n",
    "    \n",
    "    def _create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        paths = self.config['paths']\n",
    "        for path in paths.values():\n",
    "            Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        print(\"‚úÖ Directories created\")\n",
    "    \n",
    "    def _set_seeds(self, seed):\n",
    "        \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        print(f\"‚úÖ Seeds set to {seed}\")\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build model from config\"\"\"\n",
    "        model_config = self.config['model']\n",
    "        \n",
    "        # Base model\n",
    "        if model_config['name'] == 'MobileNetV2':\n",
    "            from tensorflow.keras.applications import MobileNetV2\n",
    "            base_model = MobileNetV2(\n",
    "                input_shape=(224, 224, 3),\n",
    "                include_top=False,\n",
    "                weights='imagenet' if model_config['pretrained'] else None\n",
    "            )\n",
    "            base_model.trainable = False\n",
    "        \n",
    "        # Build complete model\n",
    "        inputs = keras.Input(shape=(224, 224, 3))\n",
    "        x = base_model(inputs, training=False)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(model_config['dropout_rate'])(x)\n",
    "        outputs = layers.Dense(model_config['num_classes'], activation='softmax')(x)\n",
    "        \n",
    "        self.model = keras.Model(inputs, outputs)\n",
    "        print(\"‚úÖ Model built\")\n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        \"\"\"Compile model from config\"\"\"\n",
    "        train_config = self.config['training']\n",
    "        \n",
    "        # Optimizer\n",
    "        if train_config['optimizer'] == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(\n",
    "                learning_rate=train_config['learning_rate']\n",
    "            )\n",
    "        \n",
    "        # Compile\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=train_config['loss'],\n",
    "            metrics=train_config['metrics']\n",
    "        )\n",
    "        print(\"‚úÖ Model compiled\")\n",
    "    \n",
    "    def create_callbacks(self):\n",
    "        \"\"\"Create callbacks from config\"\"\"\n",
    "        cb_config = self.config['callbacks']\n",
    "        paths = self.config['paths']\n",
    "        \n",
    "        callbacks = []\n",
    "        \n",
    "        # Early Stopping\n",
    "        callbacks.append(keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=cb_config['early_stopping']['patience'],\n",
    "            restore_best_weights=cb_config['early_stopping']['restore_best_weights'],\n",
    "            verbose=1\n",
    "        ))\n",
    "        \n",
    "        # ReduceLROnPlateau\n",
    "        callbacks.append(keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            patience=cb_config['reduce_lr']['patience'],\n",
    "            factor=cb_config['reduce_lr']['factor'],\n",
    "            min_lr=cb_config['reduce_lr']['min_lr'],\n",
    "            verbose=1\n",
    "        ))\n",
    "        \n",
    "        # ModelCheckpoint\n",
    "        checkpoint_path = Path(paths['checkpoint_dir']) / 'best_model.keras'\n",
    "        callbacks.append(keras.callbacks.ModelCheckpoint(\n",
    "            str(checkpoint_path),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=cb_config['model_checkpoint']['save_best_only'],\n",
    "            verbose=1\n",
    "        ))\n",
    "        \n",
    "        # TensorBoard\n",
    "        log_dir = Path(paths['log_dir']) / datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        callbacks.append(keras.callbacks.TensorBoard(\n",
    "            log_dir=str(log_dir),\n",
    "            histogram_freq=1\n",
    "        ))\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(callbacks)} callbacks\")\n",
    "        return callbacks\n",
    "    \n",
    "    def train(self, train_dataset, val_dataset):\n",
    "        \"\"\"Train model\"\"\"\n",
    "        train_config = self.config['training']\n",
    "        callbacks = self.create_callbacks()\n",
    "        \n",
    "        print(\"\\nüöÄ Starting training...\\n\")\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=train_config['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Training completed!\")\n",
    "        return self.history\n",
    "    \n",
    "    def save_model(self, name='final_model'):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        model_path = Path(self.config['paths']['model_dir']) / f'{name}.keras'\n",
    "        self.model.save(str(model_path))\n",
    "        print(f\"‚úÖ Model saved to {model_path}\")\n",
    "    \n",
    "    def save_history(self, name='history'):\n",
    "        \"\"\"Save training history\"\"\"\n",
    "        history_path = Path(self.config['paths']['log_dir']) / f'{name}.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.history.history, f, indent=2)\n",
    "        print(f\"‚úÖ History saved to {history_path}\")\n",
    "\n",
    "print(\"‚úÖ TrainingPipeline class defined!\")\n",
    "print(\"\\nüìö Features:\")\n",
    "print(\"   ‚úÖ Config-driven (flexible)\")\n",
    "print(\"   ‚úÖ Reproducible (seeds, versioning)\")\n",
    "print(\"   ‚úÖ Organized (directories, logging)\")\n",
    "print(\"   ‚úÖ Easy to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Experiment Tracking\n",
    "\n",
    "### T·∫°i sao c·∫ßn Experiment Tracking?\n",
    "\n",
    "- ‚úÖ Track hyperparameters\n",
    "- ‚úÖ Compare experiments\n",
    "- ‚úÖ Reproduce results\n",
    "- ‚úÖ Share v·ªõi team\n",
    "\n",
    "### Tools ph·ªï bi·∫øn\n",
    "\n",
    "- **TensorBoard**: Built-in TensorFlow, free\n",
    "- **MLflow**: Open-source, full-featured\n",
    "- **Weights & Biases**: Cloud-based, powerful\n",
    "- **Neptune.ai**: Cloud-based\n",
    "\n",
    "### Simple Experiment Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentTracker:\n",
    "    \"\"\"Simple experiment tracker\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name, log_dir='./experiments'):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.experiment_dir = self.log_dir / experiment_name\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Experiment metadata\n",
    "        self.metadata = {\n",
    "            'name': experiment_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'config': {},\n",
    "            'metrics': {},\n",
    "            'artifacts': []\n",
    "        }\n",
    "    \n",
    "    def log_config(self, config):\n",
    "        \"\"\"Log configuration\"\"\"\n",
    "        self.metadata['config'] = config\n",
    "        self._save_metadata()\n",
    "    \n",
    "    def log_metric(self, name, value, step=None):\n",
    "        \"\"\"Log a metric\"\"\"\n",
    "        if name not in self.metadata['metrics']:\n",
    "            self.metadata['metrics'][name] = []\n",
    "        \n",
    "        self.metadata['metrics'][name].append({\n",
    "            'value': value,\n",
    "            'step': step,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self._save_metadata()\n",
    "    \n",
    "    def log_metrics(self, metrics_dict, step=None):\n",
    "        \"\"\"Log multiple metrics\"\"\"\n",
    "        for name, value in metrics_dict.items():\n",
    "            self.log_metric(name, value, step)\n",
    "    \n",
    "    def log_artifact(self, artifact_path, artifact_type='file'):\n",
    "        \"\"\"Log an artifact (model, plot, etc.)\"\"\"\n",
    "        self.metadata['artifacts'].append({\n",
    "            'path': str(artifact_path),\n",
    "            'type': artifact_type,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        self._save_metadata()\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save metadata to JSON\"\"\"\n",
    "        metadata_path = self.experiment_dir / 'metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2, default=str)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get experiment summary\"\"\"\n",
    "        summary = {\n",
    "            'name': self.metadata['name'],\n",
    "            'timestamp': self.metadata['timestamp'],\n",
    "            'config': self.metadata['config'],\n",
    "            'final_metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Get final metric values\n",
    "        for name, values in self.metadata['metrics'].items():\n",
    "            if values:\n",
    "                summary['final_metrics'][name] = values[-1]['value']\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_experiments(experiment_names, log_dir='./experiments'):\n",
    "        \"\"\"Compare multiple experiments\"\"\"\n",
    "        log_dir = Path(log_dir)\n",
    "        experiments = []\n",
    "        \n",
    "        for name in experiment_names:\n",
    "            metadata_path = log_dir / name / 'metadata.json'\n",
    "            if metadata_path.exists():\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                    experiments.append(metadata)\n",
    "        \n",
    "        return experiments\n",
    "\n",
    "# Example usage\n",
    "tracker = ExperimentTracker('exp_001')\n",
    "tracker.log_config({'learning_rate': 0.001, 'batch_size': 32})\n",
    "tracker.log_metrics({'train_loss': 0.5, 'val_loss': 0.6}, step=0)\n",
    "tracker.log_metrics({'train_loss': 0.3, 'val_loss': 0.4}, step=1)\n",
    "\n",
    "print(\"‚úÖ ExperimentTracker demo!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(tracker.get_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 2: REPRODUCIBILITY\n",
    "\n",
    "## 2.1 Reproducibility l√† g√¨?\n",
    "\n",
    "### ƒê·ªãnh nghƒ©a\n",
    "\n",
    "**Reproducibility** = Kh·∫£ nƒÉng t√°i t·∫°o l·∫°i k·∫øt qu·∫£ gi·ªëng nhau khi ch·∫°y l·∫°i code\n",
    "\n",
    "### T·∫°i sao quan tr·ªçng?\n",
    "\n",
    "| L√Ω do | Gi·∫£i th√≠ch |\n",
    "|-------|------------|\n",
    "| üî¨ **Research** | Validate k·∫øt qu·∫£ nghi√™n c·ª©u |\n",
    "| üêõ **Debugging** | D·ªÖ t√¨m l·ªói khi k·∫øt qu·∫£ consistent |\n",
    "| üë• **Collaboration** | Team c√≥ th·ªÉ reproduce |\n",
    "| üìä **Production** | ƒê·∫£m b·∫£o model deployment gi·ªëng training |\n",
    "| ‚úÖ **Trust** | TƒÉng ƒë·ªô tin c·∫≠y c·ªßa model |\n",
    "\n",
    "### C√°c y·∫øu t·ªë ·∫£nh h∆∞·ªüng Reproducibility\n",
    "\n",
    "1. **Random seeds** (NumPy, TensorFlow, Python)\n",
    "2. **Data order** (shuffle)\n",
    "3. **Hardware** (GPU, CPU)\n",
    "4. **Software versions** (TensorFlow, CUDA)\n",
    "5. **Environment** (dependencies)\n",
    "6. **Data versioning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Seeds\n",
    "\n",
    "### C√°c ngu·ªìn randomness trong ML\n",
    "\n",
    "1. **Python random**\n",
    "2. **NumPy random**\n",
    "3. **TensorFlow random**\n",
    "4. **PYTHONHASHSEED** (Python dict ordering)\n",
    "\n",
    "### C√°ch set seeds ƒë√∫ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set all random seeds for reproducibility\n",
    "    \n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy random\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # TensorFlow random\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Python hash seed (for dict ordering)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # Deterministic operations (TensorFlow 2.x)\n",
    "    # ‚ö†Ô∏è Warning: C√≥ th·ªÉ ch·∫≠m h∆°n ~10%\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    print(f\"‚úÖ All seeds set to {seed}\")\n",
    "    print(\"‚ö†Ô∏è  Deterministic mode enabled (may be slower)\")\n",
    "\n",
    "# Test reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Generate random numbers\n",
    "print(\"\\nTest reproducibility:\")\n",
    "print(f\"Python random: {random.random()}\")\n",
    "print(f\"NumPy random: {np.random.rand()}\")\n",
    "print(f\"TF random: {tf.random.normal([1]).numpy()}\")\n",
    "\n",
    "# Reset and generate again - should be same!\n",
    "set_seeds(42)\n",
    "print(\"\\nAfter reset (should be same):\")\n",
    "print(f\"Python random: {random.random()}\")\n",
    "print(f\"NumPy random: {np.random.rand()}\")\n",
    "print(f\"TF random: {tf.random.normal([1]).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Environment Management\n",
    "\n",
    "### T·∫°i sao c·∫ßn Environment Management?\n",
    "\n",
    "- ‚úÖ ƒê·∫£m b·∫£o software versions gi·ªëng nhau\n",
    "- ‚úÖ Tr√°nh dependency conflicts\n",
    "- ‚úÖ D·ªÖ share v·ªõi team\n",
    "\n",
    "### Tools\n",
    "\n",
    "- **pip + requirements.txt**: ƒê∆°n gi·∫£n\n",
    "- **conda**: Qu·∫£n l√Ω c·∫£ non-Python packages\n",
    "- **Docker**: Isolated environment\n",
    "- **Poetry**: Modern Python packaging\n",
    "\n",
    "### Best Practice: requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt\n",
    "requirements = \"\"\"\n",
    "# Core ML\n",
    "tensorflow==2.15.0\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "\n",
    "# Utilities\n",
    "pyyaml==6.0.1\n",
    "tqdm==4.66.1\n",
    "\n",
    "# Experiment tracking\n",
    "tensorboard==2.15.0\n",
    "mlflow==2.8.0  # Optional\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements.strip())\n",
    "\n",
    "print(\"‚úÖ requirements.txt created!\")\n",
    "print(\"\\nüìö C√°ch d√πng:\")\n",
    "print(\"   1. Install: pip install -r requirements.txt\")\n",
    "print(\"   2. Export: pip freeze > requirements.txt\")\n",
    "print(\"   3. Version specific: tensorflow==2.15.0\")\n",
    "print(\"   4. Version range: tensorflow>=2.13.0,<3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log environment info\n",
    "def log_environment_info(save_path='environment_info.json'):\n",
    "    \"\"\"\n",
    "    Log environment information for reproducibility\n",
    "    \"\"\"\n",
    "    import platform\n",
    "    import sys\n",
    "    \n",
    "    env_info = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'python_version': sys.version,\n",
    "        'platform': platform.platform(),\n",
    "        'tensorflow_version': tf.__version__,\n",
    "        'numpy_version': np.__version__,\n",
    "        'gpu_available': len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "        'cuda_version': tf.sysconfig.get_build_info().get('cuda_version', 'N/A'),\n",
    "        'cudnn_version': tf.sysconfig.get_build_info().get('cudnn_version', 'N/A')\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(env_info, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Environment info saved!\")\n",
    "    print(\"\\nüìã Environment:\")\n",
    "    for key, value in env_info.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return env_info\n",
    "\n",
    "# Log\n",
    "env_info = log_environment_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Reproducibility Checklist\n",
    "\n",
    "### ‚úÖ Before Training\n",
    "\n",
    "- [ ] Set all random seeds\n",
    "- [ ] Log environment info (Python, TensorFlow, CUDA versions)\n",
    "- [ ] Save config file\n",
    "- [ ] Version control code (Git)\n",
    "- [ ] Document data source & version\n",
    "\n",
    "### ‚úÖ During Training\n",
    "\n",
    "- [ ] Log hyperparameters\n",
    "- [ ] Save checkpoints\n",
    "- [ ] Log metrics to TensorBoard/MLflow\n",
    "- [ ] Track data preprocessing steps\n",
    "\n",
    "### ‚úÖ After Training\n",
    "\n",
    "- [ ] Save final model\n",
    "- [ ] Save training history\n",
    "- [ ] Document results\n",
    "- [ ] Archive experiment (code + config + model)\n",
    "\n",
    "### Template: Experiment Archive Structure\n",
    "\n",
    "```\n",
    "experiment_001/\n",
    "‚îú‚îÄ‚îÄ code/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model.py\n",
    "‚îú‚îÄ‚îÄ config.yaml\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ environment_info.json\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ data_version.txt\n",
    "‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ best_model.keras\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ final_model.keras\n",
    "‚îú‚îÄ‚îÄ logs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ training_history.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tensorboard/\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PH·∫¶N 3: MODEL EVALUATION & METRICS\n",
    "\n",
    "## 3.1 Classification Metrics\n",
    "\n",
    "### Binary Classification Metrics\n",
    "\n",
    "#### 1. Confusion Matrix\n",
    "\n",
    "```\n",
    "                 Predicted\n",
    "              Negative  Positive\n",
    "Actual  Neg      TN        FP\n",
    "        Pos      FN        TP\n",
    "```\n",
    "\n",
    "- **TP (True Positive)**: D·ª± ƒëo√°n Positive, th·ª±c t·∫ø Positive ‚úÖ\n",
    "- **TN (True Negative)**: D·ª± ƒëo√°n Negative, th·ª±c t·∫ø Negative ‚úÖ\n",
    "- **FP (False Positive)**: D·ª± ƒëo√°n Positive, th·ª±c t·∫ø Negative ‚ùå (Type I Error)\n",
    "- **FN (False Negative)**: D·ª± ƒëo√°n Negative, th·ª±c t·∫ø Positive ‚ùå (Type II Error)\n",
    "\n",
    "#### 2. Metrics t·ª´ Confusion Matrix\n",
    "\n",
    "| Metric | Formula | √ù nghƒ©a | Khi n√†o d√πng |\n",
    "|--------|---------|---------|---------------|\n",
    "| **Accuracy** | (TP+TN) / Total | T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng | Dataset c√¢n b·∫±ng |\n",
    "| **Precision** | TP / (TP+FP) | Trong c√°c d·ª± ƒëo√°n Positive, bao nhi√™u % ƒë√∫ng? | Minimize False Positive |\n",
    "| **Recall (Sensitivity)** | TP / (TP+FN) | Trong c√°c Positive th·ª±c t·∫ø, bao nhi√™u % ƒë∆∞·ª£c t√¨m th·∫•y? | Minimize False Negative |\n",
    "| **F1-Score** | 2 √ó (Precision √ó Recall) / (Precision + Recall) | Harmonic mean c·ªßa Precision v√† Recall | Dataset imbalanced |\n",
    "| **Specificity** | TN / (TN+FP) | Trong c√°c Negative th·ª±c t·∫ø, bao nhi√™u % ƒë∆∞·ª£c t√¨m th·∫•y? | Medical diagnosis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Binary Classification Evaluation\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ predictions v√† ground truth\n",
    "y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0, 0, 1, 1, 0, 0, 0, 1, 1, 1])  # C√≥ m·ªôt s·ªë sai\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"üìä CONFUSION MATRIX:\")\n",
    "print(cm)\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  TN (True Negative):  {tn}\")\n",
    "print(f\"  FP (False Positive): {fp}\")\n",
    "print(f\"  FN (False Negative): {fn}\")\n",
    "print(f\"  TP (True Positive):  {tp}\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nüìà METRICS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:    {accuracy:.4f}  ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision:   {precision:.4f}  ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:      {recall:.4f}  ({recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:    {f1:.4f}  ({f1*100:.2f}%)\")\n",
    "print(f\"Specificity: {specificity:.4f}  ({specificity*100:.2f}%)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, normalize=False):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix v·ªõi visualization ƒë·∫πp\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        class_names: Class names for labels\n",
    "        normalize: Normalize by row (recall)\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "        title = 'Normalized Confusion Matrix'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "        title = 'Confusion Matrix'\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count' if not normalize else 'Proportion'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot\n",
    "plot_confusion_matrix(y_true, y_pred, class_names=['Negative', 'Positive'])\n",
    "plot_confusion_matrix(y_true, y_pred, class_names=['Negative', 'Positive'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ROC Curve & AUC\n",
    "\n",
    "### ROC Curve l√† g√¨?\n",
    "\n",
    "**ROC (Receiver Operating Characteristic)** = ƒê·ªì th·ªã bi·ªÉu di·ªÖn trade-off gi·ªØa True Positive Rate v√† False Positive Rate ·ªü c√°c threshold kh√°c nhau\n",
    "\n",
    "- **X-axis**: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- **Y-axis**: True Positive Rate (TPR) = TP / (TP + FN) = Recall\n",
    "\n",
    "### AUC (Area Under Curve)\n",
    "\n",
    "- **AUC = 1.0**: Perfect classifier ‚úÖ\n",
    "- **AUC = 0.9-1.0**: Excellent\n",
    "- **AUC = 0.8-0.9**: Good\n",
    "- **AUC = 0.7-0.8**: Fair\n",
    "- **AUC = 0.5**: Random classifier\n",
    "- **AUC < 0.5**: Worse than random\n",
    "\n",
    "### Khi n√†o d√πng ROC & AUC?\n",
    "\n",
    "‚úÖ Binary classification\n",
    "‚úÖ Imbalanced dataset\n",
    "‚úÖ C·∫ßn ƒë√°nh gi√° t·ªïng qu√°t (kh√¥ng ph·ª• thu·ªôc threshold)\n",
    "‚úÖ So s√°nh nhi·ªÅu models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ROC Curve\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ probability predictions\n",
    "y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1])\n",
    "y_pred_proba = np.array([0.1, 0.2, 0.7, 0.8, 0.3, 0.6, 0.2, 0.9, 0.85, 0.4, 0.75, 0.82, 0.15, 0.25, 0.88])\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "if roc_auc >= 0.9:\n",
    "    print(\"‚úÖ Excellent classifier!\")\n",
    "elif roc_auc >= 0.8:\n",
    "    print(\"‚úÖ Good classifier\")\n",
    "elif roc_auc >= 0.7:\n",
    "    print(\"‚ö†Ô∏è  Fair classifier\")\n",
    "else:\n",
    "    print(\"‚ùå Poor classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-class Classification Metrics\n",
    "\n",
    "### Metrics cho Multi-class\n",
    "\n",
    "#### 1. Micro-average\n",
    "- T√≠nh t·ªïng TP, FP, FN t·ª´ t·∫•t c·∫£ classes\n",
    "- D√πng khi classes c√≥ size kh√°c nhau\n",
    "\n",
    "#### 2. Macro-average\n",
    "- T√≠nh metric cho t·ª´ng class, r·ªìi average\n",
    "- Treat all classes equally\n",
    "- D√πng khi mu·ªën classes c√≥ weight b·∫±ng nhau\n",
    "\n",
    "#### 3. Weighted-average\n",
    "- T√≠nh metric cho t·ª´ng class, weighted by support\n",
    "- D√πng khi imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-class Classification\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ 3 classes\n",
    "y_true_mc = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])\n",
    "y_pred_mc = np.array([0, 1, 2, 0, 2, 2, 0, 1, 1, 0, 1, 2, 0, 1, 2])  # C√≥ sai\n",
    "\n",
    "class_names = ['Class A', 'Class B', 'Class C']\n",
    "\n",
    "# Classification report\n",
    "print(\"üìä CLASSIFICATION REPORT:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_true_mc, y_pred_mc, target_names=class_names))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(y_true_mc, y_pred_mc, class_names=class_names)\n",
    "\n",
    "# Per-class metrics\n",
    "precision_per_class = precision_score(y_true_mc, y_pred_mc, average=None)\n",
    "recall_per_class = recall_score(y_true_mc, y_pred_mc, average=None)\n",
    "f1_per_class = f1_score(y_true_mc, y_pred_mc, average=None)\n",
    "\n",
    "print(\"\\nüìà PER-CLASS METRICS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "print(\"=\" * 70)\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"{name:<15} {precision_per_class[i]:>8.4f}    {recall_per_class[i]:>8.4f}    {f1_per_class[i]:>8.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Regression Metrics\n",
    "\n",
    "### Common Regression Metrics\n",
    "\n",
    "| Metric | Formula | √ù nghƒ©a | Khi n√†o d√πng |\n",
    "|--------|---------|---------|---------------|\n",
    "| **MAE** | mean(abs(y_true - y_pred)) | Average absolute error | Robust to outliers |\n",
    "| **MSE** | mean((y_true - y_pred)¬≤) | Average squared error | Penalize large errors |\n",
    "| **RMSE** | sqrt(MSE) | Root mean squared error | Same unit as target |\n",
    "| **R¬≤ Score** | 1 - (SS_res / SS_tot) | Proportion of variance explained | Model goodness of fit |\n",
    "| **MAPE** | mean(abs((y_true - y_pred) / y_true)) * 100 | Mean Absolute Percentage Error | When scale matters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Regression Evaluation\n",
    "\n",
    "# Gi·∫£ s·ª≠ c√≥ predictions\n",
    "y_true_reg = np.array([3.0, -0.5, 2.0, 7.0, 4.5, 2.5, 1.0, 6.0, 3.5, 4.0])\n",
    "y_pred_reg = np.array([2.5, 0.0, 2.1, 7.8, 4.0, 2.2, 1.5, 5.5, 3.8, 4.2])\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true_reg, y_pred_reg)\n",
    "\n",
    "# MAPE (careful with zero values!)\n",
    "mape = np.mean(np.abs((y_true_reg - y_pred_reg) / y_true_reg)) * 100\n",
    "\n",
    "print(\"üìä REGRESSION METRICS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MAE (Mean Absolute Error):       {mae:.4f}\")\n",
    "print(f\"MSE (Mean Squared Error):        {mse:.4f}\")\n",
    "print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}\")\n",
    "print(f\"R¬≤ Score:                        {r2:.4f}\")\n",
    "print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_true_reg, y_pred_reg, alpha=0.6, s=100, edgecolors='black')\n",
    "plt.plot([y_true_reg.min(), y_true_reg.max()], \n",
    "         [y_true_reg.min(), y_true_reg.max()], \n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('True Values', fontsize=12)\n",
    "plt.ylabel('Predictions', fontsize=12)\n",
    "plt.title('Predictions vs True Values', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_true_reg - y_pred_reg\n",
    "plt.scatter(y_pred_reg, residuals, alpha=0.6, s=100, edgecolors='black')\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Residuals', fontsize=12)\n",
    "plt.title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Gi·∫£i th√≠ch:\")\n",
    "print(\"   - Residual plot: N√™n random around 0\")\n",
    "print(\"   - Pattern trong residuals ‚Üí Model bias\")\n",
    "print(\"   - R¬≤ closer to 1 ‚Üí Better fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Cross-Validation\n",
    "\n",
    "### Cross-Validation l√† g√¨?\n",
    "\n",
    "**Cross-Validation** = Chia data th√†nh K folds, train tr√™n K-1 folds, test tr√™n 1 fold, l·∫∑p l·∫°i K l·∫ßn\n",
    "\n",
    "### T·∫°i sao c·∫ßn Cross-Validation?\n",
    "\n",
    "- ‚úÖ ƒê√°nh gi√° robust h∆°n single train/test split\n",
    "- ‚úÖ S·ª≠ d·ª•ng to√†n b·ªô data cho training\n",
    "- ‚úÖ Gi·∫£m variance trong evaluation\n",
    "- ‚úÖ T·ªët cho small datasets\n",
    "\n",
    "### C√°c lo·∫°i Cross-Validation\n",
    "\n",
    "- **K-Fold CV**: Chia th√†nh K folds ƒë·ªÅu nhau\n",
    "- **Stratified K-Fold**: Gi·ªØ nguy√™n t·ª∑ l·ªá classes\n",
    "- **Leave-One-Out**: K = N (m·ªói sample l√† 1 fold)\n",
    "- **Time Series CV**: Respect temporal order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: K-Fold Cross-Validation\n",
    "\n",
    "def cross_validate_model(model_fn, X, y, n_splits=5, stratified=True):\n",
    "    \"\"\"\n",
    "    Perform K-Fold cross-validation\n",
    "    \n",
    "    Args:\n",
    "        model_fn: Function that returns a compiled model\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        n_splits: Number of folds\n",
    "        stratified: Use stratified K-fold\n",
    "    \n",
    "    Returns:\n",
    "        scores: Dictionary of scores for each fold\n",
    "    \"\"\"\n",
    "    # Choose K-Fold strategy\n",
    "    if stratified:\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store scores\n",
    "    scores = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    print(f\"üîÑ Running {n_splits}-Fold Cross-Validation...\\n\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = model_fn()\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = (model.predict(X_val, verbose=0) > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        scores['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "        scores['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        scores['recall'].append(recall_score(y_val, y_pred, zero_division=0))\n",
    "        scores['f1'].append(f1_score(y_val, y_pred, zero_division=0))\n",
    "        \n",
    "        print(f\"  Accuracy: {scores['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Cross-Validation completed!\")\n",
    "    return scores\n",
    "\n",
    "# Example v·ªõi simple model\n",
    "def create_simple_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(16, activation='relu', input_shape=(10,)),\n",
    "        layers.Dense(8, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Fake data\n",
    "X_fake = np.random.rand(100, 10)\n",
    "y_fake = (np.random.rand(100) > 0.5).astype(int)\n",
    "\n",
    "# Run CV\n",
    "cv_scores = cross_validate_model(create_simple_model, X_fake, y_fake, n_splits=5)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä CROSS-VALIDATION RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "for metric, values in cv_scores.items():\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    print(f\"{metric.capitalize():<12} {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Model Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator:\n",
    "    \"\"\"Framework ƒë·ªÉ so s√°nh nhi·ªÅu models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def add_model(self, name, model):\n",
    "        \"\"\"Add model to comparison\"\"\"\n",
    "        self.models[name] = model\n",
    "    \n",
    "    def evaluate_all(self, X_test, y_test):\n",
    "        \"\"\"Evaluate all models\"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Evaluating {name}...\")\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test, verbose=0)\n",
    "            \n",
    "            # Binary or multi-class\n",
    "            if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "                y_pred_class = np.argmax(y_pred, axis=1)\n",
    "            else:\n",
    "                y_pred_class = (y_pred > 0.5).astype(int).flatten()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred_class),\n",
    "                'precision': precision_score(y_test, y_pred_class, average='weighted', zero_division=0),\n",
    "                'recall': recall_score(y_test, y_pred_class, average='weighted', zero_division=0),\n",
    "                'f1': f1_score(y_test, y_pred_class, average='weighted', zero_division=0)\n",
    "            }\n",
    "        \n",
    "        print(\"‚úÖ All models evaluated!\")\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print comparison table\"\"\"\n",
    "        print(\"\\nüìä MODEL COMPARISON:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for name, metrics in self.results.items():\n",
    "            print(f\"{name:<20} {metrics['accuracy']:>8.4f}    {metrics['precision']:>8.4f}    \"\n",
    "                  f\"{metrics['recall']:>8.4f}    {metrics['f1']:>8.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        \"\"\"Plot comparison bar chart\"\"\"\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        model_names = list(self.results.keys())\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.8 / len(model_names)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            values = [self.results[name][m] for m in metrics]\n",
    "            plt.bar(x + i * width, values, width, label=name, alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Metrics', fontsize=12)\n",
    "        plt.ylabel('Score', fontsize=12)\n",
    "        plt.title('Model Comparison', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(x + width * (len(model_names) - 1) / 2, \n",
    "                   [m.capitalize() for m in metrics])\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_best_model(self, metric='f1'):\n",
    "        \"\"\"Get best model by metric\"\"\"\n",
    "        best_name = max(self.results, key=lambda x: self.results[x][metric])\n",
    "        best_score = self.results[best_name][metric]\n",
    "        return best_name, best_score\n",
    "\n",
    "print(\"‚úÖ ModelComparator class defined!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  comparator = ModelComparator()\")\n",
    "print(\"  comparator.add_model('Model A', model_a)\")\n",
    "print(\"  comparator.add_model('Model B', model_b)\")\n",
    "print(\"  comparator.evaluate_all(X_test, y_test)\")\n",
    "print(\"  comparator.print_comparison()\")\n",
    "print(\"  comparator.plot_comparison()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì T·ªïng k·∫øt FILE 3-B\n",
    "\n",
    "## ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "\n",
    "### 1. Clean ML Pipeline\n",
    "- **Config Management**: YAML, Python class\n",
    "- **Data Pipeline**: Modular, reproducible\n",
    "- **Training Pipeline**: Config-driven, organized\n",
    "- **Experiment Tracking**: Log everything!\n",
    "\n",
    "### 2. Reproducibility\n",
    "- **Random Seeds**: Set all seeds (Python, NumPy, TensorFlow)\n",
    "- **Environment**: requirements.txt, environment info\n",
    "- **Version Control**: Git, data versioning\n",
    "- **Checklist**: Before/During/After training\n",
    "\n",
    "### 3. Model Evaluation\n",
    "- **Classification Metrics**: Accuracy, Precision, Recall, F1\n",
    "- **Confusion Matrix**: Visualize errors\n",
    "- **ROC & AUC**: Threshold-independent evaluation\n",
    "- **Regression Metrics**: MAE, MSE, RMSE, R¬≤\n",
    "- **Cross-Validation**: Robust evaluation\n",
    "- **Model Comparison**: Framework ƒë·ªÉ so s√°nh models\n",
    "\n",
    "## üöÄ Key Takeaways\n",
    "\n",
    "1. **Clean code** = D·ªÖ maintain, scale, collaborate\n",
    "2. **Config-driven** = Flexible, reproducible\n",
    "3. **Reproducibility** = Must-have cho production\n",
    "4. **Right metrics** = Quan tr·ªçng h∆°n high accuracy\n",
    "5. **Cross-validation** = Robust evaluation cho small data\n",
    "\n",
    "## üìù Best Practices Summary\n",
    "\n",
    "### DO ‚úÖ\n",
    "- Use config files (YAML khuy·∫øn ngh·ªã)\n",
    "- Set all random seeds\n",
    "- Log everything (hyperparams, metrics, environment)\n",
    "- Version control code + config\n",
    "- Choose metrics ph√π h·ª£p v·ªõi b√†i to√°n\n",
    "- Use cross-validation cho small data\n",
    "- Document thoroughly\n",
    "\n",
    "### DON'T ‚ùå\n",
    "- Magic numbers trong code\n",
    "- Qu√™n set seeds\n",
    "- Ch·ªâ nh√¨n accuracy (ƒë·∫∑c bi·ªát imbalanced data)\n",
    "- Test tr√™n training data\n",
    "- Kh√¥ng track experiments\n",
    "- Kh√¥ng document\n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "File ti·∫øp theo (**3-C**) s·∫Ω h·ªçc:\n",
    "- Inference Pipeline\n",
    "- Save & Load Models\n",
    "- Model Deployment\n",
    "- Performance Optimization\n",
    "- Production Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c m·ª´ng b·∫°n ƒë√£ ho√†n th√†nh FILE 3-B! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
