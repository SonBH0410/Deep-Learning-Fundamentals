{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò FILE 1-B ‚Äì Eager Execution & GradientTape\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "\n",
    "Sau b√†i n√†y b·∫°n s·∫Ω hi·ªÉu:\n",
    "- **Eager Execution** l√† g√¨ v√† t·∫°i sao n√≥ quan tr·ªçng\n",
    "- **Gradient** l√† g√¨ (theo c√°ch intuitively)\n",
    "- C√°ch d√πng `tf.GradientTape` ƒë·ªÉ t√≠nh gradient\n",
    "- C√°ch train model **KH√îNG d√πng `model.fit()`** (training loop th·ªß c√¥ng)\n",
    "- Implement Linear Regression t·ª´ ƒë·∫ßu\n",
    "\n",
    "---\n",
    "\n",
    "## üìå T·∫°i sao ph·∫£i h·ªçc GradientTape?\n",
    "\n",
    "- `model.fit()` r·∫•t ti·ªán nh∆∞ng **che gi·∫•u c∆° ch·∫ø b√™n trong**\n",
    "- Mu·ªën custom training loop ‚Üí **b·∫Øt bu·ªôc d√πng GradientTape**\n",
    "- Hi·ªÉu GradientTape ‚Üí hi·ªÉu c√°ch Deep Learning ho·∫°t ƒë·ªông\n",
    "- Production/MLOps th∆∞·ªùng c·∫ßn custom logic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Eager Execution l√† g√¨?\n",
    "\n",
    "### üîπ TensorFlow 1.x (Graph Mode - c≈©)\n",
    "\n",
    "```python\n",
    "# TensorFlow 1.x\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = x * 2\n",
    "sess = tf.Session()\n",
    "result = sess.run(y, feed_dict={x: 3.0})\n",
    "```\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:**\n",
    "- Ph·∫£i build graph tr∆∞·ªõc\n",
    "- Ph·∫£i d√πng Session\n",
    "- **Kh√≥ debug**\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ TensorFlow 2.x (Eager Execution - m·∫∑c ƒë·ªãnh)\n",
    "\n",
    "```python\n",
    "# TensorFlow 2.x\n",
    "x = tf.constant(3.0)\n",
    "y = x * 2\n",
    "print(y)  # tf.Tensor(6.0, shape=(), dtype=float32)\n",
    "```\n",
    "\n",
    "**L·ª£i √≠ch:**\n",
    "- Code ch·∫°y **ngay l·∫≠p t·ª©c** (nh∆∞ NumPy)\n",
    "- D·ªÖ debug\n",
    "- Pythonic h∆°n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra Eager Execution\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")  # True trong TF 2.x\n",
    "\n",
    "# V√≠ d·ª• ƒë∆°n gi·∫£n\n",
    "x = tf.constant([1, 2, 3])\n",
    "y = x * 2\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"y.numpy() = {y.numpy()}\")  # Chuy·ªÉn v·ªÅ NumPy ngay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Gradient l√† g√¨? (Intuitively)\n",
    "\n",
    "### üîπ ƒê·ªãnh nghƒ©a ƒë∆°n gi·∫£n\n",
    "\n",
    "**Gradient** = ƒê·ªô d·ªëc (slope) c·ªßa h√†m s·ªë\n",
    "\n",
    "**V√≠ d·ª•:**\n",
    "\n",
    "```\n",
    "f(x) = x¬≤\n",
    "```\n",
    "\n",
    "- T·∫°i `x = 2`: gradient = `2x = 4`\n",
    "- T·∫°i `x = -3`: gradient = `2x = -6`\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn gradient?\n",
    "\n",
    "**Gradient Descent** = Thu·∫≠t to√°n t·ªëi ∆∞u d·ª±a tr√™n gradient\n",
    "\n",
    "```\n",
    "C√¥ng th·ª©c:\n",
    "w_new = w_old - learning_rate √ó gradient\n",
    "```\n",
    "\n",
    "- Gradient **d∆∞∆°ng** ‚Üí gi·∫£m `w`\n",
    "- Gradient **√¢m** ‚Üí tƒÉng `w`\n",
    "- M·ª•c ti√™u: **t√¨m ƒëi·ªÉm c√≥ loss nh·ªè nh·∫•t**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: f(x) = x¬≤\n",
    "x_vals = np.linspace(-5, 5, 100)\n",
    "y_vals = x_vals ** 2\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot h√†m s·ªë\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
    "plt.scatter([2], [4], color='red', s=100, zorder=5)\n",
    "plt.title('f(x) = x¬≤')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot gradient\n",
    "plt.subplot(1, 2, 2)\n",
    "gradient_vals = 2 * x_vals  # df/dx = 2x\n",
    "plt.plot(x_vals, gradient_vals, 'g-', linewidth=2)\n",
    "plt.scatter([2], [4], color='red', s=100, zorder=5)\n",
    "plt.title('Gradient: df/dx = 2x')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('gradient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"T·∫°i x=2: gradient = 4 (d·ªëc l√™n)\")\n",
    "print(\"T·∫°i x=-3: gradient = -6 (d·ªëc xu·ªëng)\")\n",
    "print(\"T·∫°i x=0: gradient = 0 (ƒëi·ªÉm c·ª±c ti·ªÉu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ tf.GradientTape - C√°ch d√πng c∆° b·∫£n\n",
    "\n",
    "### üîπ C√∫ ph√°p c∆° b·∫£n\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    # 1. Watch variables (n·∫øu c·∫ßn)\n",
    "    tape.watch(x)\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    y = f(x)\n",
    "\n",
    "# 3. T√≠nh gradient\n",
    "grad = tape.gradient(y, x)\n",
    "```\n",
    "\n",
    "### üîπ Quy t·∫Øc quan tr·ªçng\n",
    "\n",
    "1. **Ch·ªâ watch `tf.Variable`** (m·∫∑c ƒë·ªãnh) ho·∫∑c `tape.watch()` cho tensor th∆∞·ªùng\n",
    "2. **Gradient ch·ªâ t√≠nh 1 l·∫ßn** (tape b·ªã x√≥a sau `gradient()`)\n",
    "3. **persistent=True** n·∫øu mu·ªën t√≠nh nhi·ªÅu l·∫ßn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 1: T√≠nh gradient c·ªßa f(x) = x¬≤\n",
    "x = tf.Variable(2.0)  # tf.Variable t·ª± ƒë·ªông ƒë∆∞·ª£c watch\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2  # f(x) = x¬≤\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = x¬≤ = {y.numpy()}\")\n",
    "print(f\"dy/dx = 2x = {grad.numpy()}\")\n",
    "print(f\"Ki·ªÉm tra: 2 √ó {x.numpy()} = {2 * x.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 2: Watch tensor th∆∞·ªùng\n",
    "x = tf.constant(3.0)  # Tensor th∆∞·ªùng\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # B·∫ÆT BU·ªòC watch n·∫øu kh√¥ng ph·∫£i Variable\n",
    "    y = x ** 3  # f(x) = x¬≥\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"y = x¬≥ = {y.numpy()}\")\n",
    "print(f\"dy/dx = 3x¬≤ = {grad.numpy()}\")\n",
    "print(f\"Ki·ªÉm tra: 3 √ó {x.numpy()}¬≤ = {3 * x.numpy() ** 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 3: Gradient c·ªßa nhi·ªÅu bi·∫øn\n",
    "w = tf.Variable(2.0)\n",
    "b = tf.Variable(1.0)\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = w * x + b  # y = wx + b\n",
    "\n",
    "# T√≠nh gradient theo w v√† b\n",
    "grad_w, grad_b = tape.gradient(y, [w, b])\n",
    "\n",
    "print(f\"y = {w.numpy()} √ó {x.numpy()} + {b.numpy()} = {y.numpy()}\")\n",
    "print(f\"dy/dw = x = {grad_w.numpy()}\")\n",
    "print(f\"dy/db = 1 = {grad_b.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 4: persistent=True\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x ** 2\n",
    "    z = x ** 3\n",
    "\n",
    "# T√≠nh nhi·ªÅu gradient t·ª´ c√πng 1 tape\n",
    "grad_y = tape.gradient(y, x)\n",
    "grad_z = tape.gradient(z, x)\n",
    "\n",
    "print(f\"dy/dx = {grad_y.numpy()}\")\n",
    "print(f\"dz/dx = {grad_z.numpy()}\")\n",
    "\n",
    "# QUAN TR·ªåNG: X√≥a tape ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Training Loop Th·ªß C√¥ng\n",
    "\n",
    "### üîπ Quy tr√¨nh c∆° b·∫£n\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. Forward pass\n",
    "        predictions = model(X)\n",
    "        loss = loss_function(y_true, predictions)\n",
    "    \n",
    "    # 2. Backward pass (t√≠nh gradient)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 3. Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• ƒë∆°n gi·∫£n: T·ªëi ∆∞u f(x) = (x - 3)¬≤\n",
    "# M·ª•c ti√™u: t√¨m x sao cho f(x) nh·ªè nh·∫•t (x = 3)\n",
    "\n",
    "x = tf.Variable(0.0)  # Kh·ªüi t·∫°o x = 0\n",
    "learning_rate = 0.1\n",
    "\n",
    "history = []\n",
    "\n",
    "for step in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = (x - 3) ** 2  # f(x) = (x - 3)¬≤\n",
    "    \n",
    "    # T√≠nh gradient\n",
    "    grad = tape.gradient(loss, x)\n",
    "    \n",
    "    # Update x\n",
    "    x.assign(x - learning_rate * grad)\n",
    "    \n",
    "    history.append((step, x.numpy(), loss.numpy()))\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:2d}: x = {x.numpy():.4f}, loss = {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\nK·∫øt qu·∫£ cu·ªëi: x = {x.numpy():.4f} (mong mu·ªën: 3.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "steps, x_vals, loss_vals = zip(*history)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, x_vals, 'b-', linewidth=2)\n",
    "plt.axhline(y=3, color='r', linestyle='--', label='Target (x=3)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x')\n",
    "plt.title('x convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, loss_vals, 'g-', linewidth=2)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Linear Regression t·ª´ ƒë·∫ßu\n",
    "\n",
    "### üîπ B√†i to√°n\n",
    "\n",
    "Cho d·ªØ li·ªáu `(x, y)` th·ªèa m√£n:\n",
    "\n",
    "```\n",
    "y = 2x + 1 + noise\n",
    "```\n",
    "\n",
    "T√¨m `w` v√† `b` sao cho:\n",
    "\n",
    "```\n",
    "y_pred = w * x + b\n",
    "```\n",
    "\n",
    "### üîπ Loss function\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "\n",
    "```\n",
    "loss = mean((y_true - y_pred)¬≤)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o d·ªØ li·ªáu gi·∫£\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters\n",
    "true_w = 2.0\n",
    "true_b = 1.0\n",
    "\n",
    "# Generate data\n",
    "num_samples = 100\n",
    "X = np.random.randn(num_samples, 1).astype(np.float32)\n",
    "y = true_w * X + true_b + np.random.randn(num_samples, 1).astype(np.float32) * 0.5\n",
    "\n",
    "# Convert to TensorFlow\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Training data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o parameters\n",
    "w = tf.Variable(tf.random.normal([1, 1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "print(f\"Initial w: {w.numpy().flatten()}\")\n",
    "print(f\"Initial b: {b.numpy().flatten()}\")\n",
    "print(f\"\\nTrue values: w={true_w}, b={true_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. Forward pass\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        \n",
    "        # 2. Compute loss (MSE)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "    \n",
    "    # 3. Backward pass\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "    \n",
    "    # 4. Update parameters\n",
    "    w.assign(w - learning_rate * gradients[0])\n",
    "    b.assign(b - learning_rate * gradients[1])\n",
    "    \n",
    "    loss_history.append(loss.numpy())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: loss = {loss.numpy():.4f}, \"\n",
    "              f\"w = {w.numpy().flatten()[0]:.4f}, b = {b.numpy()[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final results:\")\n",
    "print(f\"  Learned: w = {w.numpy().flatten()[0]:.4f}, b = {b.numpy()[0]:.4f}\")\n",
    "print(f\"  True:    w = {true_w:.4f}, b = {true_b:.4f}\")\n",
    "print(f\"  Error:   w = {abs(w.numpy().flatten()[0] - true_w):.4f}, \"\n",
    "      f\"b = {abs(b.numpy()[0] - true_b):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5, label='Data')\n",
    "\n",
    "# Learned line\n",
    "x_line = np.linspace(X.numpy().min(), X.numpy().max(), 100).reshape(-1, 1)\n",
    "y_line = w.numpy() * x_line + b.numpy()\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, label='Learned')\n",
    "\n",
    "# True line\n",
    "y_true_line = true_w * x_line + true_b\n",
    "plt.plot(x_line, y_true_line, 'g--', linewidth=2, label='True')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ S·ª≠ d·ª•ng Optimizer\n",
    "\n",
    "### üîπ V·∫•n ƒë·ªÅ v·ªõi manual update\n",
    "\n",
    "```python\n",
    "w.assign(w - learning_rate * grad)  # Ph·∫£i t·ª± implement\n",
    "```\n",
    "\n",
    "### üîπ Gi·∫£i ph√°p: tf.keras.optimizers\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "optimizer.apply_gradients(zip(gradients, variables))\n",
    "```\n",
    "\n",
    "**L·ª£i √≠ch:**\n",
    "- Code ng·∫Øn g·ªçn h∆°n\n",
    "- D·ªÖ thay ƒë·ªïi optimizer (Adam, RMSprop...)\n",
    "- Support advanced features (momentum, decay...)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression v·ªõi Optimizer\n",
    "# Reset parameters\n",
    "w = tf.Variable(tf.random.normal([1, 1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "# T·∫°o optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "loss_history_opt = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "    \n",
    "    # T√≠nh gradient\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "    \n",
    "    # Update b·∫±ng optimizer (NG·∫ÆN G·ªåN H∆†N!)\n",
    "    optimizer.apply_gradients(zip(gradients, [w, b]))\n",
    "    \n",
    "    loss_history_opt.append(loss.numpy())\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: loss = {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: w = {w.numpy().flatten()[0]:.4f}, b = {b.numpy()[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Common Mistakes\n",
    "\n",
    "### ‚ùå Mistake 1: Qu√™n watch tensor th∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAI\n",
    "x = tf.constant(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # QU√äN tape.watch(x)\n",
    "    y = x ** 2\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(f\"Gradient: {grad}\")  # None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√öNG\n",
    "x = tf.constant(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # B·∫ÆT BU·ªòC!\n",
    "    y = x ** 2\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "print(f\"Gradient: {grad.numpy()}\")  # 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Mistake 2: D√πng tape 2 l·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAI\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2\n",
    "\n",
    "grad1 = tape.gradient(y, x)\n",
    "print(f\"Grad 1: {grad1.numpy()}\")\n",
    "\n",
    "# L·ªñI: tape ƒë√£ b·ªã x√≥a\n",
    "try:\n",
    "    grad2 = tape.gradient(y, x)\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√öNG\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x ** 2\n",
    "\n",
    "grad1 = tape.gradient(y, x)\n",
    "grad2 = tape.gradient(y, x)\n",
    "print(f\"Grad 1: {grad1.numpy()}\")\n",
    "print(f\"Grad 2: {grad2.numpy()}\")\n",
    "\n",
    "del tape  # Nh·ªõ x√≥a!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Mistake 3: Qu√™n convert NumPy ‚Üí TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAI\n",
    "w = tf.Variable(2.0)\n",
    "x = np.array([1.0, 2.0, 3.0])  # NumPy array\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # C·∫¢NH B√ÅO: Mixing NumPy and TensorFlow\n",
    "    y = w * x  # Ho·∫°t ƒë·ªông nh∆∞ng kh√¥ng t·ªëi ∆∞u\n",
    "\n",
    "grad = tape.gradient(y, w)\n",
    "print(f\"Gradient: {grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√öNG\n",
    "w = tf.Variable(2.0)\n",
    "x = tf.constant([1.0, 2.0, 3.0])  # TensorFlow tensor\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = w * x\n",
    "\n",
    "grad = tape.gradient(y, w)\n",
    "print(f\"Gradient: {grad.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Best Practices\n",
    "\n",
    "### ‚úÖ 1. Lu√¥n d√πng tf.Variable cho trainable parameters\n",
    "\n",
    "```python\n",
    "# ƒê√öNG\n",
    "w = tf.Variable(initial_value)\n",
    "\n",
    "# SAI (kh√¥ng trainable)\n",
    "w = tf.constant(initial_value)\n",
    "```\n",
    "\n",
    "### ‚úÖ 2. D√πng optimizer thay v√¨ manual update\n",
    "\n",
    "```python\n",
    "# T·ªêT\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "optimizer.apply_gradients(zip(grads, vars))\n",
    "\n",
    "# T·ªÜ (kh√≥ maintain)\n",
    "w.assign(w - lr * grad)\n",
    "```\n",
    "\n",
    "### ‚úÖ 3. Ki·ªÉm tra gradient\n",
    "\n",
    "```python\n",
    "if grad is None:\n",
    "    print(\"WARNING: Gradient is None!\")\n",
    "```\n",
    "\n",
    "### ‚úÖ 4. X√≥a persistent tape\n",
    "\n",
    "```python\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    ...\n",
    "\n",
    "# T√≠nh gradient\n",
    "...\n",
    "\n",
    "# QUAN TR·ªåNG\n",
    "del tape\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Exercises\n",
    "\n",
    "### üìù Exercise 1: Gradient c·ªßa h√†m ph·ª©c t·∫°p\n",
    "\n",
    "T√≠nh gradient c·ªßa:\n",
    "\n",
    "```\n",
    "f(x) = 3x¬≥ - 2x¬≤ + 5x - 1\n",
    "```\n",
    "\n",
    "t·∫°i `x = 2`\n",
    "\n",
    "**Hint:** df/dx = 9x¬≤ - 4x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # TODO: Implement f(x) = 3x¬≥ - 2x¬≤ + 5x - 1\n",
    "    pass\n",
    "\n",
    "# TODO: Compute gradient\n",
    "# Expected: 9(4) - 4(2) + 5 = 36 - 8 + 5 = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Polynomial Regression\n",
    "\n",
    "Implement polynomial regression:\n",
    "\n",
    "```\n",
    "y = w2*x¬≤ + w1*x + b\n",
    "```\n",
    "\n",
    "Cho d·ªØ li·ªáu:\n",
    "\n",
    "```python\n",
    "X = [-2, -1, 0, 1, 2]\n",
    "y = [6, 2, 0, 0, 2]  # y ‚âà x¬≤ - x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_data = tf.constant([[-2.], [-1.], [0.], [1.], [2.]], dtype=tf.float32)\n",
    "y_data = tf.constant([[6.], [2.], [0.], [0.], [2.]], dtype=tf.float32)\n",
    "\n",
    "# TODO: Kh·ªüi t·∫°o w2, w1, b\n",
    "# TODO: Training loop\n",
    "# TODO: Plot k·∫øt qu·∫£\n",
    "\n",
    "# Expected: w2 ‚âà 1, w1 ‚âà -1, b ‚âà 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Logistic Regression (BONUS)\n",
    "\n",
    "Implement binary classification:\n",
    "\n",
    "```python\n",
    "z = w*x + b\n",
    "y_pred = sigmoid(z)\n",
    "loss = binary_crossentropy(y_true, y_pred)\n",
    "```\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "```python\n",
    "sigmoid(z) = 1 / (1 + exp(-z))\n",
    "binary_crossentropy = -mean(y*log(y_pred) + (1-y)*log(1-y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Generate binary classification data\n",
    "# TODO: Implement logistic regression\n",
    "# TODO: Plot decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ T√≥m t·∫Øt\n",
    "\n",
    "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "\n",
    "1. **Eager Execution**: Code ch·∫°y ngay l·∫≠p t·ª©c (nh∆∞ NumPy)\n",
    "2. **Gradient**: ƒê·ªô d·ªëc c·ªßa h√†m s·ªë, d√πng ƒë·ªÉ t·ªëi ∆∞u\n",
    "3. **tf.GradientTape**: Tool ƒë·ªÉ t√≠nh gradient t·ª± ƒë·ªông\n",
    "4. **Training Loop**: Forward ‚Üí Backward ‚Üí Update\n",
    "5. **Linear Regression**: Implement t·ª´ ƒë·∫ßu kh√¥ng d√πng `model.fit()`\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "```python\n",
    "# Pattern chu·∫©n\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(X)\n",
    "    loss = loss_fn(y_true, predictions)\n",
    "\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **File 1-C**: tf.keras & First Neural Network\n",
    "- **File 1-D**: GPU, Debugging & Exercises\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "- [TensorFlow GradientTape Guide](https://www.tensorflow.org/guide/autodiff)\n",
    "- [TensorFlow Eager Execution](https://www.tensorflow.org/guide/eager)\n",
    "- [Custom Training Walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough)\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c b·∫°n h·ªçc t·ªët! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
