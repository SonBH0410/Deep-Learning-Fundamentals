{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìó FILE 2-B ‚Äì Optimizer, Activation & Regularization\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "\n",
    "Sau b√†i n√†y b·∫°n s·∫Ω hi·ªÉu:\n",
    "- **Activation functions** n√¢ng cao (ReLU variants, GELU)\n",
    "- **Optimizers** - SGD, Adam, AdamW v√† c√°ch ch·ªçn\n",
    "- **Learning rate** - Scheduling strategies\n",
    "- **Regularization** - Dropout, L2 ƒë·ªÉ ch·ªëng overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## üìå T·∫°i sao quan tr·ªçng?\n",
    "\n",
    "- **Activation** ‚Üí Quy·∫øt ƒë·ªãnh model c√≥ h·ªçc ƒë∆∞·ª£c non-linear patterns\n",
    "- **Optimizer** ‚Üí Quy·∫øt ƒë·ªãnh training speed & final performance\n",
    "- **Learning rate** ‚Üí Too high: kh√¥ng converge, too low: qu√° ch·∫≠m\n",
    "- **Regularization** ‚Üí Quy·∫øt ƒë·ªãnh model c√≥ generalize t·ªët kh√¥ng\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Activation Functions - Deep Dive\n",
    "\n",
    "### üîπ ReLU Family\n",
    "\n",
    "**1. ReLU (Rectified Linear Unit)**\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "**2. Leaky ReLU**\n",
    "```\n",
    "f(x) = max(0.01x, x)\n",
    "```\n",
    "\n",
    "**3. PReLU (Parametric ReLU)**\n",
    "```\n",
    "f(x) = max(Œ±x, x)  # Œ± is learnable\n",
    "```\n",
    "\n",
    "**4. ELU (Exponential Linear Unit)**\n",
    "```\n",
    "f(x) = x if x > 0 else Œ±(e^x - 1)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU family\n",
    "x = np.linspace(-3, 3, 200)\n",
    "\n",
    "relu = np.maximum(0, x)\n",
    "leaky_relu = np.where(x > 0, x, 0.01 * x)\n",
    "elu = np.where(x > 0, x, 1.0 * (np.exp(x) - 1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "axes[0].plot(x, relu, 'b-', linewidth=2)\n",
    "axes[0].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].set_title('ReLU: max(0, x)')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('f(x)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[1].plot(x, leaky_relu, 'g-', linewidth=2)\n",
    "axes[1].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('Leaky ReLU: max(0.01x, x)')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('f(x)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ELU\n",
    "axes[2].plot(x, elu, 'r-', linewidth=2)\n",
    "axes[2].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[2].set_title('ELU: x if x>0 else Œ±(e^x-1)')\n",
    "axes[2].set_xlabel('x')\n",
    "axes[2].set_ylabel('f(x)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"- ReLU: Dead neurons problem (x<0 ‚Üí gradient=0)\")\n",
    "print(\"- Leaky ReLU: Small gradient when x<0 ‚Üí gi·∫£i quy·∫øt dying ReLU\")\n",
    "print(\"- ELU: Smooth, mean closer to 0 ‚Üí faster convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "GELU(x) = x * Œ¶(x)\n",
    "```\n",
    "Trong ƒë√≥ Œ¶(x) l√† CDF c·ªßa normal distribution\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- Smooth, non-monotonic\n",
    "- ƒê∆∞·ª£c d√πng trong **Transformers** (BERT, GPT)\n",
    "- T·ªët h∆°n ReLU cho NLP tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GELU implementation\n",
    "def gelu(x):\n",
    "    \"\"\"GELU activation\"\"\"\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "# Compare with ReLU\n",
    "x_vals = np.linspace(-3, 3, 200)\n",
    "x_tf = tf.constant(x_vals, dtype=tf.float32)\n",
    "\n",
    "relu_vals = tf.nn.relu(x_tf).numpy()\n",
    "gelu_vals = gelu(x_tf).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, relu_vals, label='ReLU', linewidth=2)\n",
    "plt.plot(x_vals, gelu_vals, label='GELU', linewidth=2)\n",
    "plt.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('ReLU vs GELU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"GELU advantages:\")\n",
    "print(\"- Smooth (differentiable everywhere)\")\n",
    "print(\"- Non-monotonic (c√≥ th·ªÉ gi·∫£m khi x<0)\")\n",
    "print(\"- State-of-the-art cho Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Khi n√†o d√πng activation n√†o?\n",
    "\n",
    "| Task | Hidden Layers | Output Layer |\n",
    "|------|---------------|-------------|\n",
    "| **Computer Vision** | ReLU, Leaky ReLU | Softmax |\n",
    "| **NLP (Transformers)** | GELU | Softmax |\n",
    "| **Simple MLP** | ReLU | Depends on task |\n",
    "| **Deep Networks** | ELU, SELU | - |\n",
    "\n",
    "**Rule of thumb:**\n",
    "- Start with **ReLU** (default)\n",
    "- Try **Leaky ReLU** if dying ReLU problem\n",
    "- Try **GELU** for Transformers/NLP\n",
    "- Try **ELU** for very deep networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Compare activations on real data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def build_model(activation='relu'):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation, input_shape=(20,)),\n",
    "        tf.keras.layers.Dense(32, activation=activation),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "activations = ['relu', 'elu', tf.keras.layers.LeakyReLU(alpha=0.01)]\n",
    "activation_names = ['ReLU', 'ELU', 'LeakyReLU']\n",
    "histories = []\n",
    "\n",
    "for act, name in zip(activations, activation_names):\n",
    "    print(f\"Training with {name}...\")\n",
    "    model = build_model(act)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    hist = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "    histories.append(hist)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for hist, name in zip(histories, activation_names):\n",
    "    plt.plot(hist.history['loss'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for hist, name in zip(histories, activation_names):\n",
    "    plt.plot(hist.history['val_accuracy'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Optimizers - Thu·∫≠t to√°n t·ªëi ∆∞u\n",
    "\n",
    "### üîπ Gradient Descent c∆° b·∫£n\n",
    "\n",
    "**Vanilla Gradient Descent:**\n",
    "```\n",
    "w = w - learning_rate √ó gradient\n",
    "```\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:**\n",
    "- Learning rate c·ªë ƒë·ªãnh ‚Üí kh√¥ng linh ho·∫°t\n",
    "- Ch·∫≠m v·ªõi saddle points\n",
    "- Kh√¥ng adaptive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 1. SGD (Stochastic Gradient Descent)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "w = w - learning_rate √ó gradient\n",
    "```\n",
    "\n",
    "**With momentum:**\n",
    "```\n",
    "v = momentum √ó v - learning_rate √ó gradient\n",
    "w = w + v\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- ‚úÖ Simple, stable\n",
    "- ‚úÖ Good for convex problems\n",
    "- ‚ùå Slow convergence\n",
    "- ‚ùå C·∫ßn tune learning rate carefully\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- Simple problems\n",
    "- Mu·ªën stability > speed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD demo\n",
    "model_sgd = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer_sgd = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9  # Momentum helps escape local minima\n",
    ")\n",
    "\n",
    "model_sgd.compile(optimizer=optimizer_sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist_sgd = model_sgd.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "print(f\"Final val accuracy: {hist_sgd.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 2. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "m = Œ≤1 √ó m + (1-Œ≤1) √ó gradient       # 1st moment (mean)\n",
    "v = Œ≤2 √ó v + (1-Œ≤2) √ó gradient¬≤      # 2nd moment (variance)\n",
    "m_hat = m / (1 - Œ≤1^t)               # Bias correction\n",
    "v_hat = v / (1 - Œ≤2^t)\n",
    "w = w - learning_rate √ó m_hat / (‚àöv_hat + Œµ)\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- ‚úÖ Adaptive learning rate per parameter\n",
    "- ‚úÖ Fast convergence\n",
    "- ‚úÖ Works well out-of-the-box\n",
    "- ‚ùå C√≥ th·ªÉ generalize k√©m h∆°n SGD\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `learning_rate`: 0.001 (default)\n",
    "- `beta_1`: 0.9 (momentum)\n",
    "- `beta_2`: 0.999 (variance)\n",
    "- `epsilon`: 1e-7 (numerical stability)\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- **Default choice** cho most problems\n",
    "- Mu·ªën convergence nhanh\n",
    "- Sparse gradients (NLP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam demo\n",
    "model_adam = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer_adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,  # Default\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999\n",
    ")\n",
    "\n",
    "model_adam.compile(optimizer=optimizer_adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist_adam = model_adam.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "print(f\"Final val accuracy: {hist_adam.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 3. AdamW (Adam with Weight Decay)\n",
    "\n",
    "**Kh√°c bi·ªát v·ªõi Adam:**\n",
    "```\n",
    "Adam:  w = w - lr √ó m_hat / ‚àöv_hat\n",
    "AdamW: w = w - lr √ó m_hat / ‚àöv_hat - lr √ó Œª √ó w  # Weight decay\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- ‚úÖ Better generalization than Adam\n",
    "- ‚úÖ Decoupled weight decay\n",
    "- ‚úÖ State-of-the-art cho Transformers\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- Large models (Transformers, Vision Transformers)\n",
    "- Mu·ªën generalization t·ªët h∆°n Adam\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW demo\n",
    "model_adamw = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Note: TensorFlow 2.11+ c√≥ AdamW built-in\n",
    "try:\n",
    "    optimizer_adamw = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01  # Weight decay coefficient\n",
    "    )\n",
    "except:\n",
    "    # Fallback to Adam if AdamW not available\n",
    "    print(\"AdamW not available in this TF version, using Adam\")\n",
    "    optimizer_adamw = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_adamw.compile(optimizer=optimizer_adamw, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist_adamw = model_adamw.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "print(f\"Final val accuracy: {hist_adamw.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimizers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "histories_opt = [\n",
    "    (hist_sgd, 'SGD'),\n",
    "    (hist_adam, 'Adam'),\n",
    "    (hist_adamw, 'AdamW')\n",
    "]\n",
    "\n",
    "# Loss\n",
    "for hist, name in histories_opt:\n",
    "    axes[0].plot(hist.history['loss'], label=name, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "for hist, name in histories_opt:\n",
    "    axes[1].plot(hist.history['val_accuracy'], label=name, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- SGD: Slower but stable\")\n",
    "print(\"- Adam: Fast convergence\")\n",
    "print(\"- AdamW: Similar to Adam but better generalization (on larger datasets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Learning Rate Scheduling\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn learning rate schedule?\n",
    "\n",
    "**Fixed LR:**\n",
    "- Too high ‚Üí overshoot, kh√¥ng converge\n",
    "- Too low ‚Üí qu√° ch·∫≠m\n",
    "\n",
    "**Dynamic LR:**\n",
    "- Start high ‚Üí fast progress\n",
    "- Gradually decrease ‚Üí fine-tune\n",
    "\n",
    "### üîπ C√°c strategies ph·ªï bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Exponential Decay\n",
    "lr_schedule_exp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96  # LR m·ªõi = LR c≈© √ó 0.96\n",
    ")\n",
    "\n",
    "# 2. Cosine Decay\n",
    "lr_schedule_cos = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000\n",
    ")\n",
    "\n",
    "# 3. Piecewise Constant (Step Decay)\n",
    "lr_schedule_step = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[500, 1000, 1500],\n",
    "    values=[0.1, 0.01, 0.001, 0.0001]\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "steps = np.arange(0, 2000)\n",
    "lr_exp = [lr_schedule_exp(s).numpy() for s in steps]\n",
    "lr_cos = [lr_schedule_cos(s).numpy() for s in steps]\n",
    "lr_step = [lr_schedule_step(s).numpy() for s in steps]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(steps, lr_exp, label='Exponential Decay', linewidth=2)\n",
    "plt.plot(steps, lr_cos, label='Cosine Decay', linewidth=2)\n",
    "plt.plot(steps, lr_step, label='Step Decay', linewidth=2)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"When to use:\")\n",
    "print(\"- Exponential: Smooth, gradual decrease\")\n",
    "print(\"- Cosine: Popular for vision tasks\")\n",
    "print(\"- Step: Simple, interpretable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Train with LR schedule\n",
    "model_scheduled = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Create LR schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "model_scheduled.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "hist_scheduled = model_scheduled.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Final val accuracy: {hist_scheduled.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Regularization - Ch·ªëng Overfitting\n",
    "\n",
    "### üîπ 1. L2 Regularization (Weight Decay)\n",
    "\n",
    "**Idea:** Penalize large weights\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Loss_total = Loss_original + Œª √ó Œ£(w¬≤)\n",
    "```\n",
    "\n",
    "**Effect:**\n",
    "- Weights tend to be small ‚Üí simpler model\n",
    "- Prevents overfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT L2\n",
    "model_no_l2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model WITH L2\n",
    "model_with_l2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        128, \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.L2(0.01),  # L2 regularization\n",
    "        input_shape=(20,)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(\n",
    "        64, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile & train\n",
    "model_no_l2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_with_l2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training without L2...\")\n",
    "hist_no_l2 = model_no_l2.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "print(\"Training with L2...\")\n",
    "hist_with_l2 = model_with_l2.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(hist_no_l2.history['loss'], label='Train (No L2)', linestyle='--')\n",
    "axes[0].plot(hist_no_l2.history['val_loss'], label='Val (No L2)', linestyle='--')\n",
    "axes[0].plot(hist_with_l2.history['loss'], label='Train (With L2)', linewidth=2)\n",
    "axes[0].plot(hist_with_l2.history['val_loss'], label='Val (With L2)', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('L2 Regularization Effect on Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(hist_no_l2.history['val_accuracy'], label='Val (No L2)', linestyle='--')\n",
    "axes[1].plot(hist_with_l2.history['val_accuracy'], label='Val (With L2)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"With L2: Smaller gap between train & val loss ‚Üí less overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 2. Dropout\n",
    "\n",
    "**Idea:** Randomly \"drop\" neurons during training\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Training:   Random drop neurons (e.g., 50%)\n",
    "Inference:  Use all neurons\n",
    "```\n",
    "\n",
    "**Effect:**\n",
    "- Prevents co-adaptation\n",
    "- Acts like ensemble\n",
    "- Very effective!\n",
    "\n",
    "**Rate:**\n",
    "- 0.2-0.5 for hidden layers\n",
    "- Higher rate ‚Üí more regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT Dropout\n",
    "model_no_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model WITH Dropout\n",
    "model_with_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dropout(0.5),  # Drop 50% neurons\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),  # Drop 30% neurons\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile & train\n",
    "model_no_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_with_dropout.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training without Dropout...\")\n",
    "hist_no_dropout = model_no_dropout.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "print(\"Training with Dropout...\")\n",
    "hist_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(hist_no_dropout.history['loss'], label='Train (No Dropout)', linestyle='--')\n",
    "axes[0].plot(hist_no_dropout.history['val_loss'], label='Val (No Dropout)', linestyle='--')\n",
    "axes[0].plot(hist_with_dropout.history['loss'], label='Train (With Dropout)', linewidth=2)\n",
    "axes[0].plot(hist_with_dropout.history['val_loss'], label='Val (With Dropout)', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Dropout Effect on Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(hist_no_dropout.history['val_accuracy'], label='Val (No Dropout)', linestyle='--')\n",
    "axes[1].plot(hist_with_dropout.history['val_accuracy'], label='Val (With Dropout)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Training slower v·ªõi Dropout (expected)\")\n",
    "print(\"- Validation performance better/more stable\")\n",
    "print(\"- Smaller train-val gap ‚Üí less overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ L2 vs Dropout - Khi n√†o d√πng g√¨?\n",
    "\n",
    "| Method | Pros | Cons | When to use |\n",
    "|--------|------|------|-------------|\n",
    "| **L2** | Simple, always applicable | Weaker effect | Small/medium networks |\n",
    "| **Dropout** | Very effective | Slower training | Large networks, Computer Vision |\n",
    "| **Both** | Best generalization | Most compute | Production models |\n",
    "\n",
    "**Best practice:**\n",
    "- Start with Dropout\n",
    "- Add L2 if still overfitting\n",
    "- Adjust rates based on validation performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Putting It All Together\n",
    "\n",
    "### üîπ Production-ready model template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_production_model(\n",
    "    input_shape,\n",
    "    num_classes,\n",
    "    hidden_units=[128, 64],\n",
    "    activation='relu',\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=0.01,\n",
    "    use_batch_norm=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Build production-ready model with best practices\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape tuple\n",
    "        num_classes: Number of output classes\n",
    "        hidden_units: List of hidden layer sizes\n",
    "        activation: Activation function\n",
    "        dropout_rate: Dropout rate (0 to disable)\n",
    "        l2_reg: L2 regularization coefficient (0 to disable)\n",
    "        use_batch_norm: Whether to use batch normalization\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_units:\n",
    "        # Dense layer with L2 regularization\n",
    "        regularizer = tf.keras.regularizers.L2(l2_reg) if l2_reg > 0 else None\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units,\n",
    "            kernel_regularizer=regularizer\n",
    "        ))\n",
    "        \n",
    "        # Batch normalization (optional)\n",
    "        if use_batch_norm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "        # Activation\n",
    "        model.add(tf.keras.layers.Activation(activation))\n",
    "        \n",
    "        # Dropout (optional)\n",
    "        if dropout_rate > 0:\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 2:\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model_prod = build_production_model(\n",
    "    input_shape=(20,),\n",
    "    num_classes=2,\n",
    "    hidden_units=[128, 64, 32],\n",
    "    activation='relu',\n",
    "    dropout_rate=0.3,\n",
    "    l2_reg=0.01,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "\n",
    "print(model_prod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train production model\n",
    "# LR schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile\n",
    "model_prod.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_prod = model_prod.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_prod.history['loss'], label='Train')\n",
    "plt.plot(history_prod.history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Production Model - Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_prod.history['accuracy'], label='Train')\n",
    "plt.plot(history_prod.history['val_accuracy'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Production Model - Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final val accuracy: {history_prod.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Exercises\n",
    "\n",
    "### üìù Exercise 1: Hyperparameter Tuning\n",
    "\n",
    "Try different combinations:\n",
    "- Activations: ReLU, LeakyReLU, ELU\n",
    "- Optimizers: SGD, Adam, AdamW\n",
    "- Dropout rates: 0, 0.3, 0.5\n",
    "\n",
    "Find best combination for validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Grid search or random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Custom LR Schedule\n",
    "\n",
    "Implement warmup + cosine decay:\n",
    "- Warmup: LR tƒÉng t·ª´ 0 ‚Üí max trong 5 epochs\n",
    "- Cosine decay: sau ƒë√≥ gi·∫£m theo cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Implement custom LR schedule class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Regularization Comparison\n",
    "\n",
    "Compare 4 models:\n",
    "1. No regularization\n",
    "2. L2 only\n",
    "3. Dropout only\n",
    "4. L2 + Dropout\n",
    "\n",
    "Plot validation curves and analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Train 4 models and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ T√≥m t·∫Øt\n",
    "\n",
    "### ‚úÖ ƒê√£ h·ªçc\n",
    "\n",
    "1. **Activations**: ReLU, LeakyReLU, ELU, GELU\n",
    "2. **Optimizers**: SGD, Adam, AdamW\n",
    "3. **LR Scheduling**: Exponential, Cosine, Step\n",
    "4. **Regularization**: L2, Dropout\n",
    "\n",
    "### üéì Decision Guide\n",
    "\n",
    "**Activation:**\n",
    "```\n",
    "Default: ReLU\n",
    "Dying ReLU problem: LeakyReLU\n",
    "Transformers/NLP: GELU\n",
    "```\n",
    "\n",
    "**Optimizer:**\n",
    "```\n",
    "Default: Adam (lr=0.001)\n",
    "Large models: AdamW\n",
    "Need stability: SGD with momentum\n",
    "```\n",
    "\n",
    "**Regularization:**\n",
    "```\n",
    "Always: Dropout (0.2-0.5)\n",
    "If still overfitting: Add L2 (0.001-0.01)\n",
    "```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **File 2-C**: CNN & Callbacks\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "- [Keras Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "- [Keras Activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "- [Dropout Paper](https://jmlr.org/papers/v15/srivastava14a.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c b·∫°n h·ªçc t·ªët! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
