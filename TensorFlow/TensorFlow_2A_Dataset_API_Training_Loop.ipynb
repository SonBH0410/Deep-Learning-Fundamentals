{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìó FILE 2-A ‚Äì Dataset API & Training Loop\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "\n",
    "Sau b√†i n√†y b·∫°n s·∫Ω hi·ªÉu:\n",
    "- **tf.data.Dataset** - API chuy√™n nghi·ªáp cho data pipeline\n",
    "- **Batch, Shuffle, Prefetch** - T·ªëi ∆∞u training speed\n",
    "- **Custom training loop** - Ki·ªÉm so√°t ho√†n to√†n qu√° tr√¨nh training\n",
    "- **Validation loop** - ƒê√°nh gi√° model ƒë√∫ng c√°ch\n",
    "- **Overfitting vs Underfitting** - Nh·∫≠n bi·∫øt v√† kh·∫Øc ph·ª•c\n",
    "\n",
    "---\n",
    "\n",
    "## üìå T·∫°i sao h·ªçc tf.data?\n",
    "\n",
    "**V·∫•n ƒë·ªÅ v·ªõi NumPy arrays:**\n",
    "- Load to√†n b·ªô data v√†o RAM ‚Üí OOM v·ªõi big datasets\n",
    "- Kh√¥ng c√≥ optimization t·ª± ƒë·ªông\n",
    "- Kh√¥ng streaming, kh√¥ng parallel loading\n",
    "\n",
    "**tf.data gi·∫£i quy·∫øt:**\n",
    "- ‚úÖ Streaming data (kh√¥ng c·∫ßn load h·∫øt v√†o RAM)\n",
    "- ‚úÖ Auto prefetch & parallelization\n",
    "- ‚úÖ Transformation pipeline\n",
    "- ‚úÖ Production-ready\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ tf.data.Dataset - C∆° b·∫£n\n",
    "\n",
    "### üîπ T·∫°o Dataset t·ª´ NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o data gi·∫£\n",
    "X = np.random.randn(1000, 10).astype(np.float32)\n",
    "y = np.random.randn(1000, 1).astype(np.float32)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# T·∫°o Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "print(f\"\\nDataset: {dataset}\")\n",
    "print(f\"Element spec: {dataset.element_spec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through dataset\n",
    "print(\"First 3 samples:\")\n",
    "for i, (x_sample, y_sample) in enumerate(dataset.take(3)):\n",
    "    print(f\"  Sample {i}: x.shape={x_sample.shape}, y.shape={y_sample.shape}\")\n",
    "    print(f\"    x={x_sample.numpy()[:3]}...\")  # First 3 features\n",
    "    print(f\"    y={y_sample.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ C√°c c√°ch t·∫°o Dataset kh√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°ch 1: from_tensor_slices (ƒë√£ d√πng)\n",
    "ds1 = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\n",
    "print(\"Method 1 - from_tensor_slices:\")\n",
    "print(list(ds1.as_numpy_iterator()))\n",
    "\n",
    "# C√°ch 2: from_tensors (to√†n b·ªô l√† 1 element)\n",
    "ds2 = tf.data.Dataset.from_tensors([1, 2, 3, 4, 5])\n",
    "print(\"\\nMethod 2 - from_tensors:\")\n",
    "print(list(ds2.as_numpy_iterator()))\n",
    "\n",
    "# C√°ch 3: range\n",
    "ds3 = tf.data.Dataset.range(5)\n",
    "print(\"\\nMethod 3 - range:\")\n",
    "print(list(ds3.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Dataset Transformations\n",
    "\n",
    "### üîπ map() - Transform t·ª´ng element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Square all numbers\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset_squared = dataset.map(lambda x: x ** 2)\n",
    "\n",
    "print(\"Original:\", list(dataset.as_numpy_iterator()))\n",
    "print(\"Squared: \", list(dataset_squared.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Normalize images\n",
    "def normalize(x, y):\n",
    "    \"\"\"Normalize x to [0, 1]\"\"\"\n",
    "    x = x / 255.0\n",
    "    return x, y\n",
    "\n",
    "# Fake image data\n",
    "images = np.random.randint(0, 256, size=(10, 28, 28, 1), dtype=np.uint8)\n",
    "labels = np.random.randint(0, 10, size=(10,), dtype=np.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset = dataset.map(normalize)\n",
    "\n",
    "# Check\n",
    "for x, y in dataset.take(1):\n",
    "    print(f\"Before normalize: 0-255\")\n",
    "    print(f\"After normalize: min={x.numpy().min():.2f}, max={x.numpy().max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ filter() - L·ªçc elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Keep only even numbers\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset_even = dataset.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(\"Original:\", list(dataset.as_numpy_iterator()))\n",
    "print(\"Even only:\", list(dataset_even.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Batch - Nh√≥m samples th√†nh batches\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn batch?\n",
    "\n",
    "**Kh√¥ng batch (batch_size=1):**\n",
    "```\n",
    "For each sample:\n",
    "    Forward pass ‚Üí 1 sample\n",
    "    Backward pass ‚Üí Update weights\n",
    "```\n",
    "- ‚ùå Ch·∫≠m (nhi·ªÅu update)\n",
    "- ‚ùå Gradient noisy\n",
    "- ‚ùå Kh√¥ng t·∫≠n d·ª•ng GPU\n",
    "\n",
    "**C√≥ batch (batch_size=32):**\n",
    "```\n",
    "For each batch of 32 samples:\n",
    "    Forward pass ‚Üí 32 samples parallel\n",
    "    Backward pass ‚Üí Average gradient\n",
    "    Update weights\n",
    "```\n",
    "- ‚úÖ Nhanh h∆°n\n",
    "- ‚úÖ Gradient stable h∆°n\n",
    "- ‚úÖ T·∫≠n d·ª•ng GPU t·ªët\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X = np.random.randn(100, 10).astype(np.float32)\n",
    "y = np.random.randn(100, 1).astype(np.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "# Batch\n",
    "BATCH_SIZE = 32\n",
    "dataset_batched = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "print(f\"Original: {dataset.element_spec}\")\n",
    "print(f\"Batched:  {dataset_batched.element_spec}\")\n",
    "print(f\"\\nNumber of batches: {len(list(dataset_batched))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through batches\n",
    "for i, (x_batch, y_batch) in enumerate(dataset_batched.take(3)):\n",
    "    print(f\"Batch {i}: x.shape={x_batch.shape}, y.shape={y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ drop_remainder - X·ª≠ l√Ω batch cu·ªëi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 samples, batch_size=32\n",
    "# ‚Üí 3 batches of 32 + 1 batch of 4\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(100))\n",
    "\n",
    "# Gi·ªØ batch cu·ªëi (m·∫∑c ƒë·ªãnh)\n",
    "batched_keep = dataset.batch(32, drop_remainder=False)\n",
    "batch_sizes_keep = [len(list(batch.numpy())) for batch in batched_keep]\n",
    "print(f\"Keep last batch: {batch_sizes_keep}\")\n",
    "\n",
    "# B·ªè batch cu·ªëi\n",
    "batched_drop = dataset.batch(32, drop_remainder=True)\n",
    "batch_sizes_drop = [len(list(batch.numpy())) for batch in batched_drop]\n",
    "print(f\"Drop last batch: {batch_sizes_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Shuffle - X√°o tr·ªôn data\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn shuffle?\n",
    "\n",
    "**Kh√¥ng shuffle:**\n",
    "```\n",
    "Batch 1: [class_0, class_0, class_0, ...]\n",
    "Batch 2: [class_1, class_1, class_1, ...]\n",
    "```\n",
    "- ‚ùå Model h·ªçc theo th·ª© t·ª± ‚Üí bias\n",
    "- ‚ùå Gradient kh√¥ng representative\n",
    "\n",
    "**C√≥ shuffle:**\n",
    "```\n",
    "Batch 1: [class_2, class_0, class_1, ...]\n",
    "Batch 2: [class_1, class_0, class_2, ...]\n",
    "```\n",
    "- ‚úÖ Model h·ªçc balanced\n",
    "- ‚úÖ Gradient better estimate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ordered data\n",
    "data = np.arange(20)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# No shuffle\n",
    "print(\"Without shuffle:\")\n",
    "print(list(dataset.batch(5).take(2).as_numpy_iterator()))\n",
    "\n",
    "# With shuffle\n",
    "BUFFER_SIZE = 20\n",
    "dataset_shuffled = dataset.shuffle(buffer_size=BUFFER_SIZE)\n",
    "print(\"\\nWith shuffle:\")\n",
    "print(list(dataset_shuffled.batch(5).take(2).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ buffer_size - Quan tr·ªçng!\n",
    "\n",
    "**buffer_size** = s·ªë samples ƒë∆∞·ª£c load v√†o buffer ƒë·ªÉ shuffle\n",
    "\n",
    "```\n",
    "buffer_size = 10:\n",
    "  Load 10 samples ‚Üí shuffle 10 samples ‚Üí pick 1\n",
    "  \n",
    "buffer_size = 1000:\n",
    "  Load 1000 samples ‚Üí shuffle 1000 samples ‚Üí pick 1\n",
    "```\n",
    "\n",
    "**Best practice:**\n",
    "- Small dataset: `buffer_size = len(dataset)`\n",
    "- Large dataset: `buffer_size = 10000` ho·∫∑c l·ªõn h∆°n\n",
    "- Qu√° nh·ªè ‚Üí shuffle kh√¥ng t·ªët\n",
    "- Qu√° l·ªõn ‚Üí t·ªën RAM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: buffer_size effect\n",
    "data = np.arange(100)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "buffer_sizes = [10, 50, 100]\n",
    "for ax, buf_size in zip(axes, buffer_sizes):\n",
    "    shuffled = dataset.shuffle(buf_size, seed=42)\n",
    "    samples = list(shuffled.take(50).as_numpy_iterator())\n",
    "    \n",
    "    ax.scatter(range(len(samples)), samples, alpha=0.6, s=20)\n",
    "    ax.plot(range(len(samples)), samples, alpha=0.3, linewidth=0.5)\n",
    "    ax.set_title(f'buffer_size = {buf_size}')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Quan s√°t: buffer_size c√†ng l·ªõn ‚Üí shuffle c√†ng random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Prefetch - T·ªëi ∆∞u performance\n",
    "\n",
    "### üîπ V·∫•n ƒë·ªÅ: GPU idle\n",
    "\n",
    "**Kh√¥ng prefetch:**\n",
    "```\n",
    "Time:  |----GPU train----|----CPU load data----|----GPU train----|...\n",
    "       ^                 ^\n",
    "       GPU working        GPU IDLE (waiting for data)\n",
    "```\n",
    "\n",
    "**C√≥ prefetch:**\n",
    "```\n",
    "Time:  |----GPU train----|----GPU train----|----GPU train----|...\n",
    "       |----CPU load-----|----CPU load-----|----CPU load-----|\n",
    "       ^\n",
    "       CPU & GPU overlap\n",
    "```\n",
    "\n",
    "### üîπ C√°ch d√πng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X = np.random.randn(10000, 28, 28, 1).astype(np.float32)\n",
    "y = np.random.randint(0, 10, size=(10000,), dtype=np.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "# Without prefetch\n",
    "print(\"Benchmark without prefetch...\")\n",
    "start = time.time()\n",
    "for _ in dataset.take(100):\n",
    "    pass\n",
    "time_no_prefetch = time.time() - start\n",
    "\n",
    "# With prefetch\n",
    "dataset_prefetch = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "print(\"Benchmark with prefetch...\")\n",
    "start = time.time()\n",
    "for _ in dataset_prefetch.take(100):\n",
    "    pass\n",
    "time_with_prefetch = time.time() - start\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Without prefetch: {time_no_prefetch:.4f}s\")\n",
    "print(f\"  With prefetch:    {time_with_prefetch:.4f}s\")\n",
    "print(f\"  Speedup:          {time_no_prefetch/time_with_prefetch:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ AUTOTUNE - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh\n",
    "\n",
    "```python\n",
    "dataset.prefetch(tf.data.AUTOTUNE)  # Recommended!\n",
    "```\n",
    "\n",
    "- TensorFlow t·ª± ƒë·ªông ch·ªçn buffer size t·ªëi ∆∞u\n",
    "- Th√≠ch ·ª©ng v·ªõi hardware\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Complete Pipeline Pattern\n",
    "\n",
    "### üîπ Pattern chu·∫©n cho training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, batch_size=32, shuffle=True, augment=False):\n",
    "    \"\"\"\n",
    "    Create optimized tf.data pipeline\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle\n",
    "        augment: Whether to apply augmentation\n",
    "    \"\"\"\n",
    "    # 1. Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    # 2. Shuffle (if training)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X))\n",
    "    \n",
    "    # 3. Batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # 4. Augmentation (if needed)\n",
    "    if augment:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (x + tf.random.normal(tf.shape(x)) * 0.01, y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    \n",
    "    # 5. Prefetch (ALWAYS!)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Example usage\n",
    "X_train = np.random.randn(1000, 10).astype(np.float32)\n",
    "y_train = np.random.randn(1000, 1).astype(np.float32)\n",
    "\n",
    "train_dataset = create_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_dataset = create_dataset(X_train[:200], y_train[:200], batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Val dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Custom Training Loop\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn custom loop?\n",
    "\n",
    "`model.fit()` ti·ªán nh∆∞ng:\n",
    "- ‚ùå Kh√≥ customize\n",
    "- ‚ùå Kh√¥ng linh ho·∫°t cho research\n",
    "- ‚ùå Kh√¥ng control ƒë∆∞·ª£c t·ª´ng step\n",
    "\n",
    "**Custom loop:**\n",
    "- ‚úÖ Full control\n",
    "- ‚úÖ Custom metrics\n",
    "- ‚úÖ Advanced techniques (GAN, RL...)\n",
    "\n",
    "### üîπ Pattern c∆° b·∫£n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)\n",
    "X = X.astype(np.float32)\n",
    "y = y.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_dataset = create_dataset(X_val, y_val, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Val samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Optimizer & loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "print(\"Model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop\n",
    "@tf.function  # Convert to graph for speed\n",
    "def train_step(x, y):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Update metric\n",
    "    metric.update_state(y, predictions)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    \"\"\"Single validation step\"\"\"\n",
    "    predictions = model(x, training=False)\n",
    "    loss = loss_fn(y, predictions)\n",
    "    metric.update_state(y, predictions)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "EPOCHS = 20\n",
    "history = {'train_loss': [], 'train_mae': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # ============ TRAINING ============\n",
    "    metric.reset_state()\n",
    "    train_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        train_losses.append(loss)\n",
    "    \n",
    "    train_loss = np.mean(train_losses)\n",
    "    train_mae = metric.result().numpy()\n",
    "    \n",
    "    # ============ VALIDATION ============\n",
    "    metric.reset_state()\n",
    "    val_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in val_dataset:\n",
    "        loss = val_step(x_batch, y_batch)\n",
    "        val_losses.append(loss)\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_mae = metric.result().numpy()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}, MAE: {val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['train_mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Training & Validation MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Overfitting vs Underfitting\n",
    "\n",
    "### üîπ ƒê·ªãnh nghƒ©a\n",
    "\n",
    "**Underfitting:**\n",
    "- Model **qu√° ƒë∆°n gi·∫£n**\n",
    "- Kh√¥ng h·ªçc ƒë∆∞·ª£c patterns\n",
    "- Train loss cao, Val loss cao\n",
    "\n",
    "**Good fit:**\n",
    "- Model v·ª´a ƒë·ªß\n",
    "- H·ªçc ƒë∆∞·ª£c patterns, generalize t·ªët\n",
    "- Train loss th·∫•p, Val loss th·∫•p\n",
    "\n",
    "**Overfitting:**\n",
    "- Model **qu√° ph·ª©c t·∫°p**\n",
    "- H·ªçc thu·ªôc training data\n",
    "- Train loss th·∫•p, Val loss cao\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with noise\n",
    "np.random.seed(42)\n",
    "X_demo = np.linspace(0, 10, 100).reshape(-1, 1).astype(np.float32)\n",
    "y_demo = (np.sin(X_demo) + np.random.randn(100, 1) * 0.3).astype(np.float32)\n",
    "\n",
    "X_train_demo, X_val_demo, y_train_demo, y_val_demo = train_test_split(\n",
    "    X_demo, y_demo, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and plot\n",
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, epochs=100, title=\"\"):\n",
    "    \"\"\"Train model and return history\"\"\"\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# 1. UNDERFITTING - Model qu√° ƒë∆°n gi·∫£n\n",
    "model_underfit = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(1,))  # Only 1 neuron!\n",
    "])\n",
    "model_underfit.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 2. GOOD FIT - Model v·ª´a ƒë·ªß\n",
    "model_goodfit = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model_goodfit.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 3. OVERFITTING - Model qu√° ph·ª©c t·∫°p + train qu√° l√¢u\n",
    "model_overfit = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model_overfit.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(\"Training models...\")\n",
    "hist_underfit = train_and_evaluate(model_underfit, X_train_demo, y_train_demo, \n",
    "                                   X_val_demo, y_val_demo, epochs=100)\n",
    "hist_goodfit = train_and_evaluate(model_goodfit, X_train_demo, y_train_demo,\n",
    "                                  X_val_demo, y_val_demo, epochs=100)\n",
    "hist_overfit = train_and_evaluate(model_overfit, X_train_demo, y_train_demo,\n",
    "                                  X_val_demo, y_val_demo, epochs=500)  # Train longer\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "histories = [\n",
    "    (hist_underfit, 'Underfitting'),\n",
    "    (hist_goodfit, 'Good Fit'),\n",
    "    (hist_overfit, 'Overfitting')\n",
    "]\n",
    "\n",
    "for ax, (hist, title) in zip(axes, histories):\n",
    "    ax.plot(hist.history['loss'], label='Train Loss', linewidth=2)\n",
    "    ax.plot(hist.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add interpretation\n",
    "    final_train = hist.history['loss'][-1]\n",
    "    final_val = hist.history['val_loss'][-1]\n",
    "    ax.text(0.5, 0.95, f'Train: {final_train:.4f}\\nVal: {final_val:.4f}',\n",
    "            transform=ax.transAxes, ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNh·∫≠n x√©t:\")\n",
    "print(\"1. UNDERFITTING: Train & Val loss ƒë·ªÅu cao ‚Üí Model ch∆∞a ƒë·ªß m·∫°nh\")\n",
    "print(\"2. GOOD FIT: Train & Val loss ƒë·ªÅu th·∫•p v√† g·∫ßn nhau ‚Üí Perfect!\")\n",
    "print(\"3. OVERFITTING: Train loss th·∫•p nh∆∞ng Val loss cao ‚Üí Model h·ªçc thu·ªôc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ C√°ch kh·∫Øc ph·ª•c\n",
    "\n",
    "**Underfitting:**\n",
    "- ‚úÖ TƒÉng model capacity (more layers/neurons)\n",
    "- ‚úÖ Train l√¢u h∆°n\n",
    "- ‚úÖ Th√™m features\n",
    "\n",
    "**Overfitting:**\n",
    "- ‚úÖ Regularization (L1, L2, Dropout) ‚Üí S·∫Ω h·ªçc ·ªü File 2-B\n",
    "- ‚úÖ Early stopping\n",
    "- ‚úÖ More data\n",
    "- ‚úÖ Gi·∫£m model complexity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Best Practices Summary\n",
    "\n",
    "### ‚úÖ tf.data Pipeline Pattern\n",
    "\n",
    "```python\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    .shuffle(buffer_size=len(X))  # Training only\n",
    "    .batch(batch_size)\n",
    "    .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)  # ALWAYS!\n",
    ")\n",
    "```\n",
    "\n",
    "### ‚úÖ Custom Training Loop Pattern\n",
    "\n",
    "```python\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_fn(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "```\n",
    "\n",
    "### ‚úÖ Monitoring Checklist\n",
    "\n",
    "- [ ] Train loss gi·∫£m?\n",
    "- [ ] Val loss gi·∫£m?\n",
    "- [ ] Train vs Val loss c√≥ gap l·ªõn kh√¥ng? (overfitting)\n",
    "- [ ] C·∫£ 2 ƒë·ªÅu cao? (underfitting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîü Exercises\n",
    "\n",
    "### üìù Exercise 1: Dataset Pipeline\n",
    "\n",
    "T·∫°o optimized dataset pipeline cho MNIST:\n",
    "- Load MNIST\n",
    "- Normalize to [0, 1]\n",
    "- Shuffle, batch, prefetch\n",
    "- Augmentation: random rotation ¬±10 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Load MNIST\n",
    "# TODO: Create pipeline with augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Custom Training with Metrics\n",
    "\n",
    "Implement custom training loop v·ªõi:\n",
    "- Custom metric (F1-score for classification)\n",
    "- Progress bar (tqdm)\n",
    "- Save best model based on val metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Implement custom loop with F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Detect Overfitting\n",
    "\n",
    "Given training history, vi·∫øt function detect overfitting:\n",
    "- Return True n·∫øu val_loss tƒÉng 3 epochs li√™n ti·∫øp\n",
    "- Return False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def detect_overfitting(history, patience=3):\n",
    "    \"\"\"\n",
    "    Detect overfitting from training history\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary with 'val_loss' key\n",
    "        patience: Number of epochs to check\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if overfitting detected\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "history_good = {'val_loss': [1.0, 0.8, 0.6, 0.5, 0.4]}\n",
    "history_bad = {'val_loss': [1.0, 0.8, 0.9, 1.0, 1.1]}\n",
    "\n",
    "print(detect_overfitting(history_good))  # Should be False\n",
    "print(detect_overfitting(history_bad))   # Should be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ T√≥m t·∫Øt\n",
    "\n",
    "### ‚úÖ ƒê√£ h·ªçc\n",
    "\n",
    "1. **tf.data.Dataset** - Professional data pipeline\n",
    "2. **Transformations** - map, filter, batch, shuffle\n",
    "3. **Prefetch** - Optimize GPU utilization\n",
    "4. **Custom training loop** - Full control\n",
    "5. **Overfitting vs Underfitting** - Recognize and fix\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "**Always use this pattern:**\n",
    "```python\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(data)\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitor training:**\n",
    "- Train loss ‚Üì, Val loss ‚Üì ‚Üí Good!\n",
    "- Train loss ‚Üì, Val loss ‚Üë ‚Üí Overfitting!\n",
    "- Both high ‚Üí Underfitting!\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **File 2-B**: Optimizers, Activations & Regularization\n",
    "- **File 2-C**: CNN & Callbacks\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "- [tf.data Guide](https://www.tensorflow.org/guide/data)\n",
    "- [tf.data Performance](https://www.tensorflow.org/guide/data_performance)\n",
    "- [Custom Training](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c b·∫°n h·ªçc t·ªët! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
