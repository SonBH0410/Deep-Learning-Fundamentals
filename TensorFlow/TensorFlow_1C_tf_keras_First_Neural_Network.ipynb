{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò FILE 1-C ‚Äì tf.keras & First Neural Network\n",
    "\n",
    "## üéØ M·ª•c ti√™u\n",
    "\n",
    "Sau b√†i n√†y b·∫°n s·∫Ω hi·ªÉu:\n",
    "- **tf.keras** l√† g√¨ v√† v√¨ sao n√≥ quan tr·ªçng\n",
    "- **Sequential API** - c√°ch build model ƒë∆°n gi·∫£n nh·∫•t\n",
    "- **Dense layer** - layer c∆° b·∫£n nh·∫•t\n",
    "- **Activation functions** (ReLU, Softmax, Sigmoid)\n",
    "- Build v√† train Neural Network ƒë·∫ßu ti√™n\n",
    "\n",
    "---\n",
    "\n",
    "## üìå T·∫°i sao h·ªçc tf.keras?\n",
    "\n",
    "- **High-level API** - d·ªÖ d√πng, code ng·∫Øn g·ªçn\n",
    "- **Industry standard** - 90% project d√πng Keras\n",
    "- **T√≠ch h·ª£p s·∫µn** trong TensorFlow 2.x\n",
    "- **Production-ready** - t·ª´ research ‚Üí deploy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ tf.keras l√† g√¨?\n",
    "\n",
    "### üîπ L·ªãch s·ª≠ ng·∫Øn g·ªçn\n",
    "\n",
    "- **Keras** (ban ƒë·∫ßu): Th∆∞ vi·ªán ƒë·ªôc l·∫≠p, ch·∫°y tr√™n nhi·ªÅu backend (TensorFlow, Theano, CNTK)\n",
    "- **tf.keras** (hi·ªán t·∫°i): Keras ƒë∆∞·ª£c t√≠ch h·ª£p ch√≠nh th·ª©c v√†o TensorFlow 2.x\n",
    "\n",
    "### üîπ tf.keras vs Keras\n",
    "\n",
    "```python\n",
    "# Keras c≈© (deprecated)\n",
    "from keras.models import Sequential\n",
    "\n",
    "# tf.keras (chu·∫©n hi·ªán t·∫°i)\n",
    "from tensorflow.keras.models import Sequential\n",
    "# ho·∫∑c\n",
    "tf.keras.models.Sequential\n",
    "```\n",
    "\n",
    "**üëâ Lu√¥n d√πng `tf.keras`!**\n",
    "\n",
    "### üîπ 3 level API c·ªßa TensorFlow\n",
    "\n",
    "```\n",
    "High-level (d·ªÖ):   tf.keras (‚≠ê recommend)\n",
    "Mid-level:         tf.nn, tf.layers\n",
    "Low-level (kh√≥):   tf.GradientTape + manual\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Sequential API - Build model ƒë∆°n gi·∫£n\n",
    "\n",
    "### üîπ Sequential l√† g√¨?\n",
    "\n",
    "**Sequential** = model c√≥ c√°c layer x·∫øp ch·ªìng l√™n nhau theo th·ª© t·ª±\n",
    "\n",
    "```\n",
    "Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí ... ‚Üí Output\n",
    "```\n",
    "\n",
    "### üîπ C√∫ ph√°p\n",
    "\n",
    "**C√°ch 1: List c√°c layer**\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "**C√°ch 2: Add t·ª´ng layer**\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 1: Model ƒë∆°n gi·∫£n nh·∫•t\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)  # 1 neuron, no activation\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 2: Multi-layer model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', name='hidden1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='hidden2'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Hi·ªÉu model.summary()\n",
    "\n",
    "```\n",
    "Layer (type)                Output Shape              Param #\n",
    "================================================================\n",
    "hidden1 (Dense)            (None, 64)                ???\n",
    "```\n",
    "\n",
    "**Output Shape:**\n",
    "- `(None, 64)` ‚Üí batch_size kh√¥ng c·ªë ƒë·ªãnh, 64 neurons\n",
    "\n",
    "**Param #:**\n",
    "- S·ªë parameters (weights + biases)\n",
    "- C√¥ng th·ª©c: `(input_dim + 1) √ó output_dim`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√≠nh s·ªë parameters\n",
    "# Gi·∫£ s·ª≠ input_dim = 10\n",
    "input_dim = 10\n",
    "output_dim = 64\n",
    "\n",
    "# Dense layer: y = Wx + b\n",
    "# W shape: (input_dim, output_dim)\n",
    "# b shape: (output_dim,)\n",
    "num_weights = input_dim * output_dim\n",
    "num_biases = output_dim\n",
    "total_params = num_weights + num_biases\n",
    "\n",
    "print(f\"Number of weights: {num_weights}\")\n",
    "print(f\"Number of biases: {num_biases}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Formula: ({input_dim} + 1) √ó {output_dim} = {(input_dim + 1) * output_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Dense Layer - Layer c∆° b·∫£n nh·∫•t\n",
    "\n",
    "### üîπ Dense layer l√† g√¨?\n",
    "\n",
    "**Dense** = Fully Connected Layer = m·ªçi neuron ƒë·ªÅu k·∫øt n·ªëi v·ªõi m·ªçi neuron ·ªü layer tr∆∞·ªõc\n",
    "\n",
    "### üîπ C√¥ng th·ª©c\n",
    "\n",
    "```\n",
    "output = activation(dot(input, W) + b)\n",
    "```\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- `input`: shape `(batch_size, input_dim)`\n",
    "- `W`: weights, shape `(input_dim, units)`\n",
    "- `b`: biases, shape `(units,)`\n",
    "- `activation`: h√†m k√≠ch ho·∫°t (ReLU, Sigmoid...)\n",
    "\n",
    "### üîπ Parameters\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dense(\n",
    "    units,                    # S·ªë neurons (output dimension)\n",
    "    activation=None,          # H√†m k√≠ch ho·∫°t\n",
    "    use_bias=True,           # C√≥ d√πng bias kh√¥ng\n",
    "    kernel_initializer='glorot_uniform',  # C√°ch kh·ªüi t·∫°o weights\n",
    "    bias_initializer='zeros',             # C√°ch kh·ªüi t·∫°o biases\n",
    "    name=None                # T√™n layer (optional)\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª•: Dense layer chi ti·∫øt\n",
    "layer = tf.keras.layers.Dense(\n",
    "    units=5,\n",
    "    activation='relu',\n",
    "    name='my_dense_layer'\n",
    ")\n",
    "\n",
    "# T·∫°o input gi·∫£\n",
    "input_data = tf.random.normal([3, 10])  # batch_size=3, input_dim=10\n",
    "\n",
    "# Forward pass\n",
    "output = layer(input_data)\n",
    "\n",
    "print(f\"Input shape: {input_data.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nWeights shape: {layer.weights[0].shape}\")\n",
    "print(f\"Biases shape: {layer.weights[1].shape}\")\n",
    "print(f\"\\nTotal parameters: {layer.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Visualization: Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights\n",
    "weights = layer.weights[0].numpy()  # Shape: (10, 5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(weights, cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar(label='Weight value')\n",
    "plt.xlabel('Output neurons (5)')\n",
    "plt.ylabel('Input features (10)')\n",
    "plt.title('Dense Layer Weights Visualization')\n",
    "plt.show()\n",
    "\n",
    "print(\"M·ªói c·ªôt = weights c·ªßa 1 neuron\")\n",
    "print(\"M·ªói h√†ng = weights cho 1 input feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Activation Functions\n",
    "\n",
    "### üîπ T·∫°i sao c·∫ßn activation?\n",
    "\n",
    "**Kh√¥ng c√≥ activation:**\n",
    "```\n",
    "Layer1: y1 = W1*x + b1\n",
    "Layer2: y2 = W2*y1 + b2 = W2*(W1*x + b1) + b2\n",
    "       = (W2*W1)*x + (W2*b1 + b2)\n",
    "```\n",
    "\n",
    "‚Üí Nhi·ªÅu layer v·∫´n ch·ªâ l√† **linear transformation**!\n",
    "\n",
    "**C√≥ activation:**\n",
    "```\n",
    "Layer1: y1 = activation(W1*x + b1)\n",
    "Layer2: y2 = activation(W2*y1 + b2)\n",
    "```\n",
    "\n",
    "‚Üí Model c√≥ kh·∫£ nƒÉng h·ªçc **non-linear patterns**!\n",
    "\n",
    "### üîπ C√°c activation ph·ªï bi·∫øn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x for plotting\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Compute activations\n",
    "relu = np.maximum(0, x)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "# Softmax (special case for visualization)\n",
    "# We'll show it separately\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, relu, 'b-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('ReLU: max(0, x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, sigmoid, 'g-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='0.5')\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Sigmoid: 1/(1+e^-x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Sigmoid(x)')\n",
    "plt.legend()\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, tanh, 'r-', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Tanh')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Tanh(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "```python\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- `x > 0` ‚Üí gi·ªØ nguy√™n\n",
    "- `x ‚â§ 0` ‚Üí 0\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- **Hidden layers** (default choice)\n",
    "- Regression problems\n",
    "\n",
    "**∆Øu ƒëi·ªÉm:**\n",
    "- ƒê∆°n gi·∫£n, nhanh\n",
    "- Tr√°nh vanishing gradient\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- Dying ReLU (neurons = 0 m√£i m√£i)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU trong TensorFlow\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "relu_output = tf.nn.relu(x)\n",
    "\n",
    "print(f\"Input:  {x.numpy()}\")\n",
    "print(f\"Output: {relu_output.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 2. Sigmoid\n",
    "\n",
    "```python\n",
    "Sigmoid(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- Output: `(0, 1)`\n",
    "- Smooth curve\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- **Binary classification** (output layer)\n",
    "- X√°c su·∫•t (0-1)\n",
    "\n",
    "**∆Øu ƒëi·ªÉm:**\n",
    "- Output c√≥ √Ω nghƒ©a (probability)\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm:**\n",
    "- Vanishing gradient\n",
    "- Ch·∫≠m h∆°n ReLU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid trong TensorFlow\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "sigmoid_output = tf.nn.sigmoid(x)\n",
    "\n",
    "print(f\"Input:  {x.numpy()}\")\n",
    "print(f\"Output: {sigmoid_output.numpy()}\")\n",
    "print(\"\\nNh·∫≠n x√©t:\")\n",
    "print(\"  x = 0  ‚Üí sigmoid = 0.5\")\n",
    "print(\"  x > 0  ‚Üí sigmoid > 0.5\")\n",
    "print(\"  x < 0  ‚Üí sigmoid < 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ 3. Softmax\n",
    "\n",
    "```python\n",
    "Softmax(x_i) = e^(x_i) / sum(e^(x_j))\n",
    "```\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm:**\n",
    "- Output: `(0, 1)` cho m·ªói class\n",
    "- **Sum of all outputs = 1** (probability distribution)\n",
    "\n",
    "**Khi n√†o d√πng:**\n",
    "- **Multi-class classification** (output layer)\n",
    "- C·∫ßn probability cho nhi·ªÅu class\n",
    "\n",
    "**V√≠ d·ª•:**\n",
    "```\n",
    "Input:  [2.0, 1.0, 0.1]\n",
    "Output: [0.66, 0.24, 0.10]  # Sum = 1.0\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax trong TensorFlow\n",
    "logits = tf.constant([[2.0, 1.0, 0.1]])\n",
    "softmax_output = tf.nn.softmax(logits)\n",
    "\n",
    "print(f\"Input (logits): {logits.numpy()}\")\n",
    "print(f\"Output (probs): {softmax_output.numpy()}\")\n",
    "print(f\"Sum: {tf.reduce_sum(softmax_output).numpy():.6f}\")\n",
    "\n",
    "# Visualization\n",
    "classes = ['Cat', 'Dog', 'Bird']\n",
    "probs = softmax_output.numpy()[0]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(classes, probs, color=['blue', 'green', 'red'], alpha=0.7)\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Softmax Output Example')\n",
    "plt.ylim([0, 1])\n",
    "for i, (cls, prob) in enumerate(zip(classes, probs)):\n",
    "    plt.text(i, prob + 0.02, f'{prob:.2f}', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîπ Cheat Sheet: Khi n√†o d√πng activation n√†o?\n",
    "\n",
    "| Task | Hidden Layers | Output Layer |\n",
    "|------|---------------|-------------|\n",
    "| Regression | ReLU | None (linear) |\n",
    "| Binary Classification | ReLU | Sigmoid |\n",
    "| Multi-class Classification | ReLU | Softmax |\n",
    "| Image Generation | ReLU | Tanh / Sigmoid |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ First Neural Network - Regression\n",
    "\n",
    "### üîπ B√†i to√°n: Predict House Price\n",
    "\n",
    "```\n",
    "Input:  [area, bedrooms, age]\n",
    "Output: price\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake data\n",
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "# Features: [area (m¬≤), bedrooms, age (years)]\n",
    "area = np.random.uniform(50, 200, num_samples)\n",
    "bedrooms = np.random.randint(1, 6, num_samples)\n",
    "age = np.random.uniform(0, 50, num_samples)\n",
    "\n",
    "# True relationship: price = 1000*area + 50000*bedrooms - 500*age + noise\n",
    "price = 1000 * area + 50000 * bedrooms - 500 * age + np.random.randn(num_samples) * 10000\n",
    "\n",
    "# Combine features\n",
    "X = np.column_stack([area, bedrooms, age]).astype(np.float32)\n",
    "y = price.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "print(\"Area\\tBeds\\tAge\\tPrice\")\n",
    "for i in range(5):\n",
    "    print(f\"{X[i, 0]:.1f}\\t{X[i, 1]:.0f}\\t{X[i, 2]:.1f}\\t${y[i, 0]:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,), name='hidden1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='hidden2'),\n",
    "    tf.keras.layers.Dense(1, name='output')  # No activation for regression\n",
    "], name='house_price_model')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',           # Optimizer (s·∫Ω h·ªçc k·ªπ sau)\n",
    "    loss='mse',                 # Mean Squared Error\n",
    "    metrics=['mae']             # Mean Absolute Error\n",
    ")\n",
    "\n",
    "print(\"Model compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0  # T·∫Øt log chi ti·∫øt\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss (MSE): {test_loss:,.0f}\")\n",
    "print(f\"Test MAE: ${test_mae:,.0f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new data\n",
    "new_house = np.array([[100, 3, 10]])  # 100m¬≤, 3 bedrooms, 10 years old\n",
    "\n",
    "predicted_price = model.predict(new_house, verbose=0)[0, 0]\n",
    "\n",
    "print(f\"House: {new_house[0, 0]:.0f}m¬≤, {new_house[0, 1]:.0f} bedrooms, {new_house[0, 2]:.0f} years old\")\n",
    "print(f\"Predicted price: ${predicted_price:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ First Neural Network - Classification\n",
    "\n",
    "### üîπ B√†i to√°n: Binary Classification (Cancer Detection)\n",
    "\n",
    "```\n",
    "Input:  30 medical features\n",
    "Output: 0 (benign) or 1 (malignant)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data.astype(np.float32)\n",
    "y = data.target.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nClasses: {np.unique(y)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Benign (0): {(y == 0).sum()}\")\n",
    "print(f\"  Malignant (1): {(y == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features (QUAN TR·ªåNG cho Neural Network!)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X).astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model_clf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(30,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification!\n",
    "], name='cancer_classifier')\n",
    "\n",
    "print(model_clf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model_clf.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # Binary classification loss\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_clf = model_clf.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_clf.history['loss'], label='Train Loss')\n",
    "plt.plot(history_clf.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Binary Crossentropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_clf.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history_clf.history['val_accuracy'], label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc = model_clf.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model_clf.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks([0, 1], ['Benign', 'Malignant'])\n",
    "plt.yticks([0, 1], ['Benign', 'Malignant'])\n",
    "\n",
    "# Add text\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center', fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Negatives: {cm[0, 0]}\")\n",
    "print(f\"False Positives: {cm[0, 1]}\")\n",
    "print(f\"False Negatives: {cm[1, 0]}\")\n",
    "print(f\"True Positives: {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Model Methods - T·ªïng h·ª£p\n",
    "\n",
    "### üîπ C√°c method quan tr·ªçng\n",
    "\n",
    "```python\n",
    "# 1. Build model\n",
    "model = tf.keras.Sequential([...])\n",
    "\n",
    "# 2. Compile (c·∫•u h√¨nh training)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 3. Train\n",
    "history = model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# 4. Evaluate\n",
    "loss, metric = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 5. Predict\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# 6. Save/Load\n",
    "model.save('my_model.keras')\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo save/load\n",
    "model_clf.save('/tmp/cancer_model.keras')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load\n",
    "loaded_model = tf.keras.models.load_model('/tmp/cancer_model.keras')\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Test loaded model\n",
    "test_acc_loaded = loaded_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(f\"Loaded model accuracy: {test_acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Common Mistakes\n",
    "\n",
    "### ‚ùå Mistake 1: Qu√™n activation ·ªü hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAI - Kh√¥ng c√≥ activation\n",
    "bad_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64),  # THI·∫æU activation!\n",
    "    tf.keras.layers.Dense(32),  # THI·∫æU activation!\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "print(\"Model n√†y ch·ªâ l√† linear transformation!\")\n",
    "print(\"Kh√¥ng th·ªÉ h·ªçc non-linear patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√öNG - C√≥ activation\n",
    "good_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "print(\"Model n√†y c√≥ th·ªÉ h·ªçc non-linear patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Mistake 2: Sai activation ·ªü output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAI - D√πng ReLU cho binary classification\n",
    "bad_clf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='relu')  # SAI!\n",
    "])\n",
    "\n",
    "print(\"Output kh√¥ng ph·∫£i probability (0-1)!\")\n",
    "print(\"Ph·∫£i d√πng sigmoid cho binary classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê√öNG\n",
    "good_clf = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # ƒê√öNG!\n",
    "])\n",
    "\n",
    "print(\"Output l√† probability (0-1)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå Mistake 3: Qu√™n normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª•: Features c√≥ scale kh√°c nhau\n",
    "X_bad = np.array([\n",
    "    [100000, 3, 10],  # [price, bedrooms, age]\n",
    "    [200000, 4, 5],\n",
    "    [150000, 2, 20]\n",
    "])\n",
    "\n",
    "print(\"Feature 1 (price): 100,000 - 200,000\")\n",
    "print(\"Feature 2 (beds): 2 - 4\")\n",
    "print(\"Feature 3 (age): 5 - 20\")\n",
    "print(\"\\nScale kh√°c nhau ‚Üí training kh√≥ khƒÉn!\")\n",
    "\n",
    "# ƒê√öNG: Normalize\n",
    "scaler = StandardScaler()\n",
    "X_good = scaler.fit_transform(X_bad)\n",
    "\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(X_good)\n",
    "print(\"\\nMean ‚âà 0, Std ‚âà 1 ‚Üí training d·ªÖ h∆°n!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Exercises\n",
    "\n",
    "### üìù Exercise 1: Multi-class Classification\n",
    "\n",
    "Build model ƒë·ªÉ ph√¢n lo·∫°i iris dataset (3 classes)\n",
    "\n",
    "**Hints:**\n",
    "- Output layer: `Dense(3, activation='softmax')`\n",
    "- Loss: `'sparse_categorical_crossentropy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "y = iris.target.astype(np.int32)\n",
    "\n",
    "# TODO: Normalize X\n",
    "# TODO: Split data\n",
    "# TODO: Build model (3 classes!)\n",
    "# TODO: Compile\n",
    "# TODO: Train\n",
    "# TODO: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Deeper Network\n",
    "\n",
    "Th·ª≠ nghi·ªám v·ªõi model c√≥ nhi·ªÅu layer h∆°n\n",
    "\n",
    "**Hints:**\n",
    "- Th·ª≠ 5-6 hidden layers\n",
    "- Gi·∫£m d·∫ßn s·ªë neurons: 128 ‚Üí 64 ‚Üí 32 ‚Üí 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Build deeper model\n",
    "# TODO: Compare v·ªõi model n√¥ng (2 layers)\n",
    "# TODO: V·∫Ω learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Custom Activation\n",
    "\n",
    "Implement Swish activation: `swish(x) = x * sigmoid(x)`\n",
    "\n",
    "**Hints:**\n",
    "```python\n",
    "def swish(x):\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "tf.keras.layers.Dense(64, activation=swish)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Implement swish\n",
    "# TODO: Build model v·ªõi swish\n",
    "# TODO: So s√°nh v·ªõi ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ T√≥m t·∫Øt\n",
    "\n",
    "### ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "\n",
    "1. **tf.keras**: High-level API cho TensorFlow\n",
    "2. **Sequential**: Build model ƒë∆°n gi·∫£n (layer stack)\n",
    "3. **Dense**: Fully connected layer\n",
    "4. **Activations**: ReLU (hidden), Sigmoid (binary), Softmax (multi-class)\n",
    "5. **Training flow**: Compile ‚Üí Fit ‚Üí Evaluate ‚Üí Predict\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "**Pattern chu·∫©n:**\n",
    "\n",
    "```python\n",
    "# 1. Build\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(output_dim, activation=output_activation)\n",
    "])\n",
    "\n",
    "# 2. Compile\n",
    "model.compile(optimizer='adam', loss=..., metrics=[...])\n",
    "\n",
    "# 3. Train\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "# 4. Evaluate\n",
    "model.evaluate(X_test, y_test)\n",
    "```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "- **File 1-D**: GPU, Debugging & Final Exercises\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "- [Keras Sequential Guide](https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "- [Keras Layers Reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "- [Activation Functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c b·∫°n h·ªçc t·ªët! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
