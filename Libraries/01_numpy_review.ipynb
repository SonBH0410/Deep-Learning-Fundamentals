{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š NumPy Review for ML/DL\n",
    "\n",
    "**Má»¥c tiÃªu:** Ã”n táº­p NumPy operations quan trá»ng cho ML/DL\n",
    "\n",
    "**Ná»™i dung:**\n",
    "- Array creation & manipulation\n",
    "- Indexing & slicing (fancy indexing)\n",
    "- Broadcasting rules\n",
    "- Linear algebra operations\n",
    "- Random number generation\n",
    "- Performance tips\n",
    "\n",
    "**Level:** Intermediate (Ä‘Ã£ biáº¿t cÆ¡ báº£n)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Array Creation Patterns\n",
    "\n",
    "### Common trong ML/DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeros, ones (khá»Ÿi táº¡o weights/biases)\n",
    "weights = np.zeros((3, 4))\n",
    "bias = np.ones(4)\n",
    "\n",
    "# Identity (weight initialization)\n",
    "identity = np.eye(5)\n",
    "\n",
    "# Random (Xavier/He initialization pattern)\n",
    "fan_in, fan_out = 128, 64\n",
    "xavier = np.random.randn(fan_in, fan_out) * np.sqrt(2 / (fan_in + fan_out))\n",
    "he = np.random.randn(fan_in, fan_out) * np.sqrt(2 / fan_in)\n",
    "\n",
    "# Arange & linspace (learning rate schedules)\n",
    "epochs = np.arange(0, 100)\n",
    "lr_schedule = 0.1 * (0.95 ** epochs)  # Exponential decay\n",
    "\n",
    "print(f\"Xavier init shape: {xavier.shape}, mean: {xavier.mean():.4f}, std: {xavier.std():.4f}\")\n",
    "print(f\"He init shape: {he.shape}, mean: {he.mean():.4f}, std: {he.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Indexing & Slicing\n",
    "\n",
    "### Basic vs Advanced Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: batch of images (N, H, W, C)\n",
    "images = np.random.rand(32, 28, 28, 3)\n",
    "\n",
    "# Basic slicing (returns view)\n",
    "first_10 = images[:10]  # First 10 images\n",
    "red_channel = images[:, :, :, 0]  # Red channel only\n",
    "\n",
    "# Boolean indexing (returns copy)\n",
    "bright_pixels = images > 0.8\n",
    "num_bright = bright_pixels.sum()\n",
    "\n",
    "# Fancy indexing (returns copy)\n",
    "indices = [0, 5, 10, 15]  # Select specific images\n",
    "selected = images[indices]\n",
    "\n",
    "# Advanced: Select pixels at specific coordinates\n",
    "rows = [10, 15, 20]\n",
    "cols = [5, 10, 15]\n",
    "pixels = images[0, rows, cols, :]  # Shape: (3, 3)\n",
    "\n",
    "print(f\"Original shape: {images.shape}\")\n",
    "print(f\"First 10 shape: {first_10.shape}\")\n",
    "print(f\"Red channel shape: {red_channel.shape}\")\n",
    "print(f\"Selected shape: {selected.shape}\")\n",
    "print(f\"Bright pixels: {num_bright}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ View vs Copy\n",
    "\n",
    "- **View** (basic slicing): Shares memory, changes affect original\n",
    "- **Copy** (boolean/fancy indexing): New memory, independent\n",
    "\n",
    "```python\n",
    "a = np.array([1, 2, 3])\n",
    "b = a[:]          # View\n",
    "c = a[[0, 1, 2]]  # Copy\n",
    "\n",
    "b[0] = 999  # Changes a[0]\n",
    "c[0] = 999  # Does NOT change a[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Broadcasting\n",
    "\n",
    "### Rules & Common Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting rules:\n",
    "# 1. Align shapes from right\n",
    "# 2. Dimensions are compatible if: equal OR one is 1\n",
    "# 3. Missing dimensions treated as 1\n",
    "\n",
    "# Pattern 1: Add bias to each sample\n",
    "X = np.random.randn(32, 10)  # (batch_size, features)\n",
    "bias = np.random.randn(10)    # (features,)\n",
    "result = X + bias             # Broadcasting: (32, 10) + (10,) -> (32, 10)\n",
    "\n",
    "# Pattern 2: Normalize features (mean=0, std=1)\n",
    "mean = X.mean(axis=0, keepdims=True)  # (1, 10)\n",
    "std = X.std(axis=0, keepdims=True)    # (1, 10)\n",
    "X_normalized = (X - mean) / std       # Broadcasting\n",
    "\n",
    "# Pattern 3: Apply different scale per channel\n",
    "images = np.random.rand(32, 28, 28, 3)  # (N, H, W, C)\n",
    "channel_scale = np.array([0.5, 1.0, 1.5])  # (3,)\n",
    "scaled = images * channel_scale  # Broadcasting: (32,28,28,3) * (3,) -> (32,28,28,3)\n",
    "\n",
    "# Pattern 4: Distance matrix (all pairs)\n",
    "points = np.random.randn(100, 2)  # 100 points, 2D\n",
    "# Expand dims for broadcasting\n",
    "p1 = points[:, np.newaxis, :]  # (100, 1, 2)\n",
    "p2 = points[np.newaxis, :, :]  # (1, 100, 2)\n",
    "distances = np.sqrt(((p1 - p2) ** 2).sum(axis=2))  # (100, 100)\n",
    "\n",
    "print(f\"X + bias: {result.shape}\")\n",
    "print(f\"Normalized: mean={X_normalized.mean(axis=0)[:3]}, std={X_normalized.std(axis=0)[:3]}\")\n",
    "print(f\"Distance matrix: {distances.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš ï¸ Common Broadcasting Mistakes\n",
    "\n",
    "```python\n",
    "X = np.random.randn(32, 10)\n",
    "\n",
    "# âŒ WRONG: Forgetting keepdims\n",
    "mean = X.mean(axis=0)  # Shape: (10,)\n",
    "X - mean  # Works, but conceptually unclear\n",
    "\n",
    "# âœ… CORRECT: Use keepdims for clarity\n",
    "mean = X.mean(axis=0, keepdims=True)  # Shape: (1, 10)\n",
    "X - mean  # Clear broadcasting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Algebra Operations\n",
    "\n",
    "### Essential cho Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "X = np.random.randn(32, 128)   # Input: (batch, in_features)\n",
    "W = np.random.randn(128, 64)   # Weight: (in_features, out_features)\n",
    "output = X @ W                 # or np.dot(X, W) -> (batch, out_features)\n",
    "\n",
    "# Batch matrix multiplication (for attention, etc)\n",
    "Q = np.random.randn(32, 8, 64)  # (batch, seq_len, d_k)\n",
    "K = np.random.randn(32, 8, 64)  # (batch, seq_len, d_k)\n",
    "attention = Q @ K.transpose(0, 2, 1)  # (32, 8, 8)\n",
    "# Or use: np.einsum('bid,bjd->bij', Q, K)\n",
    "\n",
    "# Frobenius norm (weight regularization)\n",
    "l2_norm = np.linalg.norm(W)  # Default: Frobenius norm\n",
    "l2_penalty = 0.01 * (l2_norm ** 2)\n",
    "\n",
    "# Dot product (similarity)\n",
    "v1 = np.random.randn(128)\n",
    "v2 = np.random.randn(128)\n",
    "similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))  # Cosine similarity\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention shape: {attention.shape}\")\n",
    "print(f\"L2 norm: {l2_norm:.4f}\")\n",
    "print(f\"Cosine similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein Summation (einsum) - Advanced\n",
    "\n",
    "Powerful notation cho complex operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch matrix multiplication\n",
    "A = np.random.randn(32, 10, 20)  # (batch, i, j)\n",
    "B = np.random.randn(32, 20, 15)  # (batch, j, k)\n",
    "\n",
    "# Standard way\n",
    "C1 = np.matmul(A, B)  # (32, 10, 15)\n",
    "\n",
    "# Einsum way (more explicit)\n",
    "C2 = np.einsum('bij,bjk->bik', A, B)  # Same result\n",
    "\n",
    "# More examples\n",
    "x = np.random.randn(32, 128)\n",
    "\n",
    "# Batch dot product\n",
    "diag = np.einsum('bi,bi->b', x, x)  # Sum of squares per sample\n",
    "\n",
    "# Outer product\n",
    "outer = np.einsum('i,j->ij', np.array([1,2,3]), np.array([4,5,6]))\n",
    "\n",
    "print(f\"Einsum result shape: {C2.shape}\")\n",
    "print(f\"Results match: {np.allclose(C1, C2)}\")\n",
    "print(f\"Batch dot product shape: {diag.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Axis Operations\n",
    "\n",
    "### Reductions & Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: predictions and labels\n",
    "logits = np.random.randn(32, 10)  # (batch_size, num_classes)\n",
    "labels = np.random.randint(0, 10, size=32)  # True labels\n",
    "\n",
    "# Softmax (numerical stable version)\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - x.max(axis=axis, keepdims=True))  # Stability trick\n",
    "    return exp_x / exp_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "probs = softmax(logits, axis=1)  # (32, 10)\n",
    "\n",
    "# Get predictions\n",
    "preds = np.argmax(probs, axis=1)  # (32,)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = (preds == labels).mean()\n",
    "\n",
    "# Top-k accuracy\n",
    "k = 3\n",
    "top_k_preds = np.argsort(probs, axis=1)[:, -k:]  # Top-3 predictions\n",
    "top_k_acc = np.any(top_k_preds == labels[:, np.newaxis], axis=1).mean()\n",
    "\n",
    "# Per-class statistics\n",
    "class_counts = np.bincount(labels, minlength=10)\n",
    "class_correct = np.bincount(labels[preds == labels], minlength=10)\n",
    "per_class_acc = class_correct / (class_counts + 1e-8)  # Avoid division by zero\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Top-{k} Accuracy: {top_k_acc:.2%}\")\n",
    "print(f\"Per-class accuracy (first 3): {per_class_acc[:3]}\")\n",
    "print(f\"Probs sum to 1: {np.allclose(probs.sum(axis=1), 1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Axis Parameter Rules\n",
    "\n",
    "```python\n",
    "X = np.random.randn(32, 10, 20)  # (batch, time, features)\n",
    "\n",
    "X.mean(axis=0)  # Average over batch -> (10, 20)\n",
    "X.mean(axis=1)  # Average over time -> (32, 20)\n",
    "X.mean(axis=2)  # Average over features -> (32, 10)\n",
    "X.mean(axis=(0, 1))  # Average over batch and time -> (20,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Number Generation\n",
    "\n",
    "### Reproducibility & Modern API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD way (still works, but not recommended)\n",
    "np.random.seed(42)\n",
    "old_random = np.random.randn(5)\n",
    "\n",
    "# NEW way (recommended for reproducibility)\n",
    "rng = np.random.default_rng(42)  # Generator object\n",
    "new_random = rng.standard_normal(5)\n",
    "\n",
    "# Common distributions\n",
    "uniform = rng.uniform(0, 1, size=(10, 10))  # Uniform [0, 1)\n",
    "normal = rng.normal(0, 1, size=(10, 10))    # Normal N(0, 1)\n",
    "integers = rng.integers(0, 10, size=100)    # Random integers [0, 10)\n",
    "\n",
    "# Shuffle (data augmentation)\n",
    "indices = np.arange(100)\n",
    "rng.shuffle(indices)  # In-place shuffle\n",
    "\n",
    "# Choice (sampling)\n",
    "batch_indices = rng.choice(100, size=32, replace=False)  # Sample 32 without replacement\n",
    "\n",
    "print(f\"Old random: {old_random[:3]}\")\n",
    "print(f\"New random: {new_random[:3]}\")\n",
    "print(f\"Batch indices: {batch_indices[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Why Use Generator?\n",
    "\n",
    "- **Thread-safe**: Multiple generators don't interfere\n",
    "- **Reproducible**: Independent streams\n",
    "- **Future-proof**: NumPy's recommended approach\n",
    "\n",
    "```python\n",
    "# Good practice for ML training\n",
    "def train(seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Use rng for all random operations\n",
    "    X_shuffled = X[rng.permutation(len(X))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Tips\n",
    "\n",
    "### Vectorization vs Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Compute pairwise distances\n",
    "points = np.random.randn(1000, 2)\n",
    "\n",
    "# âŒ Slow: Python loops\n",
    "def slow_distances(points):\n",
    "    n = len(points)\n",
    "    dist = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            dist[i, j] = np.sqrt(((points[i] - points[j]) ** 2).sum())\n",
    "    return dist\n",
    "\n",
    "# âœ… Fast: Vectorized\n",
    "def fast_distances(points):\n",
    "    # Broadcasting trick\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    return np.sqrt((diff ** 2).sum(axis=2))\n",
    "\n",
    "# Benchmark\n",
    "small_points = np.random.randn(100, 2)\n",
    "\n",
    "start = time.time()\n",
    "d1 = slow_distances(small_points)\n",
    "time_slow = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "d2 = fast_distances(small_points)\n",
    "time_fast = time.time() - start\n",
    "\n",
    "print(f\"Slow method: {time_slow:.4f}s\")\n",
    "print(f\"Fast method: {time_fast:.4f}s\")\n",
    "print(f\"Speedup: {time_slow/time_fast:.1f}x\")\n",
    "print(f\"Results match: {np.allclose(d1, d2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Efficiency\n",
    "\n",
    "```python\n",
    "# âŒ Creates intermediate arrays\n",
    "result = (X + Y) * Z\n",
    "\n",
    "# âœ… In-place operations (careful!)\n",
    "X += Y\n",
    "X *= Z\n",
    "\n",
    "# âœ… Use out parameter\n",
    "result = np.empty_like(X)\n",
    "np.add(X, Y, out=result)\n",
    "np.multiply(result, Z, out=result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Common Gotchas\n",
    "\n",
    "### Avoid These Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotcha 1: Integer division\n",
    "a = np.array([1, 2, 3])\n",
    "print(f\"a / 2 = {a / 2}\")        # âœ… Float division\n",
    "print(f\"a // 2 = {a // 2}\")      # Integer division\n",
    "\n",
    "# Gotcha 2: Array modification during iteration\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "arr_copy = arr.copy()  # âœ… Make copy if needed\n",
    "\n",
    "# Gotcha 3: Boolean array indexing\n",
    "X = np.random.randn(10, 5)\n",
    "mask = X > 0\n",
    "positive = X[mask]  # Returns 1D array of all positive values (NOT shape of X)\n",
    "print(f\"Positive values shape: {positive.shape}\")  # (N,) not (10, 5)\n",
    "\n",
    "# Gotcha 4: NaN handling\n",
    "arr_with_nan = np.array([1, 2, np.nan, 4, 5])\n",
    "print(f\"Mean (wrong): {arr_with_nan.mean()}\")     # NaN\n",
    "print(f\"Mean (correct): {np.nanmean(arr_with_nan)}\")  # âœ… Ignores NaN\n",
    "\n",
    "# Gotcha 5: Floating point precision\n",
    "print(f\"0.1 + 0.2 == 0.3? {0.1 + 0.2 == 0.3}\")  # False!\n",
    "print(f\"np.isclose(0.1 + 0.2, 0.3)? {np.isclose(0.1 + 0.2, 0.3)}\")  # âœ… True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### Must Know for ML/DL\n",
    "\n",
    "1. **Broadcasting**: Core cá»§a vectorization, hiá»ƒu shapes alignment\n",
    "2. **Indexing**: Basic slicing (view) vs fancy indexing (copy)\n",
    "3. **Axis operations**: Mean, sum, argmax vá»›i axis parameter\n",
    "4. **Matrix operations**: @ operator, einsum cho complex ops\n",
    "5. **Random generation**: Use `default_rng()` cho reproducibility\n",
    "6. **Vectorization**: ALWAYS prefer vectorized operations over loops\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Normalization\n",
    "X_norm = (X - X.mean(axis=0, keepdims=True)) / X.std(axis=0, keepdims=True)\n",
    "\n",
    "# Softmax\n",
    "exp_x = np.exp(x - x.max(axis=-1, keepdims=True))\n",
    "softmax = exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Batch operations\n",
    "output = X @ W + b  # Broadcasting bias\n",
    "```\n",
    "\n",
    "### Performance Rules\n",
    "\n",
    "1. Vectorize everything possible\n",
    "2. Use views (slicing) when you can\n",
    "3. Avoid Python loops on large arrays\n",
    "4. Use appropriate dtypes (float32 vs float64)\n",
    "5. Consider memory vs speed tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Pandas for data manipulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
