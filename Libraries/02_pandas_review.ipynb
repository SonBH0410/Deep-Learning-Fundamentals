{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêº Pandas Review for ML/DL\n",
    "\n",
    "**M·ª•c ti√™u:** √în t·∫≠p Pandas operations cho data preprocessing\n",
    "\n",
    "**N·ªôi dung:**\n",
    "- DataFrame creation & inspection\n",
    "- Indexing & selection\n",
    "- Data cleaning (missing values, duplicates)\n",
    "- Transformation & feature engineering\n",
    "- GroupBy operations\n",
    "- Merge & join\n",
    "- Time series basics\n",
    "\n",
    "**Level:** Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. DataFrame Creation & Inspection\n",
    "\n",
    "### Common patterns in ML workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset (typical ML scenario)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'user_id': np.arange(n_samples),\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'income': np.random.lognormal(10, 1, n_samples),\n",
    "    'clicks': np.random.poisson(5, n_samples),\n",
    "    'conversions': np.random.binomial(1, 0.1, n_samples),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_samples),\n",
    "    'signup_date': pd.date_range('2023-01-01', periods=n_samples, freq='h')\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (realistic)\n",
    "df.loc[np.random.choice(df.index, 50, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30, replace=False), 'clicks'] = np.nan\n",
    "\n",
    "# Quick inspection\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(\"\\nBasic stats:\")\n",
    "print(df.describe())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Essential Inspection Methods\n",
    "\n",
    "```python\n",
    "df.info()              # Overview: dtypes, memory, non-null counts\n",
    "df.describe()          # Statistical summary\n",
    "df.shape               # (rows, columns)\n",
    "df.columns             # Column names\n",
    "df.dtypes              # Column data types\n",
    "df.isnull().sum()      # Missing value counts\n",
    "df.nunique()           # Number of unique values per column\n",
    "df.memory_usage(deep=True)  # Memory footprint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Indexing & Selection\n",
    "\n",
    "### loc vs iloc vs [] (know the difference!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [] - Column selection or boolean indexing\n",
    "ages = df['age']                          # Single column (Series)\n",
    "subset = df[['age', 'income']]            # Multiple columns (DataFrame)\n",
    "high_income = df[df['income'] > 50000]    # Boolean indexing\n",
    "\n",
    "# .loc[] - Label-based indexing\n",
    "row_0 = df.loc[0]                         # Single row by label\n",
    "rows_0_to_5 = df.loc[0:5]                 # Rows 0 to 5 (INCLUSIVE)\n",
    "specific = df.loc[df['age'] > 50, ['age', 'income']]  # Boolean rows, specific columns\n",
    "\n",
    "# .iloc[] - Position-based indexing\n",
    "first_row = df.iloc[0]                    # First row\n",
    "first_5_rows = df.iloc[:5]                # First 5 rows (EXCLUSIVE)\n",
    "slice_2d = df.iloc[:10, :3]               # First 10 rows, first 3 columns\n",
    "\n",
    "# Query (SQL-like, convenient)\n",
    "filtered = df.query('age > 50 and income > 30000')\n",
    "filtered2 = df.query('category == \"A\" and clicks > @df.clicks.median()')  # @ for variables\n",
    "\n",
    "print(f\"High income users: {len(high_income)}\")\n",
    "print(f\"Filtered (query): {len(filtered)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(df.loc[0:2, ['age', 'income', 'category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Chained Assignment Warning\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Chained assignment (may not work)\n",
    "df[df['age'] > 50]['income'] = 0  # Warning!\n",
    "\n",
    "# ‚úÖ GOOD: Use .loc\n",
    "df.loc[df['age'] > 50, 'income'] = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(pd.DataFrame({'Count': missing_summary, 'Percentage': missing_pct}))\n",
    "\n",
    "# Strategy 1: Drop rows with any missing\n",
    "df_dropped = df.dropna()  # Drops rows with ANY missing value\n",
    "\n",
    "# Strategy 2: Drop rows with missing in specific columns\n",
    "df_dropped2 = df.dropna(subset=['income', 'clicks'])\n",
    "\n",
    "# Strategy 3: Fill with constant\n",
    "df_filled = df.fillna({'income': 0, 'clicks': 0})\n",
    "\n",
    "# Strategy 4: Fill with statistics\n",
    "df_filled2 = df.copy()\n",
    "df_filled2['income'] = df_filled2['income'].fillna(df_filled2['income'].median())\n",
    "df_filled2['clicks'] = df_filled2['clicks'].fillna(df_filled2['clicks'].mean())\n",
    "\n",
    "# Strategy 5: Forward/backward fill (time series)\n",
    "df_filled3 = df.fillna(method='ffill')  # Forward fill\n",
    "\n",
    "print(f\"\\nOriginal: {len(df)} rows\")\n",
    "print(f\"After dropna(): {len(df_dropped)} rows\")\n",
    "print(f\"After fillna (median): {df_filled2['income'].isnull().sum()} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check duplicates on specific columns\n",
    "print(f\"Duplicate user_ids: {df.duplicated(subset=['user_id']).sum()}\")\n",
    "\n",
    "# Remove duplicates (keep first occurrence)\n",
    "df_dedup = df.drop_duplicates(subset=['user_id'], keep='first')\n",
    "\n",
    "# Data validation\n",
    "print(\"\\nData quality checks:\")\n",
    "print(f\"  Negative age: {(df['age'] < 0).sum()}\")\n",
    "print(f\"  Negative income: {(df['income'] < 0).sum()}\")\n",
    "print(f\"  Invalid category: {(~df['category'].isin(['A', 'B', 'C'])).sum()}\")\n",
    "\n",
    "# Remove outliers (simple method)\n",
    "def remove_outliers(df, column, n_std=3):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    return df[(df[column] >= mean - n_std*std) & (df[column] <= mean + n_std*std)]\n",
    "\n",
    "df_no_outliers = remove_outliers(df, 'income', n_std=3)\n",
    "print(f\"\\nAfter outlier removal: {len(df)} -> {len(df_no_outliers)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric transformations\n",
    "df['log_income'] = np.log1p(df['income'])  # log1p(x) = log(1+x) (handles 0)\n",
    "df['income_per_age'] = df['income'] / df['age']\n",
    "df['ctr'] = df['conversions'] / (df['clicks'] + 1)  # Click-through rate\n",
    "\n",
    "# Binning (discretization)\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "df['income_quartile'] = pd.qcut(df['income'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Datetime features\n",
    "df['hour'] = df['signup_date'].dt.hour\n",
    "df['day_of_week'] = df['signup_date'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Categorical encoding\n",
    "# One-hot encoding\n",
    "category_dummies = pd.get_dummies(df['category'], prefix='cat')\n",
    "df = pd.concat([df, category_dummies], axis=1)\n",
    "\n",
    "# Label encoding (simple)\n",
    "df['category_encoded'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "# Interaction features\n",
    "df['age_x_clicks'] = df['age'] * df['clicks']\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df[['age', 'age_group', 'income', 'log_income', 'hour', 'is_weekend']].head())\n",
    "print(f\"\\nShape after feature engineering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Common Feature Engineering Patterns\n",
    "\n",
    "```python\n",
    "# Aggregation features (group statistics)\n",
    "df['user_avg_clicks'] = df.groupby('user_id')['clicks'].transform('mean')\n",
    "\n",
    "# Lag features (time series)\n",
    "df['prev_clicks'] = df['clicks'].shift(1)\n",
    "\n",
    "# Rolling statistics\n",
    "df['rolling_mean_7d'] = df['clicks'].rolling(window=7).mean()\n",
    "\n",
    "# Target encoding (be careful of leakage!)\n",
    "target_means = df.groupby('category')['conversions'].mean()\n",
    "df['category_target_enc'] = df['category'].map(target_means)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GroupBy Operations\n",
    "\n",
    "### Split-Apply-Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupby\n",
    "category_stats = df.groupby('category').agg({\n",
    "    'age': ['mean', 'std'],\n",
    "    'income': ['mean', 'median'],\n",
    "    'clicks': 'sum',\n",
    "    'conversions': 'sum'\n",
    "})\n",
    "\n",
    "print(\"Category statistics:\")\n",
    "print(category_stats)\n",
    "\n",
    "# Multiple groupby keys\n",
    "group_stats = df.groupby(['category', 'age_group'])['income'].agg(['mean', 'count'])\n",
    "print(\"\\nCategory x Age Group:\")\n",
    "print(group_stats.head(6))\n",
    "\n",
    "# Transform (broadcast aggregation back)\n",
    "df['category_mean_income'] = df.groupby('category')['income'].transform('mean')\n",
    "df['income_vs_category_mean'] = df['income'] - df['category_mean_income']\n",
    "\n",
    "# Apply custom function\n",
    "def custom_metric(group):\n",
    "    return (group['conversions'].sum() / group['clicks'].sum()) if group['clicks'].sum() > 0 else 0\n",
    "\n",
    "category_ctr = df.groupby('category').apply(custom_metric)\n",
    "print(\"\\nCategory CTR:\")\n",
    "print(category_ctr)\n",
    "\n",
    "# Filter groups\n",
    "large_groups = df.groupby('category').filter(lambda x: len(x) > 300)\n",
    "print(f\"\\nLarge groups: {len(large_groups)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Aggregations (Pandas 0.25+)\n",
    "\n",
    "Cleaner syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named aggregations (better column names)\n",
    "summary = df.groupby('category').agg(\n",
    "    avg_age=('age', 'mean'),\n",
    "    median_income=('income', 'median'),\n",
    "    total_clicks=('clicks', 'sum'),\n",
    "    conversion_rate=('conversions', 'mean'),\n",
    "    count=('user_id', 'count')\n",
    ")\n",
    "\n",
    "print(\"Summary with named aggregations:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merge & Join\n",
    "\n",
    "### Combining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional data\n",
    "user_metadata = pd.DataFrame({\n",
    "    'user_id': np.random.choice(df['user_id'], 500),\n",
    "    'country': np.random.choice(['US', 'UK', 'DE', 'FR'], 500),\n",
    "    'premium': np.random.choice([0, 1], 500)\n",
    "})\n",
    "\n",
    "# Inner join (only matching keys)\n",
    "merged_inner = df.merge(user_metadata, on='user_id', how='inner')\n",
    "\n",
    "# Left join (keep all from left)\n",
    "merged_left = df.merge(user_metadata, on='user_id', how='left')\n",
    "\n",
    "# Multiple key join\n",
    "# merged = df1.merge(df2, on=['key1', 'key2'], how='inner')\n",
    "\n",
    "# Join on index\n",
    "# merged = df1.join(df2, how='left')\n",
    "\n",
    "print(f\"Original df: {len(df)} rows\")\n",
    "print(f\"User metadata: {len(user_metadata)} rows\")\n",
    "print(f\"Inner merge: {len(merged_inner)} rows\")\n",
    "print(f\"Left merge: {len(merged_left)} rows\")\n",
    "print(f\"Missing after left join: {merged_left['country'].isnull().sum()}\")\n",
    "\n",
    "# Concatenate (stack DataFrames)\n",
    "df1 = df.head(100)\n",
    "df2 = df.tail(100)\n",
    "stacked = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(f\"\\nStacked: {len(stacked)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Merge Types\n",
    "\n",
    "```python\n",
    "# how='inner': Only matching keys (intersection)\n",
    "# how='left': All from left, matching from right\n",
    "# how='right': All from right, matching from left\n",
    "# how='outer': All keys (union)\n",
    "\n",
    "# Indicator to track merge source\n",
    "merged = df1.merge(df2, on='key', how='outer', indicator=True)\n",
    "merged['_merge'].value_counts()\n",
    "# left_only: Only in df1\n",
    "# right_only: Only in df2\n",
    "# both: In both\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Operations\n",
    "\n",
    "### Apply, Map, Applymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply: Apply function along axis\n",
    "# Row-wise (axis=1)\n",
    "def compute_score(row):\n",
    "    return row['age'] * 0.1 + row['clicks'] * 2\n",
    "\n",
    "df['score'] = df.apply(compute_score, axis=1)\n",
    "\n",
    "# Column-wise (axis=0)\n",
    "numeric_cols = ['age', 'income', 'clicks']\n",
    "normalized = df[numeric_cols].apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "\n",
    "# map: Element-wise for Series\n",
    "category_map = {'A': 'Type_A', 'B': 'Type_B', 'C': 'Type_C'}\n",
    "df['category_mapped'] = df['category'].map(category_map)\n",
    "\n",
    "# applymap: Element-wise for DataFrame (deprecated, use map)\n",
    "# df_rounded = df[numeric_cols].applymap(lambda x: round(x, 2))\n",
    "\n",
    "print(\"Score sample:\")\n",
    "print(df[['age', 'clicks', 'score']].head())\n",
    "print(\"\\nNormalized sample:\")\n",
    "print(normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text column\n",
    "df['user_name'] = ['User_' + str(i) for i in range(len(df))]\n",
    "\n",
    "# String operations (vectorized)\n",
    "df['name_length'] = df['user_name'].str.len()\n",
    "df['name_upper'] = df['user_name'].str.upper()\n",
    "df['name_contains_5'] = df['user_name'].str.contains('5')\n",
    "\n",
    "# Extract patterns\n",
    "df['user_num'] = df['user_name'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "print(\"String operations:\")\n",
    "print(df[['user_name', 'name_length', 'name_contains_5', 'user_num']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Tips\n",
    "\n",
    "### Optimize your Pandas code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large DataFrame\n",
    "large_df = pd.DataFrame({\n",
    "    'A': np.random.randn(100000),\n",
    "    'B': np.random.randn(100000),\n",
    "    'C': np.random.choice(['X', 'Y', 'Z'], 100000)\n",
    "})\n",
    "\n",
    "# ‚ùå SLOW: Iterrows\n",
    "start = time.time()\n",
    "result = []\n",
    "for idx, row in large_df.head(1000).iterrows():\n",
    "    result.append(row['A'] + row['B'])\n",
    "time_iterrows = time.time() - start\n",
    "\n",
    "# ‚úÖ FAST: Vectorized\n",
    "start = time.time()\n",
    "result_vec = large_df.head(1000)['A'] + large_df.head(1000)['B']\n",
    "time_vectorized = time.time() - start\n",
    "\n",
    "print(f\"Iterrows: {time_iterrows:.4f}s\")\n",
    "print(f\"Vectorized: {time_vectorized:.4f}s\")\n",
    "print(f\"Speedup: {time_iterrows/time_vectorized:.1f}x\")\n",
    "\n",
    "# Other tips\n",
    "print(\"\\nüí° Performance tips:\")\n",
    "print(\"1. Use vectorized operations (avoid iterrows)\")\n",
    "print(\"2. Use query() instead of boolean indexing for large datasets\")\n",
    "print(\"3. Use categorical dtype for low-cardinality strings\")\n",
    "print(\"4. Read CSV with dtype specification and usecols\")\n",
    "print(\"5. Use eval() for complex expressions\")\n",
    "\n",
    "# Categorical dtype example\n",
    "df['category'] = df['category'].astype('category')\n",
    "print(f\"\\nMemory saved with categorical: ~{df.memory_usage()['category'] / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ML Pipeline Integration\n",
    "\n",
    "### Convert to NumPy/ML-ready format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for ML\n",
    "feature_cols = ['age', 'log_income', 'clicks', 'hour', 'is_weekend', 'cat_A', 'cat_B', 'cat_C']\n",
    "target_col = 'conversions'\n",
    "\n",
    "# Handle missing values\n",
    "df_ml = df[feature_cols + [target_col]].copy()\n",
    "df_ml = df_ml.dropna()\n",
    "\n",
    "# Convert to NumPy\n",
    "X = df_ml[feature_cols].values\n",
    "y = df_ml[target_col].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X dtype: {X.dtype}\")\n",
    "print(f\"Class balance: {y.mean():.2%} positive\")\n",
    "\n",
    "# Save/load\n",
    "# df.to_csv('data.csv', index=False)\n",
    "# df.to_parquet('data.parquet')  # Faster, smaller\n",
    "# df = pd.read_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Must Know for ML/DL\n",
    "\n",
    "1. **Indexing**: `.loc[]` (label), `.iloc[]` (position), `[]` (columns/boolean)\n",
    "2. **Missing values**: `dropna()`, `fillna()`, check with `isnull()`\n",
    "3. **GroupBy**: Essential for feature engineering and EDA\n",
    "4. **Merge**: Combine datasets (`inner`, `left`, `right`, `outer`)\n",
    "5. **Vectorization**: ALWAYS prefer over loops\n",
    "6. **Datetime**: Extract features with `.dt` accessor\n",
    "\n",
    "### Common ML Preprocessing\n",
    "\n",
    "```python\n",
    "# 1. Load data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Handle missing\n",
    "df = df.dropna(subset=['important_col'])\n",
    "df['other_col'] = df['other_col'].fillna(df['other_col'].median())\n",
    "\n",
    "# 3. Feature engineering\n",
    "df['new_feature'] = df['col1'] / df['col2']\n",
    "df['category_encoded'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "# 4. Split features and target\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "1. Vectorize everything\n",
    "2. Use `query()` for complex filters\n",
    "3. Categorical dtype for string columns\n",
    "4. Specify dtypes when reading CSV\n",
    "5. Use Parquet for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Matplotlib & Seaborn for visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
