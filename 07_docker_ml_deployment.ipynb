{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê≥ Docker for ML Deployment\n",
    "\n",
    "**M·ª•c ti√™u:** Master Docker ƒë·ªÉ deploy ML models\n",
    "\n",
    "**N·ªôi dung:**\n",
    "- Docker fundamentals\n",
    "- Dockerfile for ML projects\n",
    "- Docker best practices\n",
    "- Multi-stage builds\n",
    "- Docker Compose for multi-service\n",
    "- FastAPI + Docker deployment\n",
    "- GPU support\n",
    "\n",
    "**Level:** Intermediate\n",
    "\n",
    "**Note:** Notebook n√†y ch·ª©a code examples v√† best practices. Actual Docker commands run in terminal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Docker Fundamentals\n",
    "\n",
    "## 1.1 Why Docker for ML?\n",
    "\n",
    "### Problems Docker Solves\n",
    "\n",
    "‚ùå **Without Docker:**\n",
    "- \"Works on my machine\" syndrome\n",
    "- Dependency hell (CUDA versions, Python versions)\n",
    "- Hard to reproduce environments\n",
    "- Difficult deployment\n",
    "\n",
    "‚úÖ **With Docker:**\n",
    "- Consistent environment (dev = prod)\n",
    "- Package dependencies with code\n",
    "- Easy to share and deploy\n",
    "- Isolated environments\n",
    "\n",
    "## 1.2 Core Concepts\n",
    "\n",
    "### Image vs Container\n",
    "\n",
    "**Image** = Blueprint (read-only template)\n",
    "- Contains: OS, libraries, code, dependencies\n",
    "- Stored in Docker registry (Docker Hub, ECR, etc.)\n",
    "- Built from Dockerfile\n",
    "\n",
    "**Container** = Running instance of image\n",
    "- Isolated process\n",
    "- Has its own filesystem, network, processes\n",
    "- Can be started, stopped, deleted\n",
    "\n",
    "```\n",
    "Dockerfile ‚Üí (build) ‚Üí Image ‚Üí (run) ‚Üí Container\n",
    "```\n",
    "\n",
    "### Key Commands\n",
    "\n",
    "```bash\n",
    "# Images\n",
    "docker build -t my-image .          # Build image from Dockerfile\n",
    "docker images                       # List images\n",
    "docker rmi my-image                 # Remove image\n",
    "\n",
    "# Containers\n",
    "docker run -p 8000:8000 my-image   # Run container\n",
    "docker ps                           # List running containers\n",
    "docker ps -a                        # List all containers\n",
    "docker stop container-id            # Stop container\n",
    "docker rm container-id              # Remove container\n",
    "\n",
    "# Execute commands in running container\n",
    "docker exec -it container-id bash   # Interactive shell\n",
    "\n",
    "# Logs\n",
    "docker logs container-id            # View logs\n",
    "docker logs -f container-id         # Follow logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Dockerfile for ML Projects\n",
    "\n",
    "## 2.1 Basic Dockerfile\n",
    "\n",
    "### Example: Simple ML API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.basic\n",
    "# Basic Dockerfile for ML API\n",
    "\n",
    "# Base image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first (for caching)\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile Instructions\n",
    "\n",
    "| Instruction | Purpose | Example |\n",
    "|-------------|---------|----------|\n",
    "| `FROM` | Base image | `FROM python:3.9-slim` |\n",
    "| `WORKDIR` | Set working directory | `WORKDIR /app` |\n",
    "| `COPY` | Copy files from host to container | `COPY . .` |\n",
    "| `RUN` | Execute command during build | `RUN pip install -r requirements.txt` |\n",
    "| `ENV` | Set environment variable | `ENV MODEL_PATH=/models/model.pt` |\n",
    "| `EXPOSE` | Document port (not actually open) | `EXPOSE 8000` |\n",
    "| `CMD` | Default command when container starts | `CMD [\"python\", \"app.py\"]` |\n",
    "| `ENTRYPOINT` | Command that always runs | `ENTRYPOINT [\"python\"]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Optimized Dockerfile for ML\n",
    "\n",
    "### Best Practices Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.optimized\n",
    "# Optimized Dockerfile for PyTorch ML API\n",
    "\n",
    "# Use official PyTorch base (includes CUDA if needed)\n",
    "FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1 \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy and install Python dependencies FIRST (for caching)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY ./app /app/app\n",
    "COPY ./models /app/models\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Optimization Techniques\n",
    "\n",
    "1. **Layer Caching**\n",
    "   - Copy `requirements.txt` BEFORE code\n",
    "   - Dependencies change less frequently\n",
    "   - Faster rebuilds\n",
    "\n",
    "2. **Slim Base Images**\n",
    "   - `python:3.9-slim` vs `python:3.9` (5x smaller)\n",
    "   - Remove unnecessary packages\n",
    "\n",
    "3. **No Cache Flags**\n",
    "   - `pip install --no-cache-dir` (smaller image)\n",
    "   - `apt-get` cleanup after install\n",
    "\n",
    "4. **Non-root User**\n",
    "   - Security best practice\n",
    "   - Prevents privilege escalation\n",
    "\n",
    "5. **Health Checks**\n",
    "   - Monitor container health\n",
    "   - Auto-restart unhealthy containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Multi-Stage Builds\n",
    "\n",
    "### Problem\n",
    "Including build tools (compilers, dev headers) in final image ‚Üí Large size\n",
    "\n",
    "### Solution\n",
    "Build in one stage, copy artifacts to smaller final stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.multistage\n",
    "# Multi-stage build for smaller final image\n",
    "\n",
    "# ============ Stage 1: Builder ============\n",
    "FROM python:3.9 AS builder\n",
    "\n",
    "WORKDIR /build\n",
    "\n",
    "# Install build dependencies\n",
    "RUN pip install --upgrade pip\n",
    "COPY requirements.txt .\n",
    "RUN pip wheel --no-cache-dir --wheel-dir /build/wheels -r requirements.txt\n",
    "\n",
    "# ============ Stage 2: Runtime ============\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy only wheels from builder stage\n",
    "COPY --from=builder /build/wheels /wheels\n",
    "COPY --from=builder /build/requirements.txt .\n",
    "\n",
    "# Install from wheels (faster, no compilation)\n",
    "RUN pip install --no-cache-dir --no-index --find-links=/wheels -r requirements.txt \\\n",
    "    && rm -rf /wheels\n",
    "\n",
    "# Copy application\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\n",
    "# Result: Much smaller final image (no build tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "- **Smaller image**: 1GB ‚Üí 300MB (typical)\n",
    "- **Faster deployment**: Less data to transfer\n",
    "- **More secure**: Fewer attack surfaces\n",
    "- **Keep build tools out of production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. FastAPI + Docker Example\n",
    "\n",
    "## 3.1 Simple ML API with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/main.py\n",
    "# FastAPI ML inference server\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI(title=\"ML Model API\", version=\"1.0\")\n",
    "\n",
    "# Simple model (replace with your actual model)\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Load model at startup\n",
    "model = SimpleModel()\n",
    "model.eval()\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float]\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "    probabilities: List[float]\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"ML Model API\", \"status\": \"running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        # Validate input\n",
    "        if len(request.features) != 10:\n",
    "            raise HTTPException(status_code=400, detail=\"Expected 10 features\")\n",
    "        \n",
    "        # Convert to tensor\n",
    "        x = torch.tensor([request.features], dtype=torch.float32)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            prediction = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            prediction=prediction,\n",
    "            probabilities=probs[0].tolist()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "torch==2.1.0\n",
    "numpy==1.24.3\n",
    "pydantic==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .dockerignore\n",
    "# Don't copy these to Docker image\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "*.egg-info\n",
    ".pytest_cache\n",
    ".git\n",
    ".gitignore\n",
    "*.md\n",
    "notebooks/\n",
    "tests/\n",
    ".venv/\n",
    "venv/\n",
    "*.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build and Run\n",
    "\n",
    "### Terminal Commands\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t ml-api:v1 .\n",
    "\n",
    "# Run container\n",
    "docker run -d \\\n",
    "  --name ml-api \\\n",
    "  -p 8000:8000 \\\n",
    "  ml-api:v1\n",
    "\n",
    "# Check logs\n",
    "docker logs -f ml-api\n",
    "\n",
    "# Test API\n",
    "curl http://localhost:8000/health\n",
    "\n",
    "# Stop container\n",
    "docker stop ml-api\n",
    "\n",
    "# Remove container\n",
    "docker rm ml-api\n",
    "```\n",
    "\n",
    "### Python Test Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(\"http://localhost:8000/health\")\n",
    "print(f\"Health check: {response.json()}\")\n",
    "\n",
    "# Test prediction\n",
    "features = np.random.randn(10).tolist()\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/predict\",\n",
    "    json={\"features\": features}\n",
    ")\n",
    "\n",
    "print(f\"\\nPrediction: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Docker Compose for Multi-Service\n",
    "\n",
    "## 4.1 Why Docker Compose?\n",
    "\n",
    "### Scenario\n",
    "ML system with multiple components:\n",
    "- API server (FastAPI)\n",
    "- Redis cache\n",
    "- PostgreSQL database\n",
    "- Model inference worker\n",
    "\n",
    "### Problem\n",
    "Managing multiple `docker run` commands is tedious\n",
    "\n",
    "### Solution\n",
    "Docker Compose = Multi-container orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # API Server\n",
    "  api:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - POSTGRES_HOST=postgres\n",
    "      - MODEL_PATH=/models/model.pt\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "    depends_on:\n",
    "      - redis\n",
    "      - postgres\n",
    "    restart: unless-stopped\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Redis Cache\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis-data:/data\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # PostgreSQL Database\n",
    "  postgres:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_USER: mluser\n",
    "      POSTGRES_PASSWORD: mlpassword\n",
    "      POSTGRES_DB: mldb\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres-data:/var/lib/postgresql/data\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "  # Model Inference Worker (optional)\n",
    "  worker:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.worker\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "    volumes:\n",
    "      - ./models:/models\n",
    "    depends_on:\n",
    "      - redis\n",
    "    deploy:\n",
    "      replicas: 2  # Scale workers\n",
    "    networks:\n",
    "      - ml-network\n",
    "\n",
    "volumes:\n",
    "  redis-data:\n",
    "  postgres-data:\n",
    "\n",
    "networks:\n",
    "  ml-network:\n",
    "    driver: bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Docker Compose Commands\n",
    "\n",
    "```bash\n",
    "# Start all services\n",
    "docker-compose up -d\n",
    "\n",
    "# View logs\n",
    "docker-compose logs -f api\n",
    "\n",
    "# Scale service\n",
    "docker-compose up -d --scale worker=3\n",
    "\n",
    "# Stop all services\n",
    "docker-compose down\n",
    "\n",
    "# Stop and remove volumes (clean slate)\n",
    "docker-compose down -v\n",
    "\n",
    "# Rebuild images\n",
    "docker-compose build\n",
    "\n",
    "# View running services\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "## 4.3 Key Features\n",
    "\n",
    "### Environment Variables\n",
    "```yaml\n",
    "environment:\n",
    "  - REDIS_HOST=redis\n",
    "  - DEBUG=false\n",
    "```\n",
    "\n",
    "### Volumes (Persistent Data)\n",
    "```yaml\n",
    "volumes:\n",
    "  - ./models:/models          # Bind mount (host -> container)\n",
    "  - postgres-data:/var/lib/postgresql/data  # Named volume\n",
    "```\n",
    "\n",
    "### Networks (Service Communication)\n",
    "```yaml\n",
    "networks:\n",
    "  - ml-network\n",
    "```\n",
    "Services can reach each other by service name (e.g., `http://api:8000`)\n",
    "\n",
    "### Dependencies\n",
    "```yaml\n",
    "depends_on:\n",
    "  - redis\n",
    "  - postgres\n",
    "```\n",
    "Start order: redis, postgres ‚Üí api\n",
    "\n",
    "### Health Checks\n",
    "```yaml\n",
    "healthcheck:\n",
    "  test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "  interval: 30s\n",
    "  timeout: 3s\n",
    "  retries: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. GPU Support with Docker\n",
    "\n",
    "## 5.1 NVIDIA Docker\n",
    "\n",
    "### Requirements\n",
    "1. NVIDIA GPU\n",
    "2. NVIDIA drivers installed\n",
    "3. `nvidia-docker2` package\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "# Install nvidia-docker\n",
    "distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n",
    "  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y nvidia-docker2\n",
    "sudo systemctl restart docker\n",
    "```\n",
    "\n",
    "### Dockerfile with CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile.gpu\n",
    "# GPU-enabled Dockerfile\n",
    "\n",
    "FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n",
    "\n",
    "# Install Python\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 \\\n",
    "    python3-pip \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"python3\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with GPU\n",
    "\n",
    "```bash\n",
    "# Run with GPU access\n",
    "docker run --gpus all -p 8000:8000 ml-api-gpu:v1\n",
    "\n",
    "# Specify number of GPUs\n",
    "docker run --gpus 2 -p 8000:8000 ml-api-gpu:v1\n",
    "\n",
    "# Specify GPU device IDs\n",
    "docker run --gpus '\"device=0,1\"' -p 8000:8000 ml-api-gpu:v1\n",
    "```\n",
    "\n",
    "### Docker Compose with GPU\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  ml-api:\n",
    "    build: .\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Best Practices\n",
    "\n",
    "## 6.1 Image Size Optimization\n",
    "\n",
    "### Tips\n",
    "\n",
    "1. **Use slim/alpine base images**\n",
    "   ```dockerfile\n",
    "   FROM python:3.9-slim  # ~150MB vs python:3.9 ~900MB\n",
    "   ```\n",
    "\n",
    "2. **Multi-stage builds**\n",
    "   - Build stage: Install dependencies\n",
    "   - Runtime stage: Copy only artifacts\n",
    "\n",
    "3. **Minimize layers**\n",
    "   ```dockerfile\n",
    "   # ‚ùå Multiple layers\n",
    "   RUN apt-get update\n",
    "   RUN apt-get install -y package1\n",
    "   RUN apt-get install -y package2\n",
    "   \n",
    "   # ‚úÖ Single layer\n",
    "   RUN apt-get update && apt-get install -y \\\n",
    "       package1 \\\n",
    "       package2 \\\n",
    "       && rm -rf /var/lib/apt/lists/*\n",
    "   ```\n",
    "\n",
    "4. **Clean up**\n",
    "   ```dockerfile\n",
    "   RUN pip install --no-cache-dir -r requirements.txt\n",
    "   RUN apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "   ```\n",
    "\n",
    "5. **`.dockerignore`**\n",
    "   - Exclude unnecessary files\n",
    "   - Faster builds, smaller context\n",
    "\n",
    "## 6.2 Security\n",
    "\n",
    "### Tips\n",
    "\n",
    "1. **Non-root user**\n",
    "   ```dockerfile\n",
    "   RUN useradd -m -u 1000 appuser\n",
    "   USER appuser\n",
    "   ```\n",
    "\n",
    "2. **Scan for vulnerabilities**\n",
    "   ```bash\n",
    "   docker scan my-image:latest\n",
    "   ```\n",
    "\n",
    "3. **Use official images**\n",
    "   - Trusted sources\n",
    "   - Regularly updated\n",
    "\n",
    "4. **Don't store secrets in image**\n",
    "   ```bash\n",
    "   # ‚úÖ Use environment variables\n",
    "   docker run -e API_KEY=$API_KEY my-image\n",
    "   \n",
    "   # ‚úÖ Or Docker secrets\n",
    "   docker secret create api_key api_key.txt\n",
    "   ```\n",
    "\n",
    "## 6.3 Development Workflow\n",
    "\n",
    "### Hot Reload with Volumes\n",
    "\n",
    "```bash\n",
    "# Mount code directory\n",
    "docker run -v $(pwd)/app:/app/app -p 8000:8000 ml-api:v1\n",
    "```\n",
    "\n",
    "Changes in host `app/` folder reflect in container immediately!\n",
    "\n",
    "### Docker Compose Dev Setup\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - ./app:/app/app  # Hot reload\n",
    "    environment:\n",
    "      - DEBUG=true\n",
    "    command: uvicorn app.main:app --reload --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## Essential Commands\n",
    "\n",
    "```bash\n",
    "# Build\n",
    "docker build -t my-image:v1 .\n",
    "\n",
    "# Run\n",
    "docker run -d -p 8000:8000 --name my-container my-image:v1\n",
    "\n",
    "# Logs\n",
    "docker logs -f my-container\n",
    "\n",
    "# Execute\n",
    "docker exec -it my-container bash\n",
    "\n",
    "# Stop & Remove\n",
    "docker stop my-container\n",
    "docker rm my-container\n",
    "\n",
    "# Docker Compose\n",
    "docker-compose up -d      # Start\n",
    "docker-compose down       # Stop\n",
    "docker-compose logs -f    # Logs\n",
    "```\n",
    "\n",
    "## Dockerfile Best Practices\n",
    "\n",
    "1. ‚úÖ **Order matters**: Put frequently changing files last\n",
    "2. ‚úÖ **Use `.dockerignore`**: Exclude unnecessary files\n",
    "3. ‚úÖ **Combine RUN commands**: Minimize layers\n",
    "4. ‚úÖ **Clean up**: Remove caches and temp files\n",
    "5. ‚úÖ **Multi-stage builds**: Smaller final images\n",
    "6. ‚úÖ **Non-root user**: Security\n",
    "7. ‚úÖ **Health checks**: Monitor container health\n",
    "\n",
    "## Common Patterns\n",
    "\n",
    "### Basic ML API\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\"]\n",
    "```\n",
    "\n",
    "### With GPU Support\n",
    "```dockerfile\n",
    "FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n",
    "# ... install PyTorch with CUDA\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker run --gpus all my-gpu-image\n",
    "```\n",
    "\n",
    "### Multi-Service with Compose\n",
    "```yaml\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    ports: [\"8000:8000\"]\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "  postgres:\n",
    "    image: postgres:15\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "```bash\n",
    "# Check logs\n",
    "docker logs container-name\n",
    "\n",
    "# Inspect container\n",
    "docker inspect container-name\n",
    "\n",
    "# Interactive shell\n",
    "docker exec -it container-name bash\n",
    "\n",
    "# Check resource usage\n",
    "docker stats\n",
    "\n",
    "# Clean up\n",
    "docker system prune -a  # Remove unused containers, images, networks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** timm (PyTorch Image Models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
