{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Transformers Review: Architecture & HuggingFace\n",
    "\n",
    "**M·ª•c ti√™u:** Hi·ªÉu Transformer architecture v√† s·ª≠ d·ª•ng HuggingFace cho NLP & Vision\n",
    "\n",
    "**N·ªôi dung:**\n",
    "- Transformer architecture fundamentals\n",
    "- Self-attention mechanism\n",
    "- HuggingFace Transformers library\n",
    "- Pre-trained models (BERT, GPT, ViT)\n",
    "- Pipeline API & inference\n",
    "- Fine-tuning patterns\n",
    "- Vision Transformers (ViT, CLIP)\n",
    "\n",
    "**Level:** Intermediate to Advanced\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "# !pip install transformers torch torchvision datasets pillow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers_version}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Transformer Architecture Fundamentals\n",
    "\n",
    "## 1.1 Core Components\n",
    "\n",
    "### Transformer = Encoder + Decoder (original)\n",
    "\n",
    "```\n",
    "Input ‚Üí Embedding ‚Üí Positional Encoding ‚Üí \n",
    "  ‚Üì\n",
    "Encoder (N layers):\n",
    "  - Multi-Head Self-Attention\n",
    "  - Add & Norm\n",
    "  - Feed-Forward Network\n",
    "  - Add & Norm\n",
    "  ‚Üì\n",
    "Decoder (N layers):\n",
    "  - Masked Multi-Head Self-Attention\n",
    "  - Add & Norm\n",
    "  - Cross-Attention (to encoder)\n",
    "  - Add & Norm\n",
    "  - Feed-Forward Network\n",
    "  - Add & Norm\n",
    "  ‚Üì\n",
    "Output ‚Üí Linear ‚Üí Softmax\n",
    "```\n",
    "\n",
    "### Key Innovations\n",
    "1. **Self-Attention**: All positions attend to all positions\n",
    "2. **Parallel Processing**: No sequential dependency (unlike RNNs)\n",
    "3. **Positional Encoding**: Since no recurrence, need position info\n",
    "\n",
    "### Modern Variants\n",
    "- **Encoder-only**: BERT (bidirectional, good for understanding)\n",
    "- **Decoder-only**: GPT (unidirectional, good for generation)\n",
    "- **Encoder-Decoder**: T5, BART (seq2seq tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Self-Attention Mechanism\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Query (what I'm looking for)\n",
    "- $K$ = Key (what I have)\n",
    "- $V$ = Value (what I return)\n",
    "- $d_k$ = dimension of keys (scaling factor)\n",
    "\n",
    "### Intuition\n",
    "1. Compute similarity between Query and all Keys (dot product)\n",
    "2. Scale by $\\sqrt{d_k}$ (prevent large values)\n",
    "3. Softmax to get attention weights\n",
    "4. Weighted sum of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Scaled Dot-Product Attention\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: Query (batch, seq_len, d_k)\n",
    "            K: Key (batch, seq_len, d_k)\n",
    "            V: Value (batch, seq_len, d_v)\n",
    "            mask: Optional mask (batch, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_v)\n",
    "            attention_weights: (batch, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "attention = ScaledDotProductAttention()\n",
    "output, weights = attention(Q, K, V)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights (first sample):\")\n",
    "print(weights[0].detach().numpy())\n",
    "print(f\"\\nWeights sum to 1: {weights[0].sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Multi-Head Attention\n",
    "\n",
    "### Why Multi-Head?\n",
    "\n",
    "Instead of single attention:\n",
    "- Learn **multiple attention patterns** in parallel\n",
    "- Different heads can focus on different aspects (e.g., syntax, semantics)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Implementation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q, K, V: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections and split into multiple heads\n",
    "        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        # (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "d_model, num_heads = 512, 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "X = torch.randn(2, 10, d_model)  # (batch, seq_len, d_model)\n",
    "output, weights = mha(X, X, X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\n‚úÖ Multi-head attention with {num_heads} heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Positional Encoding\n",
    "\n",
    "### Problem\n",
    "Self-attention is **permutation-invariant** ‚Üí No position information!\n",
    "\n",
    "### Solution\n",
    "Add positional encoding to input embeddings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "### Why Sinusoidal?\n",
    "- Can extrapolate to longer sequences\n",
    "- Relative positions have consistent patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 128\n",
    "pe = PositionalEncoding(d_model, max_len=100)\n",
    "\n",
    "# Get encoding\n",
    "encoding = pe.pe[0].numpy()  # (max_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(encoding.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.ylabel('Dimension', fontsize=12)\n",
    "plt.title('Positional Encoding Visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Positional encoding pattern:\")\n",
    "print(\"   - Alternating sin/cos functions\")\n",
    "print(\"   - Different frequencies for different dimensions\")\n",
    "print(\"   - Captures relative position information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. HuggingFace Transformers Library\n",
    "\n",
    "## 2.1 Core Concepts\n",
    "\n",
    "### Three Main Components\n",
    "1. **Models**: Pre-trained architectures (BERT, GPT, T5, ViT, etc.)\n",
    "2. **Tokenizers**: Convert text to tokens\n",
    "3. **Pipelines**: High-level API for inference\n",
    "\n",
    "### Model Hub\n",
    "- https://huggingface.co/models\n",
    "- 100,000+ pre-trained models\n",
    "- Easy to load and fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"‚úÖ HuggingFace Transformers imported successfully\")\n",
    "print(\"\\nüí° Key classes:\")\n",
    "print(\"   AutoTokenizer: Automatically load correct tokenizer\")\n",
    "print(\"   AutoModel: Load base model\")\n",
    "print(\"   AutoModelFor*: Task-specific models\")\n",
    "print(\"   pipeline: High-level inference API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pipeline API (Quickest Way)\n",
    "\n",
    "### Common Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"1Ô∏è‚É£ Sentiment Analysis\")\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this movie! It's amazing.\")\n",
    "print(f\"   Result: {result}\")\n",
    "\n",
    "# Multiple texts\n",
    "texts = [\n",
    "    \"This is great!\",\n",
    "    \"I hate this.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "results = classifier(texts)\n",
    "print(f\"\\n   Batch results:\")\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"   '{text}' ‚Üí {result['label']} ({result['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print(\"\\n2Ô∏è‚É£ Named Entity Recognition\")\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "text = \"Apple Inc. is located in Cupertino, California. Tim Cook is the CEO.\"\n",
    "entities = ner(text)\n",
    "\n",
    "print(f\"   Text: {text}\")\n",
    "print(f\"\\n   Entities found:\")\n",
    "for entity in entities:\n",
    "    print(f\"   - {entity['word']}: {entity['entity_group']} (score: {entity['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering\n",
    "print(\"\\n3Ô∏è‚É£ Question Answering\")\n",
    "qa = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"The Transformer is a deep learning model introduced in 2017, \n",
    "used primarily in the field of natural language processing (NLP). \n",
    "It was proposed in the paper 'Attention Is All You Need' by Vaswani et al.\"\"\"\n",
    "\n",
    "question = \"When was the Transformer introduced?\"\n",
    "answer = qa(question=question, context=context)\n",
    "\n",
    "print(f\"   Question: {question}\")\n",
    "print(f\"   Answer: {answer['answer']} (score: {answer['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "print(\"\\n4Ô∏è‚É£ Text Generation\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "output = generator(prompt, max_length=50, num_return_sequences=2, temperature=0.7)\n",
    "\n",
    "print(f\"   Prompt: {prompt}\")\n",
    "print(f\"\\n   Generated texts:\")\n",
    "for i, result in enumerate(output, 1):\n",
    "    print(f\"   {i}. {result['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Manual Model Usage (More Control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer manually\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Loaded {model_name}\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"   Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"   Num attention heads: {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "# Encode text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nTokenized:\")\n",
    "print(f\"   Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"   Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "print(f\"   Attention mask: {inputs['attention_mask']}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = tokenizer.decode(inputs['input_ids'][0])\n",
    "print(f\"\\nDecoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings\n",
    "last_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "pooler_output = outputs.pooler_output  # (batch, hidden_size) - [CLS] token\n",
    "\n",
    "print(f\"Output shapes:\")\n",
    "print(f\"   Last hidden state: {last_hidden_state.shape}\")\n",
    "print(f\"   Pooler output: {pooler_output.shape}\")\n",
    "\n",
    "# Use [CLS] token as sentence embedding\n",
    "sentence_embedding = last_hidden_state[:, 0, :]  # Same as pooler_output\n",
    "print(f\"   Sentence embedding: {sentence_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Task-Specific Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Classification (e.g., sentiment analysis)\n",
    "model_cls = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "tokenizer_cls = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "text = \"I love this product! It's amazing!\"\n",
    "inputs = tokenizer_cls(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_cls(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    prediction = torch.argmax(probs, dim=-1)\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {labels[prediction]} (prob: {probs[0][prediction]:.3f})\")\n",
    "print(f\"Probabilities: NEGATIVE={probs[0][0]:.3f}, POSITIVE={probs[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Vision Transformers (ViT)\n",
    "\n",
    "## 3.1 ViT Architecture\n",
    "\n",
    "### Key Idea\n",
    "Apply Transformer directly to **image patches**:\n",
    "\n",
    "1. Split image into patches (e.g., 16x16)\n",
    "2. Flatten patches to sequences\n",
    "3. Linear projection to get patch embeddings\n",
    "4. Add positional embeddings\n",
    "5. Apply Transformer encoder\n",
    "6. Use [CLS] token for classification\n",
    "\n",
    "```\n",
    "Image (224x224x3) ‚Üí Patches (14x14 patches of 16x16)\n",
    "  ‚Üí Flatten (196 patches of 768-dim)\n",
    "  ‚Üí Add [CLS] token (197 tokens)\n",
    "  ‚Üí Transformer Encoder\n",
    "  ‚Üí Classification Head (on [CLS])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load ViT model\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model_vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "print(f\"‚úÖ Loaded ViT model\")\n",
    "print(f\"   Patch size: {model_vit.config.patch_size}\")\n",
    "print(f\"   Image size: {model_vit.config.image_size}\")\n",
    "print(f\"   Hidden size: {model_vit.config.hidden_size}\")\n",
    "print(f\"   Num patches: {(model_vit.config.image_size // model_vit.config.patch_size) ** 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample image (or create synthetic)\n",
    "# For demo, create random image\n",
    "image = Image.new('RGB', (224, 224), color='red')\n",
    "\n",
    "# In practice:\n",
    "# image = Image.open('path/to/image.jpg')\n",
    "\n",
    "# Process image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Processed image shape: {inputs['pixel_values'].shape}\")\n",
    "print(f\"   (batch, channels, height, width)\")\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_vit(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "print(f\"\\nPredicted class ID: {predicted_class}\")\n",
    "print(f\"Predicted label: {model_vit.config.id2label[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CLIP (Contrastive Language-Image Pre-training)\n",
    "\n",
    "### Key Idea\n",
    "- Joint vision-language model\n",
    "- Image encoder + Text encoder\n",
    "- Learn shared embedding space\n",
    "- Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load CLIP\n",
    "model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor_clip = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "print(\"‚úÖ Loaded CLIP model\")\n",
    "\n",
    "# Zero-shot image classification\n",
    "image = Image.new('RGB', (224, 224), color='blue')  # Synthetic image\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n",
    "\n",
    "inputs = processor_clip(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_clip(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "print(f\"\\nZero-shot classification:\")\n",
    "for text, prob in zip(texts, probs[0]):\n",
    "    print(f\"   '{text}': {prob.item():.3f}\")\n",
    "\n",
    "print(f\"\\nüí° CLIP can classify images with arbitrary text labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Fine-tuning Patterns\n",
    "\n",
    "## 4.1 Basic Fine-tuning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Example: Fine-tune for sequence classification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration:\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "\n",
    "# Data collator (handles padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,  # Your dataset\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# Train\n",
    "# trainer.train()\n",
    "\n",
    "print(\"\\nüí° Fine-tuning pattern:\")\n",
    "print(\"   1. Load pre-trained model\")\n",
    "print(\"   2. Prepare dataset (tokenize)\")\n",
    "print(\"   3. Configure TrainingArguments\")\n",
    "print(\"   4. Create Trainer\")\n",
    "print(\"   5. trainer.train()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Common Fine-tuning Strategies\n",
    "\n",
    "### 1. Full Fine-tuning\n",
    "- Update all parameters\n",
    "- Requires more GPU memory\n",
    "- Best performance\n",
    "\n",
    "### 2. Freeze Base, Train Head\n",
    "```python\n",
    "# Freeze base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train classification head\n",
    "```\n",
    "\n",
    "### 3. Gradual Unfreezing\n",
    "- Start with frozen base\n",
    "- Gradually unfreeze layers\n",
    "\n",
    "### 4. LoRA (Low-Rank Adaptation)\n",
    "- Add small trainable matrices\n",
    "- Freeze original weights\n",
    "- Very parameter-efficient\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Key Takeaways\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "### Core Concepts\n",
    "1. **Self-Attention**: All positions attend to all positions\n",
    "   - Query, Key, Value mechanism\n",
    "   - Scaled by $\\sqrt{d_k}$\n",
    "   - Multi-head for different patterns\n",
    "\n",
    "2. **Positional Encoding**: Inject position information\n",
    "   - Sinusoidal functions\n",
    "   - Learnable alternative\n",
    "\n",
    "3. **Parallel Processing**: No sequential bottleneck\n",
    "   - Faster than RNNs\n",
    "   - Better long-range dependencies\n",
    "\n",
    "## HuggingFace Ecosystem\n",
    "\n",
    "### Quick Start\n",
    "```python\n",
    "# Pipeline API (easiest)\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this!\")\n",
    "```\n",
    "\n",
    "### Manual Control\n",
    "```python\n",
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Inference\n",
    "outputs = model(**inputs)\n",
    "```\n",
    "\n",
    "### Fine-tuning\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "## Vision Transformers\n",
    "\n",
    "### ViT (Vision Transformer)\n",
    "- Split image into patches\n",
    "- Apply standard Transformer\n",
    "- Competitive with CNNs\n",
    "\n",
    "### CLIP (Contrastive Learning)\n",
    "- Joint vision-language model\n",
    "- Zero-shot classification\n",
    "- Image-text similarity\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Start with Pipeline API** for quick experiments\n",
    "2. **Use Auto classes** (AutoTokenizer, AutoModel) for flexibility\n",
    "3. **Fine-tune from pre-trained** (don't train from scratch)\n",
    "4. **Monitor GPU memory** (use smaller models or gradient checkpointing)\n",
    "5. **Use mixed precision** (fp16) for faster training\n",
    "6. **Consider LoRA** for parameter-efficient fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Docker for ML deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
