{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñºÔ∏è timm: PyTorch Image Models\n",
    "\n",
    "**M·ª•c ti√™u:** Master timm library for computer vision\n",
    "\n",
    "**N·ªôi dung:**\n",
    "- timm overview & installation\n",
    "- Model zoo & selection\n",
    "- Transfer learning patterns\n",
    "- Feature extraction\n",
    "- Training with timm\n",
    "- Advanced techniques\n",
    "\n",
    "**Level:** Intermediate\n",
    "\n",
    "**Why timm?**\n",
    "- üèÜ State-of-the-art pretrained models\n",
    "- üì¶ Unified interface\n",
    "- ‚ö° Efficient implementations\n",
    "- üîß Easy fine-tuning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# !pip install timm\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "print(f\"‚úÖ timm version: {timm.__version__}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Model Zoo\n",
    "\n",
    "### Available Models\n",
    "\n",
    "timm c√≥ **1000+ pretrained models**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available models\n",
    "all_models = timm.list_models()\n",
    "print(f\"Total models: {len(all_models)}\")\n",
    "print(f\"\\nFirst 20 models:\")\n",
    "for model in all_models[:20]:\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "# Filter by pattern\n",
    "efficientnet_models = timm.list_models('efficientnet*')\n",
    "print(f\"\\nEfficientNet variants: {len(efficientnet_models)}\")\n",
    "print(efficientnet_models[:10])\n",
    "\n",
    "# Filter pretrained models\n",
    "resnet_pretrained = timm.list_models('resnet*', pretrained=True)\n",
    "print(f\"\\nPretrained ResNets: {len(resnet_pretrained)}\")\n",
    "\n",
    "# Popular architectures\n",
    "print(\"\\nüèÜ Popular Architectures:\")\n",
    "architectures = [\n",
    "    'resnet50',\n",
    "    'efficientnet_b0',\n",
    "    'vit_base_patch16_224',\n",
    "    'convnext_tiny',\n",
    "    'swin_tiny_patch4_window7_224',\n",
    "    'mobilenetv3_large_100'\n",
    "]\n",
    "for arch in architectures:\n",
    "    print(f\"  ‚úì {arch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Creating Models\n",
    "\n",
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with pretrained weights\n",
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Model info\n",
    "print(f\"\\nInput size: {model.default_cfg['input_size']}\")\n",
    "print(f\"Mean: {model.default_cfg['mean']}\")\n",
    "print(f\"Std: {model.default_cfg['std']}\")\n",
    "print(f\"Num classes: {model.default_cfg['num_classes']}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Number of Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model for custom dataset (e.g., 10 classes)\n",
    "model_custom = timm.create_model(\n",
    "    'efficientnet_b0',\n",
    "    pretrained=True,\n",
    "    num_classes=10  # Replace classification head\n",
    ")\n",
    "\n",
    "print(f\"Custom model classes: {model_custom.num_classes}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "output = model_custom(x)\n",
    "print(f\"Output shape: {output.shape}  # (batch_size, num_classes)\")\n",
    "\n",
    "# Model without classification head (feature extractor)\n",
    "model_features = timm.create_model(\n",
    "    'resnet50',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # Remove head\n",
    "    global_pool=''   # Remove global pooling\n",
    ")\n",
    "\n",
    "features = model_features(x)\n",
    "print(f\"Feature map shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Transfer Learning Patterns\n",
    "\n",
    "### Pattern 1: Fine-tune Last Layer Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = timm.create_model('resnet34', pretrained=True, num_classes=5)\n",
    "\n",
    "# Freeze all layers except classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' not in name:  # fc = final classification layer\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check which parameters are trainable\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
    "print(f\"Percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Setup optimizer (only for trainable parameters)\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for training (fast, good for small datasets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Gradual Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model(model, num_layers_to_unfreeze=2):\n",
    "    \"\"\"\n",
    "    Gradually unfreeze layers from end\n",
    "    \"\"\"\n",
    "    # Get all layer names\n",
    "    layers = list(model.named_parameters())\n",
    "    \n",
    "    # Unfreeze last N layers\n",
    "    for name, param in layers[-num_layers_to_unfreeze:]:\n",
    "        param.requires_grad = True\n",
    "        print(f\"Unfrozen: {name}\")\n",
    "\n",
    "# Example usage\n",
    "model = timm.create_model('resnet18', pretrained=True, num_classes=5)\n",
    "\n",
    "# Freeze all\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last 2 layers\n",
    "unfreeze_model(model, num_layers_to_unfreeze=4)\n",
    "\n",
    "print(f\"\\nüí° Gradual unfreezing strategy:\")\n",
    "print(\"   1. Train classifier only (fast)\")\n",
    "print(\"   2. Unfreeze last few layers\")\n",
    "print(\"   3. Fine-tune with lower LR\")\n",
    "print(\"   4. Repeat as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Differential Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LR for different layers\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
    "\n",
    "# Split parameters into groups\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        classifier_params.append(param)\n",
    "    else:\n",
    "        backbone_params.append(param)\n",
    "\n",
    "# Different learning rates\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': backbone_params, 'lr': 1e-5},      # Low LR for pretrained\n",
    "    {'params': classifier_params, 'lr': 1e-3}     # High LR for new layer\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Differential LR setup:\")\n",
    "print(f\"   Backbone: {len(backbone_params)} param groups, LR=1e-5\")\n",
    "print(f\"   Classifier: {len(classifier_params)} param groups, LR=1e-3\")\n",
    "print(\"\\nüí° This prevents catastrophic forgetting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Extraction\n",
    "\n",
    "### Extract Features for Downstream Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extractor\n",
    "feature_extractor = timm.create_model(\n",
    "    'resnet50',\n",
    "    pretrained=True,\n",
    "    num_classes=0,  # Remove classifier\n",
    "    global_pool='avg'  # Global average pooling\n",
    ")\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Extract features\n",
    "images = torch.randn(4, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(images)\n",
    "\n",
    "print(f\"Feature shape: {features.shape}  # (batch, feature_dim)\")\n",
    "print(f\"Feature dimension: {features.shape[1]}\")\n",
    "\n",
    "# Use features for:\n",
    "# 1. Similarity search\n",
    "# 2. Clustering\n",
    "# 3. Classical ML (SVM, KNN)\n",
    "# 4. Anomaly detection\n",
    "\n",
    "# Example: Similarity between images\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "sim = cosine_similarity(features[0].unsqueeze(0), features[1].unsqueeze(0))\n",
    "print(f\"\\nSimilarity between image 0 and 1: {sim.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from multiple layers\n",
    "model = timm.create_model('resnet50', pretrained=True, features_only=True)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = model(x)\n",
    "\n",
    "print(\"Multi-scale features:\")\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"  Level {i}: {feat.shape}\")\n",
    "\n",
    "# Useful for:\n",
    "# - Object detection (FPN)\n",
    "# - Segmentation (U-Net style)\n",
    "# - Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Preprocessing\n",
    "\n",
    "### Using timm's Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's preprocessing config\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "config = model.default_cfg\n",
    "\n",
    "print(\"Model preprocessing config:\")\n",
    "print(f\"  Input size: {config['input_size']}\")\n",
    "print(f\"  Mean: {config['mean']}\")\n",
    "print(f\"  Std: {config['std']}\")\n",
    "print(f\"  Interpolation: {config.get('interpolation', 'bilinear')}\")\n",
    "print(f\"  Crop pct: {config.get('crop_pct', 0.875)}\")\n",
    "\n",
    "# Create matching transform\n",
    "from timm.data import create_transform\n",
    "\n",
    "transform_train = create_transform(\n",
    "    input_size=config['input_size'][-2:],\n",
    "    is_training=True,\n",
    "    auto_augment='rand-m9-mstd0.5-inc1',\n",
    "    interpolation='bicubic',\n",
    "    mean=config['mean'],\n",
    "    std=config['std']\n",
    ")\n",
    "\n",
    "transform_val = create_transform(\n",
    "    input_size=config['input_size'][-2:],\n",
    "    is_training=False,\n",
    "    interpolation='bicubic',\n",
    "    mean=config['mean'],\n",
    "    std=config['std'],\n",
    "    crop_pct=config.get('crop_pct', 0.875)\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Transforms created matching model's config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Example\n",
    "\n",
    "### Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Example training setup\n",
    "print(\"üìã Training Setup Example:\")\n",
    "print(\"\"\"\n",
    "# 1. Create model\n",
    "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=10)\n",
    "model = model.to(device)\n",
    "\n",
    "# 2. Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# 3. Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = val_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Techniques\n",
    "\n",
    "### Mixed Precision Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision for faster training\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "training_example = \"\"\"\n",
    "# Setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "for inputs, labels in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward with autocast\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward with scaler\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ö° Mixed Precision Training:\")\n",
    "print(training_example)\n",
    "print(\"\\nüí° Benefits: 2-3x speedup, reduced memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA for better generalization\n",
    "ema_example = \"\"\"\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "# Create EMA model\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "model_ema = ModelEmaV2(model, decay=0.9999)\n",
    "\n",
    "# Training loop\n",
    "for inputs, labels in train_loader:\n",
    "    # Normal training step\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update EMA model\n",
    "    model_ema.update(model)\n",
    "\n",
    "# Use EMA model for inference\n",
    "ema_model = model_ema.module\n",
    "ema_model.eval()\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìà Model EMA:\")\n",
    "print(ema_example)\n",
    "print(\"\\nüí° EMA often gives 0.1-0.5% accuracy boost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### timm Essentials\n",
    "\n",
    "1. **Model Creation**\n",
    "   ```python\n",
    "   model = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "   ```\n",
    "\n",
    "2. **Feature Extraction**\n",
    "   ```python\n",
    "   model = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "   ```\n",
    "\n",
    "3. **Transfer Learning**\n",
    "   - Freeze backbone, train classifier\n",
    "   - Gradual unfreezing\n",
    "   - Differential learning rates\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Use model's default preprocessing config\n",
    "- Start with frozen backbone\n",
    "- Use differential LRs\n",
    "- Apply mixed precision training\n",
    "- Use EMA for better results\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Ignore model's preprocessing config\n",
    "- Use same LR for all layers\n",
    "- Train without augmentation\n",
    "- Forget to set model.eval() for inference\n",
    "\n",
    "### Common Workflows\n",
    "\n",
    "| Task | Recommended Models |\n",
    "|------|-------------------|\n",
    "| Image Classification | EfficientNet, ConvNeXt, ViT |\n",
    "| Feature Extraction | ResNet50, EfficientNet-B0 |\n",
    "| Real-time | MobileNet, EfficientNet-Lite |\n",
    "| High Accuracy | ConvNeXt-Large, Swin-Large |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "# List models\n",
    "timm.list_models('*efficientnet*', pretrained=True)\n",
    "\n",
    "# Create & customize\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "\n",
    "# Get config\n",
    "config = model.default_cfg\n",
    "\n",
    "# Feature extraction\n",
    "features = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
    "\n",
    "# Multi-scale features\n",
    "features = timm.create_model('resnet50', features_only=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** NLP Fundamentals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
