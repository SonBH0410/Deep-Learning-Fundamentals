{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• FILE 2-B: Optimizer, Activation & Regularization\n",
    "\n",
    "**PH·∫¶N 2 - INTERMEDIATE (CORE DEEP LEARNING)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi Dung\n",
    "\n",
    "‚úÖ Activation functions n√¢ng cao (ReLU, LeakyReLU, GELU, Swish)\n",
    "\n",
    "‚úÖ Optimizers chi ti·∫øt (SGD, Momentum, Adam, AdamW)\n",
    "\n",
    "‚úÖ Learning rate v√† ·∫£nh h∆∞·ªüng\n",
    "\n",
    "‚úÖ Learning rate scheduling\n",
    "\n",
    "‚úÖ Regularization techniques:\n",
    "- Dropout\n",
    "- Weight Decay (L2)\n",
    "- Batch Normalization\n",
    "\n",
    "‚úÖ So s√°nh v√† th·ª±c nghi·ªám\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Th·ªùi Gian H·ªçc: 2.5-3 gi·ªù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Activation Functions N√¢ng Cao\n",
    "\n",
    "## So S√°nh C√°c Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activations\n",
    "x = torch.linspace(-3, 3, 200)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': nn.ReLU()(x),\n",
    "    'LeakyReLU': nn.LeakyReLU(0.1)(x),\n",
    "    'ELU': nn.ELU()(x),\n",
    "    'GELU': nn.GELU()(x),\n",
    "    'Swish (SiLU)': nn.SiLU()(x),\n",
    "    'Tanh': nn.Tanh()(x),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, y) in enumerate(activations.items()):\n",
    "    axes[i].plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "    axes[i].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[i].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ƒê√ÅNH GI√Å:\n",
    "\n",
    "ReLU: max(0, x)\n",
    "  ‚úÖ Nhanh, ƒë∆°n gi·∫£n, ph·ªï bi·∫øn nh·∫•t\n",
    "  ‚ùå Dying ReLU problem\n",
    "  \n",
    "LeakyReLU: max(0.1x, x)\n",
    "  ‚úÖ Fix dying ReLU\n",
    "  ‚ùå C·∫ßn tune alpha\n",
    "  \n",
    "GELU: x * Œ¶(x)  [Gaussian Error Linear Unit]\n",
    "  ‚úÖ Smooth, d√πng trong Transformers (BERT, GPT)\n",
    "  ‚ùå Ch·∫≠m h∆°n ReLU\n",
    "  \n",
    "Swish/SiLU: x * sigmoid(x)\n",
    "  ‚úÖ Smooth, self-gated\n",
    "  ‚úÖ T·ªët cho deep networks\n",
    "  \n",
    "KHUY·∫æN NGH·ªä:\n",
    "  - Default: ReLU\n",
    "  - Transformers: GELU\n",
    "  - Deep networks: Swish/SiLU\n",
    "  - Dying ReLU problem: LeakyReLU\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Optimizers Chi Ti·∫øt\n",
    "\n",
    "## SGD vs Momentum vs Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZER COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train = torch.FloatTensor(X[:800])\n",
    "y_train = torch.LongTensor(y[:800])\n",
    "X_val = torch.FloatTensor(X[800:])\n",
    "y_val = torch.LongTensor(y[800:])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model\n",
    "def create_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(20, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 2)\n",
    "    )\n",
    "\n",
    "# Test different optimizers\n",
    "def train_with_optimizer(optimizer_fn, name, epochs=50):\n",
    "    model = create_model().to(device)\n",
    "    optimizer = optimizer_fn(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            losses.append(val_loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Compare optimizers\n",
    "optimizers = {\n",
    "    'SGD': lambda p: optim.SGD(p, lr=0.01),\n",
    "    'SGD+Momentum': lambda p: optim.SGD(p, lr=0.01, momentum=0.9),\n",
    "    'Adam': lambda p: optim.Adam(p, lr=0.001),\n",
    "    'AdamW': lambda p: optim.AdamW(p, lr=0.001, weight_decay=0.01),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, opt_fn in optimizers.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    results[name] = train_with_optimizer(opt_fn, name)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, losses in results.items():\n",
    "    plt.plot(losses, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Optimizer Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal losses:\")\n",
    "for name, losses in results.items():\n",
    "    print(f\"  {name:15s}: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=\" * 70\n",
    "CHI TI·∫æT C√ÅC OPTIMIZER\n",
    "=\" * 70\n",
    "\n",
    "1. SGD (Stochastic Gradient Descent)\n",
    "   Formula: Œ∏ = Œ∏ - lr * ‚àáL\n",
    "   \n",
    "   ‚úÖ ƒê∆°n gi·∫£n, ·ªïn ƒë·ªãnh\n",
    "   ‚ùå Ch·∫≠m, d·ªÖ stuck local minima\n",
    "   \n",
    "2. SGD + Momentum\n",
    "   Formula: \n",
    "     v = Œ≤*v + ‚àáL\n",
    "     Œ∏ = Œ∏ - lr * v\n",
    "   \n",
    "   ‚úÖ TƒÉng t·ªëc, v∆∞·ª£t local minima\n",
    "   ‚úÖ Th∆∞·ªùng Œ≤ = 0.9\n",
    "   \n",
    "3. Adam (Adaptive Moment Estimation)\n",
    "   Combines:\n",
    "     - Momentum (first moment)\n",
    "     - RMSprop (second moment)\n",
    "   \n",
    "   ‚úÖ T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh LR cho t·ª´ng parameter\n",
    "   ‚úÖ H·ªôi t·ª• nhanh\n",
    "   ‚úÖ PH·ªî BI·∫æN NH·∫§T\n",
    "   \n",
    "   Hyperparameters:\n",
    "     - lr: 0.001 (default)\n",
    "     - betas: (0.9, 0.999)\n",
    "     - eps: 1e-8\n",
    "   \n",
    "4. AdamW (Adam with Weight Decay)\n",
    "   = Adam + L2 regularization ƒê√öNG C√ÅCH\n",
    "   \n",
    "   ‚úÖ Fix weight decay bug trong Adam\n",
    "   ‚úÖ T·ªët h∆°n Adam cho nhi·ªÅu tasks\n",
    "   ‚úÖ D√πng trong Transformers\n",
    "   \n",
    "   weight_decay: 0.01 (typical)\n",
    "\n",
    "KHUY·∫æN NGH·ªä:\n",
    "  - B·∫Øt ƒë·∫ßu: Adam (lr=0.001)\n",
    "  - Fine-tuning: SGD + Momentum\n",
    "  - Modern: AdamW\n",
    "  - Transformers: AdamW + LR scheduling\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Learning Rate\n",
    "\n",
    "## LR Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LEARNING RATE EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Testing LR={lr}...\")\n",
    "    model = create_model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_val.to(device)), y_val.to(device))\n",
    "            losses.append(val_loss.item())\n",
    "    \n",
    "    lr_results[lr] = losses\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for lr, losses in lr_results.items():\n",
    "    plt.plot(losses, label=f'LR={lr}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate Comparison', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "K·∫æT LU·∫¨N:\n",
    "  - LR qu√° nh·ªè (0.0001): H·ªçc ch·∫≠m\n",
    "  - LR v·ª´a (0.001, 0.01): T·ªët nh·∫•t\n",
    "  - LR qu√° l·ªõn (0.1, 1.0): Kh√¥ng h·ªôi t·ª•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LEARNING RATE SCHEDULING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = create_model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Start high\n",
    "\n",
    "# StepLR: Gi·∫£m LR m·ªói N epochs\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    # Training code here...\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()  # Update LR\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('StepLR Scheduler', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "C√ÅC SCHEDULER PH·ªî BI·∫æN:\n",
    "\n",
    "1. StepLR\n",
    "   lr = lr * gamma every step_size epochs\n",
    "   \n",
    "2. ReduceLROnPlateau\n",
    "   Gi·∫£m LR khi val loss kh√¥ng c·∫£i thi·ªán\n",
    "   \n",
    "3. CosineAnnealingLR\n",
    "   LR theo cosine curve\n",
    "   \n",
    "4. OneCycleLR\n",
    "   TƒÉng r·ªìi gi·∫£m LR (super-convergence)\n",
    "\n",
    "USAGE:\n",
    "  scheduler = optim.lr_scheduler.StepLR(...)\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "      train(...)  \n",
    "      scheduler.step()  # ‚Üê After each epoch\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Regularization\n",
    "\n",
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DROPOUT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Drop neurons\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropout(dropout_rate=0.5)\n",
    "\n",
    "print(\"\"\"\n",
    "DROPOUT HO·∫†T ƒê·ªòNG:\n",
    "\n",
    "Training mode:\n",
    "  - Randomly \"drop\" p% neurons (set to 0)\n",
    "  - Prevents co-adaptation\n",
    "  - Acts like ensemble\n",
    "\n",
    "Eval mode:\n",
    "  - Use ALL neurons\n",
    "  - Scale by (1-p)\n",
    "\n",
    "L·ª¢I √çCH:\n",
    "  ‚úÖ Gi·∫£m overfitting\n",
    "  ‚úÖ ƒê∆°n gi·∫£n, hi·ªáu qu·∫£\n",
    "  ‚úÖ Ensemble effect\n",
    "\n",
    "DROPOUT RATE:\n",
    "  - Typical: 0.2 - 0.5\n",
    "  - Input layer: 0.1 - 0.2\n",
    "  - Hidden layers: 0.5\n",
    "\n",
    "‚ö†Ô∏è QUAN TR·ªåNG:\n",
    "  model.train()  # Enable dropout\n",
    "  model.eval()   # Disable dropout\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WEIGHT DECAY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Without weight decay\n",
    "optimizer_no_wd = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# With weight decay\n",
    "optimizer_wd = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(\"\"\"\n",
    "WEIGHT DECAY:\n",
    "\n",
    "Formula: L_total = L + Œª * ||w||¬≤\n",
    "\n",
    "√ù nghƒ©a:\n",
    "  - Penalize large weights\n",
    "  - Prefer smaller weights\n",
    "  - Prevent overfitting\n",
    "\n",
    "C√ÅCH D√ôNG:\n",
    "  optimizer = optim.AdamW(\n",
    "      model.parameters(),\n",
    "      lr=0.001,\n",
    "      weight_decay=0.01  # Œª\n",
    "  )\n",
    "\n",
    "GI√Å TR·ªä:\n",
    "  - Typical: 0.0001 - 0.01\n",
    "  - L·ªõn h∆°n = regularization m·∫°nh h∆°n\n",
    "\n",
    "Adam vs AdamW:\n",
    "  - Adam: Weight decay implementation c√≥ bug\n",
    "  - AdamW: Correct implementation\n",
    "  - Khuy·∫øn ngh·ªã: D√πng AdamW\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BATCH NORMALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class ModelWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # BatchNorm after Linear\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)      # Normalize\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"\"\"\n",
    "BATCH NORMALIZATION:\n",
    "\n",
    "C√¥ng th·ª©c:\n",
    "  x_norm = (x - mean) / sqrt(var + eps)\n",
    "  y = Œ≥ * x_norm + Œ≤\n",
    "\n",
    "L·ª¢I √çCH:\n",
    "  ‚úÖ TƒÉng t·ªëc training\n",
    "  ‚úÖ Cho ph√©p LR cao h∆°n\n",
    "  ‚úÖ Gi·∫£m sensitivity to initialization\n",
    "  ‚úÖ Regularization effect\n",
    "\n",
    "V·ªä TR√ç:\n",
    "  Linear ‚Üí BatchNorm ‚Üí Activation\n",
    "  HO·∫∂C\n",
    "  Linear ‚Üí Activation ‚Üí BatchNorm\n",
    "\n",
    "TYPES:\n",
    "  - BatchNorm1d: For FC layers\n",
    "  - BatchNorm2d: For Conv layers\n",
    "  - BatchNorm3d: For 3D Conv\n",
    "\n",
    "‚ö†Ô∏è L∆ØU √ù:\n",
    "  - Kh√°c nhau gi·ªØa train/eval mode\n",
    "  - model.train() / model.eval()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ T·ªïng K·∫øt FILE 2-B\n",
    "\n",
    "## H·ªçc ƒê∆∞·ª£c\n",
    "\n",
    "‚úÖ **Activations**: ReLU, LeakyReLU, GELU, Swish\n",
    "\n",
    "‚úÖ **Optimizers**: SGD, Momentum, Adam, AdamW\n",
    "\n",
    "‚úÖ **Learning Rate**: Experiments, Scheduling\n",
    "\n",
    "‚úÖ **Regularization**: Dropout, Weight Decay, BatchNorm\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# Model with regularization\n",
    "class RegularizedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                        lr=0.001, \n",
    "                        weight_decay=0.01)\n",
    "\n",
    "# LR Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, \n",
    "                                      step_size=10, \n",
    "                                      gamma=0.5)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
