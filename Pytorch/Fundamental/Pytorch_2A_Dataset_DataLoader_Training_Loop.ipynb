{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• FILE 2-A: Dataset, DataLoader & Training Loop\n",
    "\n",
    "**PH·∫¶N 2 - INTERMEDIATE (CORE DEEP LEARNING)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi Dung\n",
    "\n",
    "‚úÖ Dataset l√† g√¨ v√† t·∫°i sao c·∫ßn thi·∫øt\n",
    "\n",
    "‚úÖ Custom Dataset - T·∫°o dataset ri√™ng\n",
    "\n",
    "‚úÖ DataLoader - Load data hi·ªáu qu·∫£\n",
    "\n",
    "‚úÖ Train/Validation/Test split\n",
    "\n",
    "‚úÖ Training loop chu·∫©n v·ªõi validation\n",
    "\n",
    "‚úÖ Overfitting vs Underfitting\n",
    "\n",
    "‚úÖ Early Stopping\n",
    "\n",
    "‚úÖ D·ª± √°n ho√†n ch·ªânh: Classification pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ M·ª•c Ti√™u\n",
    "\n",
    "Sau file n√†y, b·∫°n s·∫Ω:\n",
    "- T·∫°o ƒë∆∞·ª£c custom Dataset cho b·∫•t k·ª≥ data n√†o\n",
    "- S·ª≠ d·ª•ng DataLoader hi·ªáu qu·∫£\n",
    "- Vi·∫øt training loop production-ready\n",
    "- Hi·ªÉu v√† detect overfitting/underfitting\n",
    "- Implement early stopping\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Th·ªùi Gian H·ªçc: 3-4 gi·ªù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"‚úÖ Ready to learn Dataset & DataLoader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Dataset L√† G√¨?\n",
    "\n",
    "## V·∫•n ƒê·ªÅ V·ªõi C√°ch C≈©\n",
    "\n",
    "·ªû Foundation level, ch√∫ng ta load to√†n b·ªô data v√†o RAM:\n",
    "\n",
    "```python\n",
    "X = torch.randn(1000, 10)  # Load h·∫øt v√†o RAM\n",
    "y = torch.randn(1000, 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X)  # Process to√†n b·ªô c√πng l√∫c\n",
    "    loss = criterion(y_pred, y)\n",
    "    ...\n",
    "```\n",
    "\n",
    "### V·∫•n ƒê·ªÅ:\n",
    "\n",
    "‚ùå **Kh√¥ng scale**: D·ªØ li·ªáu l·ªõn (GB, TB) kh√¥ng fit v√†o RAM\n",
    "\n",
    "‚ùå **Kh√¥ng shuffle**: Data kh√¥ng ƒë∆∞·ª£c tr·ªôn\n",
    "\n",
    "‚ùå **Kh√¥ng batch**: Ph·∫£i process to√†n b·ªô c√πng l√∫c\n",
    "\n",
    "‚ùå **Kh√¥ng parallel**: Kh√¥ng load data song song\n",
    "\n",
    "## Gi·∫£i Ph√°p: Dataset + DataLoader\n",
    "\n",
    "### Dataset\n",
    "- ƒê·ªãnh nghƒ©a **c√°ch load m·ªôt sample**\n",
    "- Load on-demand (ch·ªâ khi c·∫ßn)\n",
    "- Support nhi·ªÅu ƒë·ªãnh d·∫°ng (images, text, audio)\n",
    "\n",
    "### DataLoader\n",
    "- **Batch**: Chia data th√†nh batches\n",
    "- **Shuffle**: Tr·ªôn data m·ªói epoch\n",
    "- **Parallel**: Load data song song (multi-processing)\n",
    "- **Memory efficient**: Ch·ªâ load batch hi·ªán t·∫°i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Custom Dataset\n",
    "\n",
    "## C·∫•u Tr√∫c Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOM DATASET - TEMPLATE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Template cho custom dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data, targets):\n",
    "        \"\"\"\n",
    "        Constructor - Kh·ªüi t·∫°o dataset\n",
    "        Args:\n",
    "            data: Input data\n",
    "            targets: Labels/targets\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng samples\n",
    "        B·∫ÆT BU·ªòC implement!\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Tr·∫£ v·ªÅ sample t·∫°i index idx\n",
    "        B·∫ÆT BU·ªòC implement!\n",
    "        Args:\n",
    "            idx: Index c·ªßa sample\n",
    "        Returns:\n",
    "            (data, target) tuple\n",
    "        \"\"\"\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "print(\"\"\"\n",
    "3 PH∆Ø∆†NG TH·ª®C B·∫ÆT BU·ªòC:\n",
    "\n",
    "1. __init__(self, ...)\n",
    "   ‚Üí Kh·ªüi t·∫°o dataset, l∆∞u paths, metadata\n",
    "\n",
    "2. __len__(self)\n",
    "   ‚Üí Tr·∫£ v·ªÅ t·ªïng s·ªë samples\n",
    "   ‚Üí DataLoader d√πng ƒë·ªÉ bi·∫øt khi n√†o h·∫øt data\n",
    "\n",
    "3. __getitem__(self, idx)\n",
    "   ‚Üí Load v√† tr·∫£ v·ªÅ sample t·∫°i index idx\n",
    "   ‚Üí DataLoader g·ªçi method n√†y ƒë·ªÉ l·∫•y data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V√≠ D·ª• Th·ª±c T·∫ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"V√ç D·ª§: REGRESSION DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    \"\"\"Dataset cho regression task\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        # Apply transform n·∫øu c√≥\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu m·∫´u\n",
    "X_data = np.random.randn(1000, 10)\n",
    "y_data = np.random.randn(1000, 1)\n",
    "\n",
    "# T·∫°o dataset\n",
    "dataset = RegressionDataset(X_data, y_data)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "x, y = dataset[0]\n",
    "print(f\"  X: shape={x.shape}, dtype={x.dtype}\")\n",
    "print(f\"  y: shape={y.shape}, dtype={y.dtype}\")\n",
    "\n",
    "# Access multiple samples\n",
    "print(f\"\\nSamples 0-2:\")\n",
    "for i in range(3):\n",
    "    x, y = dataset[i]\n",
    "    print(f\"  Sample {i}: X={x[:3]}..., y={y.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"V√ç D·ª§: CLASSIFICATION DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset cho classification task\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)  # LongTensor cho classification\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=15, n_redundant=5,\n",
    "                           n_classes=3, random_state=42)\n",
    "\n",
    "dataset = ClassificationDataset(X, y)\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Feature dim: {dataset.features.shape[1]}\")\n",
    "print(f\"  Number of classes: {len(torch.unique(dataset.labels))}\")\n",
    "\n",
    "# Check class distribution\n",
    "unique, counts = torch.unique(dataset.labels, return_counts=True)\n",
    "print(f\"\\nClass distribution:\")\n",
    "for class_id, count in zip(unique, counts):\n",
    "    print(f\"  Class {class_id}: {count} samples ({count/len(dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ DataLoader\n",
    "\n",
    "## DataLoader L√† G√¨?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATALOADER BASICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o dataset\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "dataset = ClassificationDataset(X, y)\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,      # M·ªói batch 16 samples\n",
    "    shuffle=True,       # Shuffle m·ªói epoch\n",
    "    num_workers=0,      # Number of subprocesses (0 = main process)\n",
    "    drop_last=False     # Drop batch cu·ªëi n·∫øu kh√¥ng ƒë·ªß size\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"  Dataset size: {len(dataset)}\")\n",
    "print(f\"  Batch size: {dataloader.batch_size}\")\n",
    "print(f\"  Number of batches: {len(dataloader)}\")\n",
    "print(f\"  Shuffle: {dataloader.shuffle}\")\n",
    "\n",
    "# Iterate through batches\n",
    "print(f\"\\nFirst 3 batches:\")\n",
    "for i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"  Batch {i}: X={batch_X.shape}, y={batch_y.shape}\")\n",
    "\n",
    "print(\"\\nüí° DataLoader t·ª± ƒë·ªông:\")\n",
    "print(\"  - Chia th√†nh batches\")\n",
    "print(\"  - Shuffle (n·∫øu shuffle=True)\")\n",
    "print(\"  - Convert sang tensor\")\n",
    "print(\"  - Collate (gom samples th√†nh batch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATALOADER PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "QUAN TR·ªåNG:\n",
    "\n",
    "1. batch_size (int)\n",
    "   ‚Üí S·ªë samples m·ªói batch\n",
    "   ‚Üí L·ªõn h∆°n = nhanh h∆°n nh∆∞ng t·ªën RAM\n",
    "   ‚Üí Th∆∞·ªùng: 16, 32, 64, 128, 256\n",
    "\n",
    "2. shuffle (bool)\n",
    "   ‚Üí True: Tr·ªôn data m·ªói epoch\n",
    "   ‚Üí QUAN TR·ªåNG cho training!\n",
    "   ‚Üí False cho validation/test\n",
    "\n",
    "3. num_workers (int)\n",
    "   ‚Üí S·ªë processes load data song song\n",
    "   ‚Üí 0 = main process (ƒë∆°n gi·∫£n, d·ªÖ debug)\n",
    "   ‚Üí >0 = faster nh∆∞ng ph·ª©c t·∫°p h∆°n\n",
    "   ‚Üí Khuy·∫øn ngh·ªã: 0-4\n",
    "\n",
    "4. drop_last (bool)\n",
    "   ‚Üí True: B·ªè batch cu·ªëi n·∫øu kh√¥ng ƒë·ªß size\n",
    "   ‚Üí False: Gi·ªØ l·∫°i (batch cu·ªëi nh·ªè h∆°n)\n",
    "\n",
    "5. pin_memory (bool)\n",
    "   ‚Üí True: TƒÉng t·ªëc CPU ‚Üí GPU transfer\n",
    "   ‚Üí Ch·ªâ d√πng khi train tr√™n GPU\n",
    "\"\"\")\n",
    "\n",
    "# V√≠ d·ª• v·ªõi c√°c parameters kh√°c nhau\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SO S√ÅNH CONFIGURATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "configs = [\n",
    "    {'batch_size': 32, 'shuffle': True, 'drop_last': False},\n",
    "    {'batch_size': 32, 'shuffle': True, 'drop_last': True},\n",
    "    {'batch_size': 16, 'shuffle': False, 'drop_last': False},\n",
    "]\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    loader = DataLoader(dataset, **config)\n",
    "    print(f\"\\nConfig {i+1}: {config}\")\n",
    "    print(f\"  ‚Üí {len(loader)} batches\")\n",
    "    last_batch_size = len(dataset) % config['batch_size']\n",
    "    if last_batch_size > 0:\n",
    "        print(f\"  ‚Üí Last batch: {last_batch_size if not config['drop_last'] else 'dropped'} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAIN / VALIDATION / TEST SPLIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "T·∫†I SAO C·∫¶N CHIA?\n",
    "\n",
    "TRAINING SET (60-80%):\n",
    "  ‚Üí D√πng ƒë·ªÉ train model\n",
    "  ‚Üí Update weights/parameters\n",
    "  ‚Üí Optimize loss\n",
    "\n",
    "VALIDATION SET (10-20%):\n",
    "  ‚Üí D√πng ƒë·ªÉ tune hyperparameters\n",
    "  ‚Üí Select best model\n",
    "  ‚Üí Early stopping\n",
    "  ‚Üí KH√îNG d√πng ƒë·ªÉ train!\n",
    "\n",
    "TEST SET (10-20%):\n",
    "  ‚Üí ƒê√°nh gi√° final performance\n",
    "  ‚Üí CH·ªà d√πng M·ªòT L·∫¶N cu·ªëi c√πng\n",
    "  ‚Üí Simulate real-world performance\n",
    "\"\"\")\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_classes=3, random_state=42)\n",
    "\n",
    "# C√ÅCH 1: Sklearn train_test_split\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"C√ÅCH 1: Sklearn train_test_split\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train/Temp split (80/20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Val/Test split (50/50 of temp = 10/10 of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(X_train)} samples ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Val:   {len(X_val)} samples ({len(X_val)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Test:  {len(X_test)} samples ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "\n",
    "# T·∫°o datasets\n",
    "train_dataset = ClassificationDataset(X_train, y_train)\n",
    "val_dataset = ClassificationDataset(X_val, y_val)\n",
    "test_dataset = ClassificationDataset(X_test, y_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√ÅCH 2: PyTorch random_split\n",
    "print(\"=\" * 70)\n",
    "print(\"C√ÅCH 2: PyTorch random_split\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o full dataset\n",
    "full_dataset = ClassificationDataset(X, y)\n",
    "\n",
    "# Define sizes\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Random split\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val:   {len(val_dataset)}\")\n",
    "print(f\"  Test:  {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nüí° Sklearn vs PyTorch:\")\n",
    "print(\"  Sklearn: C√≥ stratify (gi·ªØ class balance)\")\n",
    "print(\"  PyTorch: ƒê∆°n gi·∫£n, random thu·∫ßn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o DataLoaders\n",
    "print(\"=\" * 70)\n",
    "print(\"T·∫†O DATALOADERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,        # ‚Üê SHUFFLE cho training\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,       # ‚Üê KH√îNG shuffle cho val\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,       # ‚Üê KH√îNG shuffle cho test\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val:   {len(val_loader)} batches\")\n",
    "print(f\"  Test:  {len(test_loader)} batches\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è QUAN TR·ªåNG:\")\n",
    "print(\"  - Training: shuffle=True\")\n",
    "print(\"  - Val/Test: shuffle=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Training Loop Chu·∫©n V·ªõi Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING LOOP TEMPLATE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Classifier(input_dim=20, hidden_dim=64, num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model created\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING WITH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train m·ªôt epoch\"\"\"\n",
    "    model.train()  # Set to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Move to device\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate m·ªôt epoch\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Kh√¥ng t√≠nh gradient\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward only\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "num_epochs = 50\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Epoch':>6} {'Train Loss':>12} {'Train Acc':>12} {'Val Loss':>12} {'Val Acc':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"{epoch+1:6d} {train_loss:12.4f} {train_acc:12.2%} {val_loss:12.4f} {val_acc:12.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Train Acc:  {history['train_acc'][-1]:.2%}\")\n",
    "print(f\"  Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Val Acc:    {history['val_acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Overfitting vs Underfitting\n",
    "\n",
    "## ƒê·ªãnh Nghƒ©a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"OVERFITTING VS UNDERFITTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "UNDERFITTING (H·ªçc k√©m):\n",
    "  üìâ Train loss: CAO\n",
    "  üìâ Val loss: CAO\n",
    "  üìâ Gap: NH·ªé\n",
    "  \n",
    "  ‚Üí Model qu√° ƒë∆°n gi·∫£n\n",
    "  ‚Üí Kh√¥ng h·ªçc ƒë∆∞·ª£c pattern\n",
    "  \n",
    "  Gi·∫£i ph√°p:\n",
    "  ‚úÖ TƒÉng model complexity\n",
    "  ‚úÖ Train l√¢u h∆°n\n",
    "  ‚úÖ Gi·∫£m regularization\n",
    "\n",
    "---\n",
    "\n",
    "GOOD FIT (V·ª´a ƒë·ªß):\n",
    "  üìä Train loss: TH·∫§P\n",
    "  üìä Val loss: TH·∫§P\n",
    "  üìä Gap: NH·ªé\n",
    "  \n",
    "  ‚Üí Model h·ªçc t·ªët\n",
    "  ‚Üí Generalize t·ªët\n",
    "  \n",
    "  ‚úÖ M·ª•c ti√™u c·∫ßn ƒë·∫°t!\n",
    "\n",
    "---\n",
    "\n",
    "OVERFITTING (H·ªçc v·∫πt):\n",
    "  üìà Train loss: R·∫§T TH·∫§P\n",
    "  üìâ Val loss: CAO\n",
    "  üìà Gap: L·ªöN\n",
    "  \n",
    "  ‚Üí Model nh·ªõ data training\n",
    "  ‚Üí Kh√¥ng generalize\n",
    "  \n",
    "  Gi·∫£i ph√°p:\n",
    "  ‚úÖ Th√™m data\n",
    "  ‚úÖ Regularization (Dropout, Weight Decay)\n",
    "  ‚úÖ Gi·∫£m model complexity\n",
    "  ‚úÖ Early stopping\n",
    "  ‚úÖ Data augmentation\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DEMO: T·∫†O OVERFITTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o data NH·ªé ƒë·ªÉ d·ªÖ overfit\n",
    "X_small, y_small = make_classification(\n",
    "    n_samples=200,      # ‚Üê √çT data\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_small, y_small, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = ClassificationDataset(X_train, y_train)\n",
    "val_ds = ClassificationDataset(X_val, y_val)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_ld = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model R·∫§T L·ªöN so v·ªõi data\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(20, 256)   # ‚Üê R·∫§T L·ªöN\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "large_model = LargeModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(large_model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"\\nModel size: {sum(p.numel() for p in large_model.parameters()):,} parameters\")\n",
    "print(f\"Data size: {len(train_ds)} training samples\")\n",
    "print(f\"Ratio: {sum(p.numel() for p in large_model.parameters()) / len(train_ds):.1f} params/sample\")\n",
    "print(\"\\n‚ö†Ô∏è Model qu√° l·ªõn so v·ªõi data ‚Üí S·∫º OVERFIT!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ƒë·ªÉ th·∫•y overfitting\n",
    "num_epochs = 100\n",
    "overfit_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"Training (s·∫Ω overfit)...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(large_model, train_ld, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(large_model, val_ld, criterion, device)\n",
    "    \n",
    "    overfit_history['train_loss'].append(train_loss)\n",
    "    overfit_history['val_loss'].append(val_loss)\n",
    "    overfit_history['train_acc'].append(train_acc)\n",
    "    overfit_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        gap = train_loss - val_loss\n",
    "        print(f\"Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, Gap={gap:+.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(overfit_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(overfit_history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Overfitting: Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate overfitting region\n",
    "overfit_start = 20\n",
    "axes[0].axvline(x=overfit_start, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].text(overfit_start+5, max(overfit_history['val_loss'])*0.8, \n",
    "             'Overfitting\\nstarts here', fontsize=10, color='red')\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(overfit_history['train_acc'], label='Train Acc', linewidth=2)\n",
    "axes[1].plot(overfit_history['val_acc'], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Overfitting: Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä D·∫§U HI·ªÜU OVERFITTING:\")\n",
    "print(f\"  Train loss gi·∫£m ti·∫øp: {overfit_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Val loss TƒÇNG l·∫°i: {overfit_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Gap L·ªöN: {abs(overfit_history['train_loss'][-1] - overfit_history['val_loss'][-1]):.4f}\")\n",
    "print(f\"  Train acc = {overfit_history['train_acc'][-1]:.2%}\")\n",
    "print(f\"  Val acc = {overfit_history['val_acc'][-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EARLY STOPPING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping ƒë·ªÉ tr√°nh overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: S·ªë epochs ch·ªù ƒë·ª£i tr∆∞·ªõc khi stop\n",
    "            min_delta: Minimum thay ƒë·ªïi ƒë·ªÉ coi l√† improvement\n",
    "            verbose: In ra th√¥ng tin\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"Initial best loss: {val_loss:.4f}\")\n",
    "        \n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"New best loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "C√ÅCH HO·∫†T ƒê·ªòNG:\n",
    "\n",
    "1. Track val_loss m·ªói epoch\n",
    "2. N·∫øu val_loss KH√îNG c·∫£i thi·ªán sau `patience` epochs\n",
    "   ‚Üí D·ª´ng training\n",
    "3. Save best model (val_loss th·∫•p nh·∫•t)\n",
    "\n",
    "L·ª¢I √çCH:\n",
    "‚úÖ Tr√°nh overfitting\n",
    "‚úÖ Ti·∫øt ki·ªám th·ªùi gian\n",
    "‚úÖ T·ª± ƒë·ªông t√¨m s·ªë epochs t·ªëi ∆∞u\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training v·ªõi Early Stopping\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING V·ªöI EARLY STOPPING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Reset model\n",
    "model_es = Classifier(20, 64, 3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_es.parameters(), lr=0.001)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "history_es = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "for epoch in range(200):  # Max 200 epochs\n",
    "    train_loss, train_acc = train_epoch(model_es, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(model_es, val_loader, criterion, device)\n",
    "    \n",
    "    history_es['train_loss'].append(train_loss)\n",
    "    history_es['val_loss'].append(val_loss)\n",
    "    history_es['train_acc'].append(train_acc)\n",
    "    history_es['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    early_stopping(val_loss)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print()\n",
    "\n",
    "print(f\"\\n‚úÖ Training stopped at epoch {epoch+1}\")\n",
    "print(f\"Best val loss: {early_stopping.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ T·ªïng K·∫øt FILE 2-A\n",
    "\n",
    "## B·∫°n ƒê√£ H·ªçc ƒê∆∞·ª£c\n",
    "\n",
    "‚úÖ **Dataset** - Custom dataset cho b·∫•t k·ª≥ data n√†o\n",
    "\n",
    "‚úÖ **DataLoader** - Batch, shuffle, parallel loading\n",
    "\n",
    "‚úÖ **Train/Val/Test split** - Chia data ƒë√∫ng c√°ch\n",
    "\n",
    "‚úÖ **Training loop chu·∫©n** - V·ªõi validation\n",
    "\n",
    "‚úÖ **Overfitting vs Underfitting** - Nh·∫≠n bi·∫øt v√† x·ª≠ l√Ω\n",
    "\n",
    "‚úÖ **Early Stopping** - T·ª± ƒë·ªông d·ª´ng khi overfit\n",
    "\n",
    "---\n",
    "\n",
    "## Template Code - S·ª≠ D·ª•ng Ngay\n",
    "\n",
    "```python\n",
    "# 1. Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 2. DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            val_loss = criterion(outputs, batch_y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## B√†i T·∫≠p Th·ª±c H√†nh\n",
    "\n",
    "### B√†i 1: Custom Dataset\n",
    "1. T·∫°o dataset cho regression v·ªõi 1000 samples\n",
    "2. Implement __init__, __len__, __getitem__\n",
    "3. Add normalization transform\n",
    "4. Test v·ªõi DataLoader\n",
    "\n",
    "### B√†i 2: Complete Pipeline\n",
    "1. Load iris dataset\n",
    "2. Split 70/15/15 (train/val/test)\n",
    "3. Create DataLoaders\n",
    "4. Train v·ªõi validation\n",
    "5. Plot curves\n",
    "6. Final test evaluation\n",
    "\n",
    "### B√†i 3: Detect Overfitting\n",
    "1. T·∫°o small dataset (100 samples)\n",
    "2. Build large model\n",
    "3. Train v√† observe overfitting\n",
    "4. Implement early stopping\n",
    "5. Compare v·ªõi/kh√¥ng early stopping\n",
    "\n",
    "---\n",
    "\n",
    "## Ti·∫øp Theo\n",
    "\n",
    "üìó **FILE 2-B: Optimizer, Activation & Regularization**\n",
    "\n",
    "B·∫°n s·∫Ω h·ªçc:\n",
    "- Advanced activation functions\n",
    "- Optimizers chi ti·∫øt (SGD, Adam, AdamW)\n",
    "- Learning rate scheduling\n",
    "- Regularization (Dropout, Weight Decay, BatchNorm)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
