{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• FILE 1-B: Autograd & Training from Scratch\n",
    "\n",
    "**PH·∫¶N 1 - FOUNDATION (BEGINNER)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi Dung\n",
    "\n",
    "‚úÖ Gradient l√† g√¨ v√† t·∫°i sao quan tr·ªçng\n",
    "\n",
    "‚úÖ Autograd - T√≠nh ƒë·∫°o h√†m t·ª± ƒë·ªông\n",
    "\n",
    "‚úÖ Computational Graph\n",
    "\n",
    "‚úÖ backward() v√† zero_grad()\n",
    "\n",
    "‚úÖ Gradient Descent c∆° b·∫£n\n",
    "\n",
    "‚úÖ D·ª± √°n: Linear Regression t·ª´ ƒë·∫ßu (KH√îNG d√πng torch.nn)\n",
    "\n",
    "‚úÖ Training loop ho√†n ch·ªânh\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ M·ª•c Ti√™u\n",
    "\n",
    "Sau file n√†y, b·∫°n s·∫Ω:\n",
    "- Hi·ªÉu r√µ c√°ch PyTorch t√≠nh gradient\n",
    "- Bi·∫øt c√°ch train m·ªôt m√¥ h√¨nh t·ª´ ƒë·∫ßu\n",
    "- N·∫Øm v·ªØng training loop c∆° b·∫£n\n",
    "- S·∫µn s√†ng chuy·ªÉn sang torch.nn (FILE 1-C)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Th·ªùi Gian H·ªçc: 2-2.5 gi·ªù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"‚úÖ Ready to learn Autograd!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Gradient L√† G√¨?\n",
    "\n",
    "## ƒê·ªãnh Nghƒ©a ƒê∆°n Gi·∫£n\n",
    "\n",
    "**Gradient** = ƒê·∫°o h√†m c·ªßa m·ªôt h√†m s·ªë\n",
    "\n",
    "- V·ªõi h√†m 1 bi·∫øn: $f(x) = x^2$ ‚Üí $f'(x) = 2x$ ‚Üê Gradient\n",
    "- V·ªõi h√†m nhi·ªÅu bi·∫øn: $f(x, y) = x^2 + y^2$ ‚Üí $\\nabla f = [2x, 2y]$ ‚Üê Gradient vector\n",
    "\n",
    "## T·∫°i Sao Gradient Quan Tr·ªçng?\n",
    "\n",
    "**Gradient cho bi·∫øt h∆∞·ªõng ƒëi ƒë·ªÉ tƒÉng/gi·∫£m h√†m s·ªë nhanh nh·∫•t!**\n",
    "\n",
    "### V√≠ D·ª• Th·ª±c T·∫ø: Leo N√∫i\n",
    "\n",
    "```\n",
    "B·∫°n ƒëang ·ªü tr√™n n√∫i (ƒë·ªô cao = f(x, y))\n",
    "Mu·ªën xu·ªëng ch√¢n n√∫i nhanh nh·∫•t?\n",
    "‚Üí ƒêi theo h∆∞·ªõng d·ªëc nh·∫•t (h∆∞·ªõng gradient √¢m)\n",
    "```\n",
    "\n",
    "### Trong Deep Learning\n",
    "\n",
    "```\n",
    "Loss function = L(w, b)  ‚Üê ƒê·ªô cao tr√™n \"n√∫i\"\n",
    "Mu·ªën gi·∫£m loss?\n",
    "‚Üí Di chuy·ªÉn w, b theo h∆∞·ªõng -‚àáL (gradient descent)\n",
    "```\n",
    "\n",
    "## C√¥ng Th·ª©c Gradient Descent\n",
    "\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $\\eta$ = learning rate (t·ªëc ƒë·ªô h·ªçc)\n",
    "- $\\frac{\\partial L}{\\partial w}$ = gradient c·ªßa loss theo w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Autograd - T·ª± ƒê·ªông T√≠nh Gradient\n",
    "\n",
    "## Autograd L√† G√¨?\n",
    "\n",
    "**Autograd** = Automatic Differentiation (T√≠nh ƒë·∫°o h√†m t·ª± ƒë·ªông)\n",
    "\n",
    "### V·∫•n ƒê·ªÅ N·∫øu Kh√¥ng C√≥ Autograd\n",
    "\n",
    "M√¥ h√¨nh neural network ph·ª©c t·∫°p:\n",
    "```\n",
    "Input ‚Üí Layer1 ‚Üí Layer2 ‚Üí ... ‚Üí Layer100 ‚Üí Loss\n",
    "```\n",
    "\n",
    "T√≠nh gradient th·ªß c√¥ng?\n",
    "- ‚ùå C·ª±c k·ª≥ ph·ª©c t·∫°p\n",
    "- ‚ùå D·ªÖ sai\n",
    "- ‚ùå M·∫•t nhi·ªÅu th·ªùi gian\n",
    "\n",
    "### Gi·∫£i Ph√°p: Autograd\n",
    "\n",
    "‚úÖ PyTorch t·ª± ƒë·ªông theo d√µi m·ªçi ph√©p to√°n\n",
    "\n",
    "‚úÖ X√¢y d·ª±ng \"computational graph\"\n",
    "\n",
    "‚úÖ T√≠nh gradient ch√≠nh x√°c b·∫±ng c√°ch g·ªçi `.backward()`\n",
    "\n",
    "## Computational Graph\n",
    "\n",
    "V√≠ d·ª•: $z = 2x + 3$\n",
    "\n",
    "```\n",
    "x ‚îÄ‚îÄ‚Üí [√ó2] ‚îÄ‚îÄ‚Üí y ‚îÄ‚îÄ‚Üí [+3] ‚îÄ‚îÄ‚Üí z\n",
    "     w=2              b=3\n",
    "\n",
    "Forward:  z = 2x + 3\n",
    "Backward: dz/dx = 2, dz/dw = x, dz/db = 1\n",
    "```\n",
    "\n",
    "PyTorch t·ª± ƒë·ªông l∆∞u graph n√†y!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Autograd C∆° B·∫£n\n",
    "\n",
    "## 3.1 V√≠ D·ª• ƒê∆°n Gi·∫£n: $y = x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"V√ç D·ª§: y = x¬≤\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# B∆∞·ªõc 1: T·∫°o tensor v·ªõi requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"\\nx = {x}\")\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "print(\"‚Üí B·∫≠t tracking gradient cho x\")\n",
    "\n",
    "# B∆∞·ªõc 2: Forward pass (t√≠nh y)\n",
    "y = x ** 2\n",
    "print(f\"\\ny = x¬≤ = {y}\")\n",
    "print(f\"y.requires_grad = {y.requires_grad}  ‚Üê T·ª± ƒë·ªông True\")\n",
    "\n",
    "# B∆∞·ªõc 3: Backward pass (t√≠nh gradient)\n",
    "y.backward()\n",
    "\n",
    "# B∆∞·ªõc 4: Xem gradient\n",
    "print(f\"\\nGradient dy/dx = {x.grad}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GI·∫¢I TH√çCH TO√ÅN H·ªåC\")\n",
    "print(\"=\" * 70)\n",
    "print(\"y = x¬≤\")\n",
    "print(\"dy/dx = 2x\")\n",
    "print(f\"T·∫°i x = {x.item()}: dy/dx = 2 √ó {x.item()} = {2 * x.item()}\")\n",
    "print(f\"\\n‚úÖ PyTorch t√≠nh ƒë∆∞·ª£c: {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 V√≠ D·ª• Ph·ª©c T·∫°p H∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"V√ç D·ª§: z = 3x¬≤ + 2x + 1\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o input\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"\\nx = {x.item()}\")\n",
    "\n",
    "# Forward pass\n",
    "z = 3 * x**2 + 2 * x + 1\n",
    "print(f\"z = 3x¬≤ + 2x + 1 = {z.item()}\")\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\nGradient dz/dx = {x.grad.item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KI·ªÇM TRA\")\n",
    "print(\"=\" * 70)\n",
    "print(\"z = 3x¬≤ + 2x + 1\")\n",
    "print(\"dz/dx = 6x + 2\")\n",
    "print(f\"T·∫°i x = {x.item()}: dz/dx = 6√ó{x.item()} + 2 = {6*x.item() + 2}\")\n",
    "print(f\"PyTorch: {x.grad.item()}\")\n",
    "print(\"\\n‚úÖ Ch√≠nh x√°c!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Nhi·ªÅu Bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"NHI·ªÄU BI·∫æN: y = sum(x * w) + b\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o variables\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "w = torch.tensor([0.5, -0.3, 0.8], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "print(\"\\nInput:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"w = {w}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Forward: y = sum(x * w) + b\n",
    "y = (x * w).sum() + b\n",
    "print(f\"\\ny = sum(x * w) + b = {y.item()}\")\n",
    "\n",
    "# Chi ti·∫øt\n",
    "xw = x * w\n",
    "print(f\"\\nChi ti·∫øt:\")\n",
    "print(f\"x * w = {xw}\")\n",
    "print(f\"sum(x * w) = {xw.sum().item()}\")\n",
    "print(f\"+ b = {y.item()}\")\n",
    "\n",
    "# Backward\n",
    "y.backward()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GRADIENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"dy/dx = {x.grad}  ‚Üê M·ªói x[i] c√≥ grad = w[i]\")\n",
    "print(f\"dy/dw = {w.grad}  ‚Üê M·ªói w[i] c√≥ grad = x[i]\")\n",
    "print(f\"dy/db = {b.grad}  ‚Üê Gradient = 1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GI·∫¢I TH√çCH\")\n",
    "print(\"=\" * 70)\n",
    "print(\"y = x[0]*w[0] + x[1]*w[1] + x[2]*w[2] + b\")\n",
    "print(\"\")\n",
    "print(\"ƒê·∫°o h√†m ri√™ng:\")\n",
    "print(\"  dy/dx[i] = w[i]\")\n",
    "print(\"  dy/dw[i] = x[i]\")\n",
    "print(\"  dy/db = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ L∆∞u √ù Quan Tr·ªçng\n",
    "\n",
    "## 4.1 Gradient T√≠ch L≈©y (Accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"‚ö†Ô∏è GRADIENT T√çCH L≈®Y\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "print(\"\\nKh√¥ng zero gradient:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# L·∫ßn 1\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"L·∫ßn 1 - Gradient: {x.grad.item()}\")\n",
    "\n",
    "# L·∫ßn 2 (KH√îNG zero)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"L·∫ßn 2 - Gradient: {x.grad.item()}  ‚Üê C·ªông d·ªìn!\")\n",
    "\n",
    "# L·∫ßn 3\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(f\"L·∫ßn 3 - Gradient: {x.grad.item()}  ‚Üê Ti·∫øp t·ª•c c·ªông!\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è V·∫§N ƒê·ªÄ: Gradient c·ªông d·ªìn sau m·ªói backward()\")\n",
    "print(\"‚Üí K·∫øt qu·∫£ sai!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ GI·∫¢I PH√ÅP: Zero Gradient\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    print(f\"L·∫ßn {i+1} - Gradient: {x.grad.item()}\")\n",
    "    x.grad.zero_()  # ‚Üê Zero gradient\n",
    "\n",
    "print(\"\\n‚úÖ Gi·ªù gradient ƒë√∫ng m·ªói l·∫ßn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Backward Ch·ªâ M·ªôt L·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"‚ö†Ô∏è CH·ªà BACKWARD M·ªòT L·∫¶N\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# Backward l·∫ßn 1: OK\n",
    "y.backward()\n",
    "print(f\"L·∫ßn 1 - Gradient: {x.grad.item()}\")\n",
    "\n",
    "# Backward l·∫ßn 2: L·ªñI!\n",
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n‚ùå L·ªñI khi backward l·∫ßn 2:\")\n",
    "    print(f\"   {str(e)[:60]}...\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Computational graph b·ªã x√≥a sau backward()!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ GI·∫¢I PH√ÅP: retain_graph=True\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "y.backward(retain_graph=True)  # ‚Üê Gi·ªØ graph\n",
    "print(f\"L·∫ßn 1: {x.grad.item()}\")\n",
    "\n",
    "x.grad.zero_()  # Zero gradient\n",
    "y.backward()  # OK\n",
    "print(f\"L·∫ßn 2: {x.grad.item()}\")\n",
    "\n",
    "print(\"\\nüí° Nh∆∞ng th∆∞·ªùng th√¨ kh√¥ng c·∫ßn backward 2 l·∫ßn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 T·∫Øt Autograd Khi Kh√¥ng C·∫ßn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"T·∫ÆT AUTOGRAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKhi n√†o n√™n t·∫Øt?\")\n",
    "print(\"  - Inference (d·ª± ƒëo√°n)\")\n",
    "print(\"  - Validation/Testing\")\n",
    "print(\"  - Kh√¥ng c·∫ßn backward()\")\n",
    "print(\"\\n‚Üí Ti·∫øt ki·ªám memory + TƒÉng t·ªëc\\n\")\n",
    "\n",
    "x = torch.randn(100, 100)\n",
    "w = torch.randn(100, 100, requires_grad=True)\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"C√ÅCH 1: torch.no_grad()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x @ w\n",
    "    print(f\"y.requires_grad = {y.requires_grad}  ‚Üê False\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"C√ÅCH 2: .detach()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "y = x @ w\n",
    "y_detached = y.detach()\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")\n",
    "print(f\"y_detached.requires_grad = {y_detached.requires_grad}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"C√ÅCH 3: @torch.no_grad() decorator\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(x, w):\n",
    "    return x @ w\n",
    "\n",
    "y = inference(x, w)\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Gradient Descent C∆° B·∫£n\n",
    "\n",
    "## T√¨m Minimum c·ªßa H√†m S·ªë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"B√ÄI TO√ÅN: T√¨m min c·ªßa f(x) = x¬≤ - 4x + 5\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gi·∫£i ph√°p to√°n h·ªçc\n",
    "print(\"\\nGi·∫£i ph√°p to√°n h·ªçc:\")\n",
    "print(\"f(x) = x¬≤ - 4x + 5\")\n",
    "print(\"f'(x) = 2x - 4 = 0\")\n",
    "print(\"‚Üí x = 2\")\n",
    "print(\"f(2) = 4 - 8 + 5 = 1\")\n",
    "\n",
    "# Gi·∫£i b·∫±ng Gradient Descent\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"GI·∫¢I B·∫∞NG GRADIENT DESCENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Kh·ªüi t·∫°o\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_steps = 50\n",
    "\n",
    "print(f\"\\nB·∫Øt ƒë·∫ßu t·ª´ x = {x.item()}\")\n",
    "print(f\"Learning rate = {learning_rate}\")\n",
    "print(f\"S·ªë b∆∞·ªõc = {num_steps}\\n\")\n",
    "\n",
    "x_history = []\n",
    "f_history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward: T√≠nh f(x)\n",
    "    f = x**2 - 4*x + 5\n",
    "    \n",
    "    # Backward: T√≠nh gradient\n",
    "    f.backward()\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠\n",
    "    x_history.append(x.item())\n",
    "    f_history.append(f.item())\n",
    "    \n",
    "    # In m·ªói 10 steps\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:2d}: x={x.item():6.3f}, f(x)={f.item():6.3f}, grad={x.grad.item():6.3f}\")\n",
    "    \n",
    "    # Update x: x_new = x_old - lr * gradient\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Zero gradient\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(f\"\\n‚úÖ K·∫øt qu·∫£: x = {x.item():.4f}\")\n",
    "print(f\"ƒê√°p √°n ƒë√∫ng: x = 2.0000\")\n",
    "print(f\"f(x) = {f.item():.4f}\")\n",
    "print(f\"f_min = 1.0000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize qu√° tr√¨nh\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: f(x) v√† trajectory\n",
    "plt.subplot(1, 2, 1)\n",
    "x_plot = np.linspace(-1, 5, 100)\n",
    "f_plot = x_plot**2 - 4*x_plot + 5\n",
    "plt.plot(x_plot, f_plot, 'b-', linewidth=2, label='f(x) = x¬≤ - 4x + 5')\n",
    "plt.scatter(x_history, f_history, c=range(len(x_history)), cmap='viridis', s=30, alpha=0.6)\n",
    "plt.plot(2, 1, 'r*', markersize=20, label='Minimum (2, 1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: f(x) theo steps\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(f_history, linewidth=2)\n",
    "plt.axhline(y=1, color='r', linestyle='--', label='f_min = 1')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('f(x) Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Quan s√°t:\")\n",
    "print(\"  - x d·∫ßn ti·∫øn v·ªÅ 2\")\n",
    "print(\"  - f(x) d·∫ßn gi·∫£m v·ªÅ 1\")\n",
    "print(\"  - Gradient descent ho·∫°t ƒë·ªông!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ D·ª∞ √ÅN: Linear Regression T·ª´ ƒê·∫ßu\n",
    "\n",
    "## M·ª•c Ti√™u\n",
    "\n",
    "X√¢y d·ª±ng Linear Regression ho√†n ch·ªânh **KH√îNG d√πng torch.nn**\n",
    "\n",
    "### B√†i To√°n\n",
    "\n",
    "Cho d·ªØ li·ªáu: $y = 3x + 2 + \\text{noise}$\n",
    "\n",
    "M·ª•c ti√™u: H·ªçc ƒë∆∞·ª£c $w \\approx 3$ v√† $b \\approx 2$\n",
    "\n",
    "### C√°c B∆∞·ªõc\n",
    "\n",
    "1. T·∫°o d·ªØ li·ªáu\n",
    "2. Kh·ªüi t·∫°o parameters (w, b)\n",
    "3. ƒê·ªãnh nghƒ©a model v√† loss\n",
    "4. Training loop\n",
    "5. Visualize k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 1: T·∫°o D·ªØ Li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"B∆Ø·ªöC 1: T·∫†O D·ªÆ LI·ªÜU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set seed ƒë·ªÉ reproducible\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Tham s·ªë th·ª±c\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "\n",
    "print(f\"\\nTham s·ªë th·ª±c:\")\n",
    "print(f\"  w = {true_w}\")\n",
    "print(f\"  b = {true_b}\")\n",
    "print(f\"  ‚Üí y = {true_w}x + {true_b}\")\n",
    "\n",
    "# T·∫°o 100 ƒëi·ªÉm d·ªØ li·ªáu\n",
    "n_samples = 100\n",
    "X = torch.randn(n_samples, 1)  # Input (100, 1)\n",
    "noise = torch.randn(n_samples, 1) * 0.3  # Nhi·ªÖu\n",
    "y = true_w * X + true_b + noise  # Output (100, 1)\n",
    "\n",
    "print(f\"\\nD·ªØ li·ªáu:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  S·ªë samples: {n_samples}\")\n",
    "\n",
    "print(f\"\\n5 samples ƒë·∫ßu:\")\n",
    "for i in range(5):\n",
    "    print(f\"  X={X[i].item():6.2f}, y={y[i].item():6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize d·ªØ li·ªáu\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5, s=30, label='Data points')\n",
    "plt.plot(X.numpy(), (true_w * X + true_b).numpy(), 'r-', linewidth=2, \n",
    "         label=f'True line: y = {true_w}x + {true_b}')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Linear Regression Data', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 2: Kh·ªüi T·∫°o Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"B∆Ø·ªöC 2: KH·ªûI T·∫†O PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Kh·ªüi t·∫°o ng·∫´u nhi√™n, requires_grad=True\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "print(f\"\\nKh·ªüi t·∫°o:\")\n",
    "print(f\"  w = {w.item():.4f}  (requires_grad={w.requires_grad})\")\n",
    "print(f\"  b = {b.item():.4f}  (requires_grad={b.requires_grad})\")\n",
    "\n",
    "print(f\"\\nM·ª•c ti√™u:\")\n",
    "print(f\"  w ‚Üí {true_w}\")\n",
    "print(f\"  b ‚Üí {true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 3: Model v√† Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"B∆Ø·ªöC 3: MODEL V√Ä LOSS FUNCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def linear_model(X, w, b):\n",
    "    \"\"\"Forward pass: y_pred = X * w + b\"\"\"\n",
    "    return X @ w + b  # @ = matrix multiplication\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"Mean Squared Error: 1/n * sum((y_pred - y_true)¬≤)\"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "print(\"\\nƒê·ªãnh nghƒ©a:\")\n",
    "print(\"  Model: y_pred = X @ w + b\")\n",
    "print(\"  Loss:  MSE = mean((y_pred - y_true)¬≤)\")\n",
    "\n",
    "# Test\n",
    "y_pred = linear_model(X, w, b)\n",
    "loss = mse_loss(y_pred, y)\n",
    "\n",
    "print(f\"\\nInitial loss: {loss.item():.4f}\")\n",
    "print(\"(Loss cao v√¨ w, b ch∆∞a ƒë∆∞·ª£c train)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"B∆Ø·ªöC 4: TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {n_epochs}\")\n",
    "\n",
    "# L∆∞u l·ªãch s·ª≠\n",
    "loss_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "print(f\"\\n{'Epoch':>6} {'Loss':>10} {'w':>10} {'b':>10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Forward pass\n",
    "    y_pred = linear_model(X, w, b)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    \n",
    "    # 3. Backward pass (t√≠nh gradient)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Update parameters (Gradient Descent)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 5. Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # L∆∞u l·ªãch s·ª≠\n",
    "    loss_history.append(loss.item())\n",
    "    w_history.append(w.item())\n",
    "    b_history.append(b.item())\n",
    "    \n",
    "    # In ra m·ªói 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch+1:6d} {loss.item():10.4f} {w.item():10.4f} {b.item():10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING HO√ÄN TH√ÄNH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nK·∫øt qu·∫£ cu·ªëi c√πng:\")\n",
    "print(f\"  w = {w.item():.4f}  (target: {true_w})\")\n",
    "print(f\"  b = {b.item():.4f}  (target: {true_b})\")\n",
    "print(f\"  Final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆∞·ªõc 5: Visualize K·∫øt Qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0, 0].plot(loss_history, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Loss (MSE)', fontsize=11)\n",
    "axes[0, 0].set_title('Training Loss Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameters evolution\n",
    "axes[0, 1].plot(w_history, label=f'w (target: {true_w})', linewidth=2)\n",
    "axes[0, 1].plot(b_history, label=f'b (target: {true_b})', linewidth=2)\n",
    "axes[0, 1].axhline(y=true_w, color='C0', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].axhline(y=true_b, color='C1', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Value', fontsize=11)\n",
    "axes[0, 1].set_title('Parameters Evolution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Data vs Prediction\n",
    "with torch.no_grad():\n",
    "    y_pred_final = linear_model(X, w, b)\n",
    "\n",
    "axes[1, 0].scatter(X.numpy(), y.numpy(), alpha=0.5, s=30, label='Data')\n",
    "axes[1, 0].plot(X.numpy(), (true_w * X + true_b).numpy(), 'r-', \n",
    "                linewidth=2, label=f'True: y={true_w}x+{true_b}')\n",
    "axes[1, 0].plot(X.numpy(), y_pred_final.numpy(), 'g--', \n",
    "                linewidth=2, label=f'Predicted: y={w.item():.2f}x+{b.item():.2f}')\n",
    "axes[1, 0].set_xlabel('X', fontsize=11)\n",
    "axes[1, 0].set_ylabel('y', fontsize=11)\n",
    "axes[1, 0].set_title('Linear Regression Result', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residuals\n",
    "residuals = (y - y_pred_final).numpy()\n",
    "axes[1, 1].scatter(y_pred_final.numpy(), residuals, alpha=0.5, s=30)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Predicted y', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Residuals (y - y_pred)', fontsize=11)\n",
    "axes[1, 1].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Ph√¢n t√≠ch k·∫øt qu·∫£:\")\n",
    "print(f\"  ‚úÖ Loss gi·∫£m d·∫ßn ‚Üí Model ƒëang h·ªçc\")\n",
    "print(f\"  ‚úÖ w, b ti·∫øn g·∫ßn target ‚Üí Tham s·ªë ƒë√∫ng\")\n",
    "print(f\"  ‚úÖ Predicted line g·∫ßn True line ‚Üí Fit t·ªët\")\n",
    "print(f\"  ‚úÖ Residuals quanh 0 ‚Üí Kh√¥ng bias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Gi·∫£i Th√≠ch Chi Ti·∫øt Training Loop\n",
    "\n",
    "## C·∫•u Tr√∫c Training Loop Chu·∫©n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"C·∫§U TR√öC TRAINING LOOP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. FORWARD PASS\n",
    "    y_pred = model(X, w, b)\n",
    "    ‚Üí T√≠nh d·ª± ƒëo√°n t·ª´ input\n",
    "    \n",
    "    # 2. COMPUTE LOSS\n",
    "    loss = loss_function(y_pred, y_true)\n",
    "    ‚Üí ƒêo ƒë·ªô sai kh√°c\n",
    "    \n",
    "    # 3. BACKWARD PASS\n",
    "    loss.backward()\n",
    "    ‚Üí T√≠nh gradient (dL/dw, dL/db)\n",
    "    \n",
    "    # 4. UPDATE PARAMETERS\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    ‚Üí C·∫≠p nh·∫≠t theo gradient descent\n",
    "    \n",
    "    # 5. ZERO GRADIENTS\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    ‚Üí X√≥a gradient ƒë·ªÉ epoch ti·∫øp theo\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"L∆ØU √ù QUAN TR·ªåNG\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ PH·∫¢I zero gradient m·ªói epoch\n",
    "   ‚Üí N·∫øu kh√¥ng, gradient s·∫Ω c·ªông d·ªìn (SAI!)\n",
    "\n",
    "‚úÖ D√πng torch.no_grad() khi update\n",
    "   ‚Üí Kh√¥ng track gradient cho ph√©p update\n",
    "\n",
    "‚úÖ Th·ª© t·ª± quan tr·ªçng:\n",
    "   Forward ‚Üí Loss ‚Üí Backward ‚Üí Update ‚Üí Zero\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Th·ª≠ Nghi·ªám: ·∫¢nh H∆∞·ªüng c·ªßa Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"·∫¢NH H∆Ø·ªûNG C·ª¶A LEARNING RATE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def train_linear_regression(lr, epochs=100):\n",
    "    \"\"\"Train linear regression v·ªõi learning rate c·ª• th·ªÉ\"\"\"\n",
    "    w = torch.randn(1, 1, requires_grad=True)\n",
    "    b = torch.zeros(1, 1, requires_grad=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_pred = linear_model(X, w, b)\n",
    "        loss = mse_loss(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "        \n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses, w.item(), b.item()\n",
    "\n",
    "# Th·ª≠ c√°c learning rates kh√°c nhau\n",
    "lrs = [0.001, 0.01, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    losses, final_w, final_b = train_linear_regression(lr)\n",
    "    results[lr] = {'losses': losses, 'w': final_w, 'b': final_b}\n",
    "    print(f\"\\nLR={lr:5.3f}: w={final_w:6.3f}, b={final_b:6.3f}, final_loss={losses[-1]:8.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for lr, result in results.items():\n",
    "    plt.plot(result['losses'], label=f'LR={lr}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Effect of Learning Rate', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale ƒë·ªÉ d·ªÖ nh√¨n\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"K·∫æT LU·∫¨N\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "LR qu√° nh·ªè (0.001):\n",
    "  ‚Üí H·ªçc ch·∫≠m, c·∫ßn nhi·ªÅu epochs\n",
    "\n",
    "LR v·ª´a ph·∫£i (0.01-0.1):\n",
    "  ‚Üí H·ªçc nhanh, ·ªïn ƒë·ªãnh\n",
    "\n",
    "LR qu√° l·ªõn (0.5):\n",
    "  ‚Üí C√≥ th·ªÉ kh√¥ng h·ªôi t·ª•, oscillate\n",
    "\n",
    "üí° Ch·ªçn LR l√† ngh·ªá thu·∫≠t l·∫´n khoa h·ªçc!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ T·ªïng K·∫øt FILE 1-B\n",
    "\n",
    "## B·∫°n ƒê√£ H·ªçc ƒê∆∞·ª£c\n",
    "\n",
    "‚úÖ **Gradient** l√† g√¨ v√† t·∫°i sao quan tr·ªçng trong Deep Learning\n",
    "\n",
    "‚úÖ **Autograd** - H·ªá th·ªëng t√≠nh ƒë·∫°o h√†m t·ª± ƒë·ªông c·ªßa PyTorch\n",
    "\n",
    "‚úÖ **Computational Graph** - C√°ch PyTorch theo d√µi ph√©p to√°n\n",
    "\n",
    "‚úÖ **backward()** - T√≠nh gradient ng∆∞·ª£c t·ª´ output v·ªÅ input\n",
    "\n",
    "‚úÖ **zero_grad()** - X√≥a gradient ƒë·ªÉ tr√°nh t√≠ch l≈©y\n",
    "\n",
    "‚úÖ **Gradient Descent** - Thu·∫≠t to√°n t·ªëi ∆∞u c∆° b·∫£n\n",
    "\n",
    "‚úÖ **Training Loop** - C·∫•u tr√∫c chu·∫©n ƒë·ªÉ train model\n",
    "\n",
    "‚úÖ **Linear Regression t·ª´ ƒë·∫ßu** - D·ª± √°n ho√†n ch·ªânh kh√¥ng d√πng torch.nn\n",
    "\n",
    "‚úÖ **Learning Rate** - ·∫¢nh h∆∞·ªüng ƒë·∫øn t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng h·ªçc\n",
    "\n",
    "---\n",
    "\n",
    "## L∆∞u √ù Quan Tr·ªçng\n",
    "\n",
    "### ‚ö†Ô∏è Ph·∫£i Nh·ªõ\n",
    "\n",
    "1. **requires_grad=True** ƒë·ªÉ track gradient\n",
    "2. **loss.backward()** ƒë·ªÉ t√≠nh gradient\n",
    "3. **torch.no_grad()** khi update parameters\n",
    "4. **.grad.zero_()** sau m·ªói update\n",
    "5. **Learning rate** ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn k·∫øt qu·∫£\n",
    "\n",
    "### üí° Training Loop Template\n",
    "\n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    with torch.no_grad():\n",
    "        for param in parameters:\n",
    "            param -= lr * param.grad\n",
    "    \n",
    "    # Zero\n",
    "    for param in parameters:\n",
    "        param.grad.zero_()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## B√†i T·∫≠p Th·ª±c H√†nh\n",
    "\n",
    "### B√†i 1: Thay ƒê·ªïi Learning Rate\n",
    "1. Train v·ªõi lr = 0.001, 0.01, 0.1, 1.0\n",
    "2. So s√°nh t·ªëc ƒë·ªô h·ªôi t·ª•\n",
    "3. T√¨m lr t·ªëi ∆∞u\n",
    "\n",
    "### B√†i 2: Th√™m Noise\n",
    "1. TƒÉng noise l√™n std = 1.0\n",
    "2. Model c√≥ h·ªçc ƒë∆∞·ª£c kh√¥ng?\n",
    "3. C·∫ßn bao nhi√™u epochs?\n",
    "\n",
    "### B√†i 3: Polynomial Regression\n",
    "1. T·∫°o data: y = 2x¬≤ + 3x + 1\n",
    "2. Th·ª≠ fit b·∫±ng linear model\n",
    "3. Quan s√°t k·∫øt qu·∫£ (s·∫Ω kh√¥ng t·ªët!)\n",
    "\n",
    "### B√†i 4: Multiple Features\n",
    "1. T·∫°o X c√≥ 3 features\n",
    "2. y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ + b\n",
    "3. Train ƒë·ªÉ t√¨m w‚ÇÅ, w‚ÇÇ, w‚ÇÉ, b\n",
    "\n",
    "### B√†i 5: Implement L1 Loss\n",
    "1. Vi·∫øt h√†m l1_loss()\n",
    "2. Train v·ªõi L1 thay v√¨ MSE\n",
    "3. So s√°nh k·∫øt qu·∫£\n",
    "\n",
    "---\n",
    "\n",
    "## Ti·∫øp Theo\n",
    "\n",
    "üìò **FILE 1-C: torch.nn & First Neural Network**\n",
    "\n",
    "B·∫°n s·∫Ω h·ªçc:\n",
    "- torch.nn.Module - X√¢y d·ª±ng model d·ªÖ d√†ng\n",
    "- nn.Linear - Layer chu·∫©n\n",
    "- Activation functions (ReLU, Sigmoid, Tanh)\n",
    "- Build MLP (Multi-Layer Perceptron)\n",
    "- So s√°nh v·ªõi code t·ª´ ƒë·∫ßu\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Ch√∫c M·ª´ng!\n",
    "\n",
    "B·∫°n ƒë√£ hi·ªÉu r√µ **tr√°i tim c·ªßa Deep Learning** - Autograd!\n",
    "\n",
    "T·ª´ gi·ªù, b·∫°n c√≥ th·ªÉ:\n",
    "- ‚úÖ Hi·ªÉu c√°ch PyTorch train model\n",
    "- ‚úÖ T·ª± implement thu·∫≠t to√°n t·ª´ ƒë·∫ßu\n",
    "- ‚úÖ Debug gradient khi c√≥ v·∫•n ƒë·ªÅ\n",
    "- ‚úÖ S·∫µn s√†ng h·ªçc torch.nn\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
