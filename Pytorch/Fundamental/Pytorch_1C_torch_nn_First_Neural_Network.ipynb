{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• FILE 1-C: torch.nn & First Neural Network\n",
    "\n",
    "**PH·∫¶N 1 - FOUNDATION (BEGINNER)**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi Dung\n",
    "\n",
    "‚úÖ torch.nn l√† g√¨ v√† t·∫°i sao c·∫ßn thi·∫øt\n",
    "\n",
    "‚úÖ nn.Module - Base class cho m·ªçi model\n",
    "\n",
    "‚úÖ nn.Linear - Fully connected layer\n",
    "\n",
    "‚úÖ Activation functions (ReLU, Sigmoid, Tanh, Softmax)\n",
    "\n",
    "‚úÖ nn.Sequential - X√¢y d·ª±ng model nhanh\n",
    "\n",
    "‚úÖ Custom nn.Module - T·∫°o model linh ho·∫°t\n",
    "\n",
    "‚úÖ Loss functions (MSE, CrossEntropy)\n",
    "\n",
    "‚úÖ Optimizers (SGD, Adam)\n",
    "\n",
    "‚úÖ D·ª± √°n: MLP ƒë∆°n gi·∫£n\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ M·ª•c Ti√™u\n",
    "\n",
    "Chuy·ªÉn t·ª´ code \"th·ªß c√¥ng\" sang s·ª≠ d·ª•ng torch.nn ƒë·ªÉ:\n",
    "- Vi·∫øt code ng·∫Øn g·ªçn, d·ªÖ b·∫£o tr√¨\n",
    "- T·ª± ƒë·ªông qu·∫£n l√Ω parameters\n",
    "- X√¢y d·ª±ng m·∫°ng neural ph·ª©c t·∫°p\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Th·ªùi Gian H·ªçc: 2-2.5 gi·ªù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"‚úÖ Ready to learn torch.nn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ T·∫°i Sao C·∫ßn torch.nn?\n",
    "\n",
    "## So S√°nh: Tr∆∞·ªõc v√† Sau torch.nn\n",
    "\n",
    "### C√°ch C≈© (FILE 1-B): L√†m Th·ªß C√¥ng\n",
    "\n",
    "```python\n",
    "# Kh·ªüi t·∫°o parameters\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a model\n",
    "def linear_model(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_pred = linear_model(X, w, b)\n",
    "    loss = mse_loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "```\n",
    "\n",
    "### C√°ch M·ªõi (torch.nn): T·ª± ƒê·ªông\n",
    "\n",
    "```python\n",
    "# T·∫°o model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Loss v√† optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "## ∆Øu ƒêi·ªÉm c·ªßa torch.nn\n",
    "\n",
    "‚úÖ **Ng·∫Øn g·ªçn**: √çt code h∆°n nhi·ªÅu\n",
    "\n",
    "‚úÖ **T·ª± ƒë·ªông**: Qu·∫£n l√Ω parameters t·ª± ƒë·ªông\n",
    "\n",
    "‚úÖ **Linh ho·∫°t**: D·ªÖ x√¢y d·ª±ng m·∫°ng ph·ª©c t·∫°p\n",
    "\n",
    "‚úÖ **Chu·∫©n**: Best practices ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p\n",
    "\n",
    "‚úÖ **D·ªÖ debug**: C·∫•u tr√∫c r√µ r√†ng\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ nn.Module - Base Class\n",
    "\n",
    "## nn.Module L√† G√¨?\n",
    "\n",
    "**nn.Module** l√† base class cho T·∫§T C·∫¢ neural network trong PyTorch.\n",
    "\n",
    "### T√≠nh NƒÉng Ch√≠nh\n",
    "\n",
    "1. ‚úÖ T·ª± ƒë·ªông qu·∫£n l√Ω parameters\n",
    "2. ‚úÖ T·ª± ƒë·ªông theo d√µi gradients\n",
    "3. ‚úÖ Chuy·ªÉn ƒë·ªïi device (CPU/GPU) d·ªÖ d√†ng\n",
    "4. ‚úÖ L∆∞u/load model ƒë∆°n gi·∫£n\n",
    "5. ‚úÖ Training/eval mode\n",
    "\n",
    "## C·∫•u Tr√∫c nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"C·∫§U TR√öC nn.Module C∆† B·∫¢N\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"Model ƒë∆°n gi·∫£n k·∫ø th·ª´a nn.Module\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()  # ‚Üê B·∫Øt bu·ªôc!\n",
    "        \n",
    "        # ƒê·ªãnh nghƒ©a layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass - ƒê·ªãnh nghƒ©a c√°ch t√≠nh to√°n\"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "# T·∫°o model\n",
    "model = SimpleModel(input_dim=3, output_dim=1)\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TH√îNG TIN MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Xem parameters\n",
    "print(\"\\nParameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "# T·ªïng s·ªë parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nT·ªïng parameters: {total_params}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(5, 3)  # Batch=5, Features=3\n",
    "y = model(x)  # T·ª± ƒë·ªông g·ªçi forward()\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ nn.Linear - Fully Connected Layer\n",
    "\n",
    "## nn.Linear L√† G√¨?\n",
    "\n",
    "**nn.Linear(in_features, out_features)** = Fully connected layer (Dense layer)\n",
    "\n",
    "$$y = xW^T + b$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $x$: Input (batch_size, in_features)\n",
    "- $W$: Weight matrix (out_features, in_features)\n",
    "- $b$: Bias vector (out_features)\n",
    "- $y$: Output (batch_size, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"nn.Linear C∆† B·∫¢N\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o Linear layer\n",
    "linear = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "print(f\"\\nLayer: {linear}\")\n",
    "\n",
    "# Xem weight v√† bias\n",
    "print(f\"\\nWeight shape: {linear.weight.shape}  ‚Üê (out_features, in_features)\")\n",
    "print(f\"Bias shape: {linear.bias.shape}     ‚Üê (out_features,)\")\n",
    "\n",
    "print(f\"\\nWeight:\\n{linear.weight.data}\")\n",
    "print(f\"\\nBias:\\n{linear.bias.data}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.tensor([[1.0, 2.0, 3.0]])  # 1 sample, 3 features\n",
    "y = linear(x)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nInput: {x}\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"\\nOutput: {y}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# T√≠nh to√°n th·ªß c√¥ng ƒë·ªÉ verify\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION (Manual Calculation)\")\n",
    "print(\"=\" * 70)\n",
    "manual_output = x @ linear.weight.T + linear.bias\n",
    "print(f\"Manual: {manual_output}\")\n",
    "print(f\"Match: {torch.allclose(y, manual_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LINEAR LAYER V·ªöI BATCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "linear = nn.Linear(4, 2)  # 4 input features ‚Üí 2 output features\n",
    "\n",
    "# Batch c·ªßa 3 samples\n",
    "x = torch.randn(3, 4)  # (batch_size=3, in_features=4)\n",
    "y = linear(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}   ‚Üê (batch_size, in_features)\")\n",
    "print(f\"Output shape: {y.shape}  ‚Üê (batch_size, out_features)\")\n",
    "\n",
    "print(f\"\\nInput:\\n{x}\")\n",
    "print(f\"\\nOutput:\\n{y}\")\n",
    "\n",
    "print(\"\\nüí° Linear layer x·ª≠ l√Ω batch t·ª± ƒë·ªông!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Activation Functions\n",
    "\n",
    "## T·∫°i Sao C·∫ßn Activation?\n",
    "\n",
    "N·∫øu ch·ªâ d√πng Linear layers:\n",
    "```\n",
    "y = W‚ÇÉ(W‚ÇÇ(W‚ÇÅx + b‚ÇÅ) + b‚ÇÇ) + b‚ÇÉ\n",
    "  = W_total √ó x + b_total  ‚Üê V·∫´n l√† linear!\n",
    "```\n",
    "\n",
    "‚Üí Kh√¥ng th·ªÉ h·ªçc ƒë∆∞·ª£c pattern ph·ª©c t·∫°p (phi tuy·∫øn)\n",
    "\n",
    "**Activation functions th√™m t√≠nh phi tuy·∫øn!**\n",
    "\n",
    "## C√°c Activation Ph·ªï Bi·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test v·ªõi c√πng input\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# 1. ReLU (Rectified Linear Unit)\n",
    "relu = nn.ReLU()\n",
    "y_relu = relu(x)\n",
    "\n",
    "# 2. Sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "y_sigmoid = sigmoid(x)\n",
    "\n",
    "# 3. Tanh\n",
    "tanh = nn.Tanh()\n",
    "y_tanh = tanh(x)\n",
    "\n",
    "# 4. LeakyReLU\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "y_leaky = leaky_relu(x)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(x.numpy(), y_relu.numpy(), 'b-', linewidth=2)\n",
    "plt.title('ReLU', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(x.numpy(), y_sigmoid.numpy(), 'r-', linewidth=2)\n",
    "plt.title('Sigmoid', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('œÉ(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(x.numpy(), y_tanh.numpy(), 'g-', linewidth=2)\n",
    "plt.title('Tanh', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(x.numpy(), y_leaky.numpy(), 'm-', linewidth=2)\n",
    "plt.title('LeakyReLU', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('LeakyReLU(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ƒê·∫∂C ƒêI·ªÇM C√ÅC ACTIVATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ReLU (Rectified Linear Unit)\n",
    "   Formula: f(x) = max(0, x)\n",
    "   Range: [0, +‚àû)\n",
    "   ‚úÖ ∆Øu: Nhanh, ƒë∆°n gi·∫£n, tr√°nh vanishing gradient\n",
    "   ‚ùå Nh∆∞·ª£c: Dying ReLU (neurons ch·∫øt khi x < 0)\n",
    "   üí° D√πng: Hidden layers (ph·ªï bi·∫øn nh·∫•t)\n",
    "\n",
    "2. Sigmoid (Logistic)\n",
    "   Formula: œÉ(x) = 1 / (1 + e^(-x))\n",
    "   Range: (0, 1)\n",
    "   ‚úÖ ∆Øu: Smooth, output gi·ªëng probability\n",
    "   ‚ùå Nh∆∞·ª£c: Vanishing gradient, kh√¥ng zero-centered\n",
    "   üí° D√πng: Binary classification (output layer)\n",
    "\n",
    "3. Tanh (Hyperbolic Tangent)\n",
    "   Formula: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "   Range: (-1, 1)\n",
    "   ‚úÖ ∆Øu: Zero-centered, m·∫°nh h∆°n Sigmoid\n",
    "   ‚ùå Nh∆∞·ª£c: V·∫´n c√≥ vanishing gradient\n",
    "   üí° D√πng: Hidden layers (RNN c≈©)\n",
    "\n",
    "4. LeakyReLU\n",
    "   Formula: f(x) = max(Œ±x, x), Œ±=0.01\n",
    "   Range: (-‚àû, +‚àû)\n",
    "   ‚úÖ ∆Øu: Kh·∫Øc ph·ª•c dying ReLU\n",
    "   ‚ùå Nh∆∞·ª£c: Ph·∫£i tune Œ±\n",
    "   üí° D√πng: Khi ReLU g·∫∑p v·∫•n ƒë·ªÅ\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üí° KHUY·∫æN NGH·ªä\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Hidden layers: ReLU (default choice)\n",
    "Binary classification output: Sigmoid\n",
    "Multi-class classification output: Softmax\n",
    "Regression output: Kh√¥ng d√πng activation (linear)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax - Cho Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SOFTMAX ACTIVATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Softmax chuy·ªÉn logits th√†nh probabilities:\n",
    "\n",
    "softmax(x)·µ¢ = e^(x·µ¢) / Œ£‚±º e^(x‚±º)\n",
    "\n",
    "ƒê·∫∑c ƒëi·ªÉm:\n",
    "  - Output: (0, 1) cho m·ªói class\n",
    "  - T·ªïng = 1\n",
    "  - D√πng cho multi-class classification\n",
    "\"\"\")\n",
    "\n",
    "# V√≠ d·ª•\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])  # 3 classes\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(f\"\\nLogits (raw scores): {logits}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Sum of probs: {probs.sum().item():.4f}  ‚Üê Ph·∫£i = 1\")\n",
    "\n",
    "# Predicted class\n",
    "predicted_class = probs.argmax(dim=1)\n",
    "print(f\"\\nPredicted class: {predicted_class.item()}\")\n",
    "\n",
    "# Batch example\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Batch Example (3 samples, 4 classes)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "logits_batch = torch.randn(3, 4)  # 3 samples, 4 classes\n",
    "probs_batch = softmax(logits_batch)\n",
    "\n",
    "print(f\"\\nLogits:\\n{logits_batch}\")\n",
    "print(f\"\\nProbabilities:\\n{probs_batch}\")\n",
    "print(f\"\\nSum per sample: {probs_batch.sum(dim=1)}\")\n",
    "print(f\"Predicted classes: {probs_batch.argmax(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ nn.Sequential - X√¢y D·ª±ng Model Nhanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"nn.Sequential - C√ÅCH ƒê∆†N GI·∫¢N\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o model b·∫±ng Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),   # Input: 10 ‚Üí Hidden: 64\n",
    "    nn.ReLU(),           # Activation\n",
    "    nn.Linear(64, 32),   # Hidden: 64 ‚Üí Hidden: 32\n",
    "    nn.ReLU(),           # Activation\n",
    "    nn.Linear(32, 1)     # Hidden: 32 ‚Üí Output: 1\n",
    ")\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward\n",
    "x = torch.randn(5, 10)  # Batch=5, Features=10\n",
    "y = model(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# ƒê·∫øm parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARAMETERS BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    print(f\"{i}: {name:20s} {str(param.shape):20s} {param.numel():>8,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Custom nn.Module - T·∫°o Model Linh Ho·∫°t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOM nn.Module\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # ƒê·ªãnh nghƒ©a layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# T·∫°o model\n",
    "model = MLP(input_dim=10, hidden_dim=64, output_dim=3)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(8, 10)  # Batch=8\n",
    "y = model(x)\n",
    "\n",
    "print(f\"\\nInput: {x.shape}\")\n",
    "print(f\"Output: {y.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ∆ØU ƒêI·ªÇM CUSTOM MODULE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. Linh ho·∫°t h∆°n nn.Sequential\n",
    "2. C√≥ th·ªÉ th√™m logic ph·ª©c t·∫°p trong forward()\n",
    "3. D·ªÖ debug v√† customize\n",
    "4. C√≥ th·ªÉ th√™m helper methods\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOSS FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. nn.MSELoss() - Mean Squared Error\n",
    "   D√πng cho: Regression\n",
    "   Formula: MSE = mean((y_pred - y_true)¬≤)\n",
    "\"\"\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "y_pred = torch.tensor([1.0, 2.0, 3.0])\n",
    "y_true = torch.tensor([1.5, 2.5, 2.8])\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "2. nn.L1Loss() - Mean Absolute Error\n",
    "   D√πng cho: Regression (robust to outliers)\n",
    "   Formula: MAE = mean(|y_pred - y_true|)\n",
    "\"\"\")\n",
    "\n",
    "l1_loss = nn.L1Loss()\n",
    "loss = l1_loss(y_pred, y_true)\n",
    "print(f\"L1 Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "3. nn.CrossEntropyLoss() - Cross Entropy\n",
    "   D√πng cho: Multi-class classification\n",
    "   L∆∞u √Ω: ƒê√É BAO G·ªíM Softmax!\n",
    "\"\"\")\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# Logits (raw scores) - KH√îNG c·∫ßn softmax tr∆∞·ªõc!\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1],   # Sample 1\n",
    "                       [0.5, 2.5, 0.3]])   # Sample 2\n",
    "# Targets: class indices\n",
    "targets = torch.tensor([0, 1])  # Class 0 v√† class 1\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"CrossEntropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "4. nn.BCEWithLogitsLoss() - Binary Cross Entropy\n",
    "   D√πng cho: Binary classification\n",
    "   L∆∞u √Ω: ƒê√É BAO G·ªíM Sigmoid!\n",
    "\"\"\")\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "logits = torch.tensor([0.5, -0.3, 2.1])\n",
    "targets = torch.tensor([1.0, 0.0, 1.0])\n",
    "loss = bce_loss(logits, targets)\n",
    "print(f\"BCE Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o model m·∫´u\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "print(\"\"\"\n",
    "1. SGD (Stochastic Gradient Descent)\n",
    "   w = w - lr * gradient\n",
    "   ƒê∆°n gi·∫£n, ·ªïn ƒë·ªãnh\n",
    "\"\"\")\n",
    "sgd = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "print(f\"   {sgd}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "2. SGD with Momentum\n",
    "   Th√™m momentum ƒë·ªÉ tƒÉng t·ªëc\n",
    "\"\"\")\n",
    "sgd_momentum = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(f\"   {sgd_momentum}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "3. Adam (Adaptive Moment Estimation)\n",
    "   T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh learning rate\n",
    "   PH·ªî BI·∫æN NH·∫§T!\n",
    "\"\"\")\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(f\"   {adam}\\n\")\n",
    "\n",
    "print(\"\"\"\n",
    "4. AdamW (Adam with Weight Decay)\n",
    "   Adam + L2 regularization ƒë√∫ng c√°ch\n",
    "\"\"\")\n",
    "adamw = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "print(f\"   {adamw}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üí° KHUY·∫æN NGH·ªä\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "B·∫Øt ƒë·∫ßu: Adam (lr=0.001)\n",
    "Fine-tuning: SGD with momentum\n",
    "Modern: AdamW v·ªõi weight decay\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ D·ª∞ √ÅN: Linear Regression V·ªõi torch.nn\n",
    "\n",
    "## So S√°nh V·ªõi FILE 1-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CHU·∫®N B·ªä D·ªÆ LI·ªÜU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu (gi·ªëng FILE 1-B)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "\n",
    "n_samples = 100\n",
    "X = torch.randn(n_samples, 1)\n",
    "y = true_w * X + true_b + torch.randn(n_samples, 1) * 0.3\n",
    "\n",
    "print(f\"Data: {n_samples} samples\")\n",
    "print(f\"True parameters: w={true_w}, b={true_b}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5, s=30)\n",
    "plt.plot(X.numpy(), (true_w * X + true_b).numpy(), 'r-', linewidth=2, \n",
    "         label=f'y = {true_w}x + {true_b}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING V·ªöI torch.nn\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. T·∫°o model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 2. Loss v√† optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(model)\n",
    "print(f\"\\nInitial parameters:\")\n",
    "print(f\"  w = {model.weight.item():.4f}\")\n",
    "print(f\"  b = {model.bias.item():.4f}\")\n",
    "\n",
    "# 3. Training loop\n",
    "n_epochs = 100\n",
    "loss_history = []\n",
    "\n",
    "print(f\"\\n{'Epoch':>6} {'Loss':>10} {'w':>10} {'b':>10}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()  # ‚Üê Zero gradients\n",
    "    loss.backward()        # ‚Üê Compute gradients\n",
    "    optimizer.step()       # ‚Üê Update parameters\n",
    "    \n",
    "    # Log\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"{epoch+1:6d} {loss.item():10.4f} {model.weight.item():10.4f} {model.bias.item():10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING HO√ÄN TH√ÄNH\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFinal parameters:\")\n",
    "print(f\"  w = {model.weight.item():.4f}  (target: {true_w})\")\n",
    "print(f\"  b = {model.bias.item():.4f}  (target: {true_b})\")\n",
    "print(f\"  Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k·∫øt qu·∫£\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Fit\n",
    "plt.subplot(1, 2, 2)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "plt.scatter(X.numpy(), y.numpy(), alpha=0.5, s=30, label='Data')\n",
    "plt.plot(X.numpy(), (true_w * X + true_b).numpy(), 'r-', linewidth=2, \n",
    "         label=f'True: y={true_w}x+{true_b}')\n",
    "plt.plot(X.numpy(), y_pred.numpy(), 'g--', linewidth=2,\n",
    "         label=f'Pred: y={model.weight.item():.2f}x+{model.bias.item():.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Result')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SO S√ÅNH: FILE 1-B vs FILE 1-C\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "FILE 1-B (Th·ªß c√¥ng):\n",
    "  ‚ùå Nhi·ªÅu code\n",
    "  ‚ùå Ph·∫£i qu·∫£n l√Ω w, b th·ªß c√¥ng\n",
    "  ‚ùå Update parameters ph·ª©c t·∫°p\n",
    "  ‚úÖ Hi·ªÉu r√µ b√™n trong\n",
    "\n",
    "FILE 1-C (torch.nn):\n",
    "  ‚úÖ Code ng·∫Øn g·ªçn\n",
    "  ‚úÖ T·ª± ƒë·ªông qu·∫£n l√Ω parameters\n",
    "  ‚úÖ D·ªÖ m·ªü r·ªông\n",
    "  ‚úÖ Best practices c√≥ s·∫µn\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîü D·ª∞ √ÅN: MLP ƒê∆°n Gi·∫£n\n",
    "\n",
    "## B√†i To√°n: Ph√¢n Lo·∫°i D·ªØ Li·ªáu Phi Tuy·∫øn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"T·∫†O D·ªÆ LI·ªÜU PHI TUY·∫æN (Non-linear)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu h√¨nh b√°n nguy·ªát\n",
    "X_np, y_np = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "\n",
    "# Chuy·ªÉn sang tensor\n",
    "X = torch.FloatTensor(X_np)\n",
    "y = torch.LongTensor(y_np)\n",
    "\n",
    "print(f\"\\nData shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Classes: {y.unique()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==0, 0].numpy(), X[y==0, 1].numpy(), c='blue', label='Class 0', alpha=0.6, s=30)\n",
    "plt.scatter(X[y==1, 0].numpy(), X[y==1, 1].numpy(), c='red', label='Class 1', alpha=0.6, s=30)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Non-linear Binary Classification Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° D·ªØ li·ªáu n√†y KH√îNG th·ªÉ ph√¢n lo·∫°i b·∫±ng linear model!\")\n",
    "print(\"   ‚Üí C·∫ßn neural network v·ªõi activation functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"X√ÇY D·ª∞NG MLP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, 2)  # 2 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)  # Logits (kh√¥ng c·∫ßn Softmax)\n",
    "        return x\n",
    "\n",
    "# T·∫°o model\n",
    "model = BinaryClassifier(input_dim=2, hidden_dim=16)\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING MLP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Loss v√† optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cho classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "n_epochs = 200\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "print(f\"\\n{'Epoch':>6} {'Loss':>10} {'Accuracy':>10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    with torch.no_grad():\n",
    "        pred = logits.argmax(dim=1)\n",
    "        acc = (pred == y).float().mean()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    acc_history.append(acc.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"{epoch+1:6d} {loss.item():10.4f} {acc.item():10.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Training ho√†n th√†nh!\")\n",
    "print(f\"Final accuracy: {acc_history[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k·∫øt qu·∫£\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Loss\n",
    "axes[0].plot(loss_history, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy\n",
    "axes[1].plot(acc_history, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Decision boundary\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.argmax(dim=1).numpy().reshape(xx.shape)\n",
    "\n",
    "axes[2].contourf(xx, yy, Z, alpha=0.3, levels=[0, 0.5, 1], colors=['blue', 'red'])\n",
    "axes[2].scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6, s=20)\n",
    "axes[2].scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6, s=20)\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].set_title('Decision Boundary')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ MLP ƒë√£ h·ªçc ƒë∆∞·ª£c decision boundary phi tuy·∫øn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ T·ªïng K·∫øt FILE 1-C\n",
    "\n",
    "## B·∫°n ƒê√£ H·ªçc ƒê∆∞·ª£c\n",
    "\n",
    "‚úÖ **torch.nn** - Framework x√¢y d·ª±ng neural networks\n",
    "\n",
    "‚úÖ **nn.Module** - Base class cho m·ªçi model\n",
    "\n",
    "‚úÖ **nn.Linear** - Fully connected layer\n",
    "\n",
    "‚úÖ **Activation functions** - ReLU, Sigmoid, Tanh, Softmax\n",
    "\n",
    "‚úÖ **nn.Sequential** - X√¢y d·ª±ng model nhanh\n",
    "\n",
    "‚úÖ **Custom nn.Module** - T·∫°o model linh ho·∫°t\n",
    "\n",
    "‚úÖ **Loss functions** - MSE, L1, CrossEntropy, BCE\n",
    "\n",
    "‚úÖ **Optimizers** - SGD, Adam, AdamW\n",
    "\n",
    "‚úÖ **D·ª± √°n ho√†n ch·ªânh** - Linear Regression v√† MLP\n",
    "\n",
    "---\n",
    "\n",
    "## Training Loop V·ªõi torch.nn\n",
    "\n",
    "```python\n",
    "# Setup\n",
    "model = YourModel()\n",
    "criterion = nn.MSELoss()  # ho·∫∑c loss kh√°c\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## B√†i T·∫≠p Th·ª±c H√†nh\n",
    "\n",
    "### B√†i 1: Linear Regression 3D\n",
    "1. T·∫°o data: y = 2x‚ÇÅ + 3x‚ÇÇ + 4x‚ÇÉ + 1\n",
    "2. D√πng nn.Linear(3, 1) ƒë·ªÉ fit\n",
    "3. So s√°nh parameters h·ªçc ƒë∆∞·ª£c vs true\n",
    "\n",
    "### B√†i 2: Th·ª≠ C√°c Activations\n",
    "1. T·∫°o MLP v·ªõi ReLU\n",
    "2. Thay b·∫±ng Tanh\n",
    "3. Thay b·∫±ng Sigmoid\n",
    "4. So s√°nh performance\n",
    "\n",
    "### B√†i 3: Multi-class Classification\n",
    "1. T·∫°o data 3 classes\n",
    "2. Build MLP v·ªõi output=3\n",
    "3. D√πng CrossEntropyLoss\n",
    "4. Visualize decision boundaries\n",
    "\n",
    "### B√†i 4: Deeper Network\n",
    "1. T·∫°o MLP 5 layers\n",
    "2. So s√°nh v·ªõi 2 layers\n",
    "3. Quan s√°t overfitting\n",
    "\n",
    "### B√†i 5: Custom Activation\n",
    "1. Implement Swish: f(x) = x * sigmoid(x)\n",
    "2. D√πng trong MLP\n",
    "3. So s√°nh v·ªõi ReLU\n",
    "\n",
    "---\n",
    "\n",
    "## Ti·∫øp Theo\n",
    "\n",
    "üìò **FILE 1-D: GPU, Debugging & Exercises**\n",
    "\n",
    "B·∫°n s·∫Ω h·ªçc:\n",
    "- CPU vs GPU\n",
    "- Chuy·ªÉn model l√™n GPU\n",
    "- Common errors v√† c√°ch fix\n",
    "- Debugging techniques\n",
    "- T·ªïng h·ª£p b√†i t·∫≠p Foundation\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
