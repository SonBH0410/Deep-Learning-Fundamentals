{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• FILE 3-A: Advanced Training Techniques\n",
    "\n",
    "**PH·∫¶N 3 - ADVANCED & PROFESSIONAL**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã N·ªôi Dung\n",
    "\n",
    "‚úÖ Custom Loss Functions\n",
    "\n",
    "‚úÖ Custom Layers/Modules\n",
    "\n",
    "‚úÖ Advanced Gradient Operations\n",
    "\n",
    "‚úÖ Learning Rate Scheduling n√¢ng cao\n",
    "\n",
    "‚úÖ Gradient Accumulation\n",
    "\n",
    "‚úÖ Debugging Training Issues\n",
    "\n",
    "‚úÖ Advanced Optimization Tricks\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Th·ªùi Gian H·ªçc: 3-4 gi·ªù\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Custom Loss Functions\n",
    "\n",
    "## T·∫°i Sao C·∫ßn Custom Loss?\n",
    "\n",
    "Built-in losses (MSE, CrossEntropy) kh√¥ng ph·∫£i l√∫c n√†o c≈©ng ph√π h·ª£p:\n",
    "- Domain-specific requirements\n",
    "- Multi-task learning\n",
    "- Weighted losses\n",
    "- Custom metrics optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOM LOSS - BASIC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for imbalanced classification\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: (N, C) logits\n",
    "        # targets: (N,) class indices\n",
    "        \n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        p_t = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Test\n",
    "logits = torch.randn(10, 5)  # 10 samples, 5 classes\n",
    "targets = torch.randint(0, 5, (10,))\n",
    "\n",
    "focal = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "loss = focal(logits, targets)\n",
    "\n",
    "print(f\"\\nFocal Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compare v·ªõi CrossEntropy\n",
    "ce = nn.CrossEntropyLoss()\n",
    "ce_loss = ce(logits, targets)\n",
    "print(f\"CrossEntropy: {ce_loss.item():.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "FOCAL LOSS:\n",
    "  - Down-weight easy examples\n",
    "  - Focus on hard examples\n",
    "  - T·ªët cho imbalanced data\n",
    "  \n",
    "  FL = -Œ±(1-p_t)^Œ≥ * log(p_t)\n",
    "  \n",
    "  Œ≥=0 ‚Üí CrossEntropy\n",
    "  Œ≥‚Üë ‚Üí Focus more on hard examples\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOM LOSS - MULTI-TASK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"Combine multiple losses for multi-task learning\"\"\"\n",
    "    \n",
    "    def __init__(self, task_weights=None):\n",
    "        super().__init__()\n",
    "        self.task_weights = task_weights or [1.0, 1.0]\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        # outputs: dict {'task1': pred1, 'task2': pred2}\n",
    "        # targets: dict {'task1': y1, 'task2': y2}\n",
    "        \n",
    "        # Task 1: Classification\n",
    "        loss1 = F.cross_entropy(outputs['classification'], targets['classification'])\n",
    "        \n",
    "        # Task 2: Regression\n",
    "        loss2 = F.mse_loss(outputs['regression'], targets['regression'])\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.task_weights[0] * loss1 + \n",
    "                     self.task_weights[1] * loss2)\n",
    "        \n",
    "        return total_loss, {'cls_loss': loss1.item(), 'reg_loss': loss2.item()}\n",
    "\n",
    "# Example usage\n",
    "outputs = {\n",
    "    'classification': torch.randn(8, 3),\n",
    "    'regression': torch.randn(8, 1)\n",
    "}\n",
    "targets = {\n",
    "    'classification': torch.randint(0, 3, (8,)),\n",
    "    'regression': torch.randn(8, 1)\n",
    "}\n",
    "\n",
    "mtl = MultiTaskLoss(task_weights=[1.0, 0.5])\n",
    "total_loss, individual_losses = mtl(outputs, targets)\n",
    "\n",
    "print(f\"\\nTotal Loss: {total_loss.item():.4f}\")\n",
    "print(f\"Classification Loss: {individual_losses['cls_loss']:.4f}\")\n",
    "print(f\"Regression Loss: {individual_losses['reg_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Custom Layers\n",
    "\n",
    "## Custom Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOM LAYER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    \"\"\"Custom implementation of Linear layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize weights (Xavier/Glorot)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / np.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # y = xW^T + b\n",
    "        output = x @ self.weight.t()\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "\n",
    "# Test\n",
    "custom = CustomLinear(10, 5)\n",
    "builtin = nn.Linear(10, 5)\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "out1 = custom(x)\n",
    "out2 = builtin(x)\n",
    "\n",
    "print(f\"\\nCustom output shape: {out1.shape}\")\n",
    "print(f\"Built-in output shape: {out2.shape}\")\n",
    "print(f\"\\nCustom parameters: {sum(p.numel() for p in custom.parameters())}\")\n",
    "print(f\"Built-in parameters: {sum(p.numel() for p in builtin.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ATTENTION LAYER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Simple self-attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = np.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        \n",
    "        Q = self.query(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.bmm(attn_weights, V)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test\n",
    "attn = SelfAttention(embed_dim=64)\n",
    "x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, embed=64\n",
    "\n",
    "output, weights = attn(x)\n",
    "print(f\"\\nInput: {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(f\"Attention weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Advanced Gradient Operations\n",
    "\n",
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT CLIPPING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randn(32, 1)\n",
    "\n",
    "# Forward\n",
    "outputs = model(x)\n",
    "loss = F.mse_loss(outputs, y)\n",
    "\n",
    "# Backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients before clipping\n",
    "total_norm_before = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_norm_before += p.grad.data.norm(2).item() ** 2\n",
    "total_norm_before = total_norm_before ** 0.5\n",
    "\n",
    "print(f\"\\nGradient norm before clipping: {total_norm_before:.4f}\")\n",
    "\n",
    "# Clip gradients\n",
    "max_norm = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "# Check after clipping\n",
    "total_norm_after = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_norm_after += p.grad.data.norm(2).item() ** 2\n",
    "total_norm_after = total_norm_after ** 0.5\n",
    "\n",
    "print(f\"Gradient norm after clipping: {total_norm_after:.4f}\")\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "GRADIENT CLIPPING:\n",
    "  - Prevents exploding gradients\n",
    "  - Essential for RNNs, LSTMs\n",
    "  - max_norm = 1.0 (typical)\n",
    "  \n",
    "USAGE:\n",
    "  loss.backward()\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "  optimizer.step()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT ACCUMULATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "GRADIENT ACCUMULATION:\n",
    "  - Simulate larger batch size\n",
    "  - Useful when GPU memory limited\n",
    "  - Accumulate gradients over N batches\n",
    "  - Then update weights\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "accumulation_steps = 4  # Effective batch = 32 * 4 = 128\n",
    "\n",
    "# Training loop\n",
    "for step in range(10):\n",
    "    # Dummy mini-batch\n",
    "    x = torch.randn(32, 10)\n",
    "    y = torch.randn(32, 1)\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Normalize loss (average over accumulation steps)\n",
    "    loss = loss / accumulation_steps\n",
    "    \n",
    "    # Backward (accumulate gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights every N steps\n",
    "    if (step + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Step {step+1}: Updated weights (effective batch=128)\")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "BENEFITS:\n",
    "  ‚úÖ Larger effective batch size\n",
    "  ‚úÖ Same memory as small batch\n",
    "  ‚úÖ Better gradient estimates\n",
    "  \n",
    "TRADEOFF:\n",
    "  ‚ùå Slower (more forward passes)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Advanced LR Scheduling\n",
    "\n",
    "## Warmup + Cosine Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WARMUP + COSINE ANNEALING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"Warmup followed by cosine annealing\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step < self.warmup_steps:\n",
    "            # Linear warmup\n",
    "            lr = self.base_lr * self.current_step / self.warmup_steps\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "\n",
    "# Demo\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_steps=100, total_steps=1000)\n",
    "\n",
    "lrs = []\n",
    "for step in range(1000):\n",
    "    lr = scheduler.step()\n",
    "    lrs.append(lr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(lrs, linewidth=2)\n",
    "plt.axvline(x=100, color='r', linestyle='--', alpha=0.5, label='Warmup end')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Warmup + Cosine Annealing')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "WARMUP + COSINE:\n",
    "  - Warmup: Gradually increase LR (stabilize training)\n",
    "  - Cosine: Smooth decay\n",
    "  - Very effective for Transformers\n",
    "  - Used in BERT, GPT, etc.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Debugging Training\n",
    "\n",
    "## Monitor Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GRADIENT MONITORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "class GradientMonitor:\n",
    "    \"\"\"Monitor gradient statistics\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def check_gradients(self):\n",
    "        stats = {\n",
    "            'mean': [],\n",
    "            'std': [],\n",
    "            'max': [],\n",
    "            'min': [],\n",
    "            'norm': []\n",
    "        }\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad = param.grad.data\n",
    "                \n",
    "                stats['mean'].append(grad.mean().item())\n",
    "                stats['std'].append(grad.std().item())\n",
    "                stats['max'].append(grad.max().item())\n",
    "                stats['min'].append(grad.min().item())\n",
    "                stats['norm'].append(grad.norm().item())\n",
    "                \n",
    "                # Check for issues\n",
    "                if grad.norm().item() > 100:\n",
    "                    print(f\"‚ö†Ô∏è  Large gradient in {name}: {grad.norm().item():.2f}\")\n",
    "                if grad.norm().item() < 1e-7:\n",
    "                    print(f\"‚ö†Ô∏è  Vanishing gradient in {name}: {grad.norm().item():.2e}\")\n",
    "                if torch.isnan(grad).any():\n",
    "                    print(f\"‚ùå NaN gradient in {name}\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Test\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "monitor = GradientMonitor(model)\n",
    "\n",
    "# Dummy forward/backward\n",
    "x = torch.randn(32, 10)\n",
    "y = torch.randn(32, 1)\n",
    "loss = F.mse_loss(model(x), y)\n",
    "loss.backward()\n",
    "\n",
    "stats = monitor.check_gradients()\n",
    "\n",
    "print(f\"\\nGradient statistics:\")\n",
    "print(f\"  Mean: {np.mean(stats['mean']):.6f}\")\n",
    "print(f\"  Std: {np.mean(stats['std']):.6f}\")\n",
    "print(f\"  Max: {np.max(stats['max']):.6f}\")\n",
    "print(f\"  Min: {np.min(stats['min']):.6f}\")\n",
    "print(f\"  Norm: {np.mean(stats['norm']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WEIGHT INITIALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Custom weight initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Apply initialization\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(\"\\n‚úÖ Weights initialized\")\n",
    "\n",
    "# Check initialization\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"{name:20s}: mean={param.mean():.4f}, std={param.std():.4f}\")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "COMMON INITIALIZATIONS:\n",
    "\n",
    "1. Xavier/Glorot:\n",
    "   nn.init.xavier_uniform_(weight)\n",
    "   ‚Üí For tanh, sigmoid\n",
    "\n",
    "2. Kaiming/He:\n",
    "   nn.init.kaiming_normal_(weight, nonlinearity='relu')\n",
    "   ‚Üí For ReLU, LeakyReLU\n",
    "\n",
    "3. Orthogonal:\n",
    "   nn.init.orthogonal_(weight)\n",
    "   ‚Üí For RNNs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ T·ªïng K·∫øt FILE 3-A\n",
    "\n",
    "## B·∫°n ƒê√£ H·ªçc\n",
    "\n",
    "‚úÖ **Custom Loss**: Focal Loss, Multi-task Loss\n",
    "\n",
    "‚úÖ **Custom Layers**: Linear, Self-Attention\n",
    "\n",
    "‚úÖ **Gradient Operations**: Clipping, Accumulation\n",
    "\n",
    "‚úÖ **Advanced LR Scheduling**: Warmup + Cosine\n",
    "\n",
    "‚úÖ **Debugging**: Gradient monitoring, Weight init\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "```python\n",
    "# Complete training setup\n",
    "model = YourModel()\n",
    "model.apply(init_weights)  # Custom initialization\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_steps=100, total_steps=1000)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        # Forward\n",
    "        loss = model(batch)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Ti·∫øp Theo\n",
    "\n",
    "üìï **FILE 3-B: Transfer Learning & Mixed Precision**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
