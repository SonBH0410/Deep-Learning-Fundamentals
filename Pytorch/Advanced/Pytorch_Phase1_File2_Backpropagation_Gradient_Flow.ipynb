{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò PYTORCH PHASE 1 - FILE 2: BACKPROPAGATION & GRADIENT FLOW\n",
    "\n",
    "**Core Concepts:** Gradient Issues, Initialization, Gradient Flow Analysis\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- ‚úÖ Hi·ªÉu backpropagation mechanism\n",
    "- ‚úÖ Identify gradient issues (vanishing/exploding)\n",
    "- ‚úÖ Master initialization strategies\n",
    "- ‚úÖ Analyze gradient flow per layer\n",
    "- ‚úÖ Practical gradient monitoring\n",
    "\n",
    "**Th·ªùi l∆∞·ª£ng:** 2-3 tu·∫ßn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö M·ª•c L·ª•c\n",
    "\n",
    "### 1. BACKPROPAGATION BASICS\n",
    "1.1 Chain Rule Review\n",
    "1.2 Computational Graph\n",
    "1.3 Forward vs Backward Pass\n",
    "1.4 PyTorch Autograd\n",
    "\n",
    "### 2. GRADIENT ISSUES\n",
    "2.1 Vanishing Gradients\n",
    "2.2 Exploding Gradients\n",
    "2.3 Why Deep Networks Are Hard\n",
    "2.4 Role of Depth & Activation\n",
    "\n",
    "### 3. GRADIENT FLOW ANALYSIS\n",
    "3.1 Gradient Norm Per Layer\n",
    "3.2 Effect of Initialization\n",
    "3.3 Effect of Activation Functions\n",
    "3.4 Effect of Normalization\n",
    "\n",
    "### 4. INITIALIZATION STRATEGIES\n",
    "4.1 Zero Initialization (Why It Fails)\n",
    "4.2 Xavier/Glorot Initialization\n",
    "4.3 He Initialization\n",
    "4.4 Initialization vs Activation\n",
    "\n",
    "### 5. PRACTICAL EXPERIMENTS\n",
    "5.1 Track Gradient Norms\n",
    "5.2 Compare Initializations\n",
    "5.3 Deep Network Failure Modes\n",
    "5.4 Gradient Clipping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. BACKPROPAGATION BASICS\n",
    "\n",
    "## 1.1 Chain Rule Review\n",
    "\n",
    "### Fundamental Rule\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$$\n",
    "\n",
    "### Deep Network Example\n",
    "\n",
    "$$y = f_3(f_2(f_1(x)))$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial f_3}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial x}$$\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "Gradient = **Product** of local gradients\n",
    "\n",
    "‚Üí If any local gradient is small ‚Üí Overall gradient vanishes\n",
    "‚Üí If any local gradient is large ‚Üí Overall gradient explodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple backprop example\n",
    "\n",
    "# Forward: y = w3 * (w2 * (w1 * x))\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w1 = torch.tensor(0.5, requires_grad=True)\n",
    "w2 = torch.tensor(0.5, requires_grad=True)\n",
    "w3 = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z1 = w1 * x\n",
    "z2 = w2 * z1\n",
    "y = w3 * z2\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  z1 = w1 * x = {z1.item():.4f}\")\n",
    "print(f\"  z2 = w2 * z1 = {z2.item():.4f}\")\n",
    "print(f\"  y = w3 * z2 = {y.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "print(\"\\nBackward pass (gradients):\")\n",
    "print(f\"  dy/dw3 = {w3.grad.item():.4f}\")\n",
    "print(f\"  dy/dw2 = {w2.grad.item():.4f}\")\n",
    "print(f\"  dy/dw1 = {w1.grad.item():.4f}\")\n",
    "print(f\"  dy/dx = {x.grad.item():.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "print(\"\\nManual verification:\")\n",
    "print(f\"  dy/dw3 = z2 = {z2.item():.4f} ‚úì\")\n",
    "print(f\"  dy/dw2 = w3 * z1 = {(w3 * z1).item():.4f} ‚úì\")\n",
    "print(f\"  dy/dw1 = w3 * w2 * x = {(w3 * w2 * x).item():.4f} ‚úì\")\n",
    "print(f\"  dy/dx = w3 * w2 * w1 = {(w3 * w2 * w1).item():.4f} ‚úì\")\n",
    "\n",
    "print(\"\\nüí° Key: Gradient = Product of local derivatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Computational Graph\n",
    "\n",
    "### Forward Pass\n",
    "```\n",
    "x ‚Üí [Layer 1] ‚Üí h1 ‚Üí [Layer 2] ‚Üí h2 ‚Üí [Layer 3] ‚Üí y ‚Üí Loss\n",
    "```\n",
    "\n",
    "### Backward Pass\n",
    "```\n",
    "x ‚Üê [Layer 1] ‚Üê h1 ‚Üê [Layer 2] ‚Üê h2 ‚Üê [Layer 3] ‚Üê y ‚Üê ‚àáLoss\n",
    "```\n",
    "\n",
    "### PyTorch Autograd\n",
    "\n",
    "- Automatically builds computational graph\n",
    "- Tracks operations with `requires_grad=True`\n",
    "- `.backward()` computes all gradients\n",
    "- `.grad` stores gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. GRADIENT ISSUES\n",
    "\n",
    "## 2.1 Vanishing Gradients\n",
    "\n",
    "### Problem\n",
    "\n",
    "Trong deep networks:\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\cdot \\prod_{i=2}^{n} \\frac{\\partial h_i}{\\partial h_{i-1}}$$\n",
    "\n",
    "If $\\frac{\\partial h_i}{\\partial h_{i-1}} < 1$ ‚Üí Product becomes very small\n",
    "\n",
    "### Causes\n",
    "\n",
    "1. **Sigmoid/Tanh activations**\n",
    "   - Derivative max = 0.25 (sigmoid)\n",
    "   - With 10 layers: $0.25^{10} \\approx 10^{-6}$\n",
    "\n",
    "2. **Poor initialization**\n",
    "   - Weights too small ‚Üí activations saturate\n",
    "\n",
    "3. **Deep networks**\n",
    "   - More layers ‚Üí more multiplications ‚Üí smaller gradient\n",
    "\n",
    "### Consequences\n",
    "- Early layers don't learn\n",
    "- Training extremely slow\n",
    "- Network behaves like shallow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients\n",
    "\n",
    "class DeepSigmoidNet(nn.Module):\n",
    "    \"\"\"Deep network with sigmoid (prone to vanishing gradients)\"\"\"\n",
    "    def __init__(self, depth=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            layers.append(nn.Linear(100, 100))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        layers.append(nn.Linear(100, 10))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model and compute gradients\n",
    "model = DeepSigmoidNet(depth=10)\n",
    "x = torch.randn(32, 100)\n",
    "y = torch.randint(0, 10, (32,))\n",
    "\n",
    "# Forward + backward\n",
    "output = model(x)\n",
    "loss = F.cross_entropy(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Collect gradient norms per layer\n",
    "layer_names = []\n",
    "grad_norms = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and param.grad is not None:\n",
    "        layer_names.append(name)\n",
    "        grad_norms.append(param.grad.norm().item())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(range(len(grad_norms)), grad_norms)\n",
    "plt.xlabel('Layer Index (0 = first layer)', fontsize=12)\n",
    "plt.ylabel('Gradient Norm', fontsize=12)\n",
    "plt.title('Vanishing Gradients in Deep Sigmoid Network', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(f\"   First layer gradient: {grad_norms[0]:.2e}\")\n",
    "print(f\"   Last layer gradient: {grad_norms[-1]:.2e}\")\n",
    "print(f\"   Ratio: {grad_norms[0]/grad_norms[-1]:.2e}\")\n",
    "print(\"\\n‚ö†Ô∏è  Early layers have MUCH smaller gradients!\")\n",
    "print(\"   ‚Üí Vanishing gradient problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploding Gradients\n",
    "\n",
    "### Problem\n",
    "\n",
    "If $\\frac{\\partial h_i}{\\partial h_{i-1}} > 1$ ‚Üí Product becomes very large\n",
    "\n",
    "### Causes\n",
    "\n",
    "1. **Poor initialization**\n",
    "   - Weights too large\n",
    "\n",
    "2. **Recurrent networks**\n",
    "   - Same weight matrix multiplied many times\n",
    "\n",
    "3. **No normalization**\n",
    "   - Activations grow unbounded\n",
    "\n",
    "### Consequences\n",
    "- Loss becomes NaN\n",
    "- Weights become NaN\n",
    "- Training diverges\n",
    "\n",
    "### Solutions\n",
    "- **Gradient clipping**: Limit gradient magnitude\n",
    "- **Proper initialization**: He/Xavier\n",
    "- **Batch normalization**: Normalize activations\n",
    "- **Residual connections**: Skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate exploding gradients\n",
    "\n",
    "class PoorlyInitializedNet(nn.Module):\n",
    "    \"\"\"Network with large initialization (prone to exploding)\"\"\"\n",
    "    def __init__(self, depth=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            linear = nn.Linear(100, 100)\n",
    "            # BAD: Initialize with large weights\n",
    "            nn.init.uniform_(linear.weight, -2, 2)\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(100, 10))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "model = PoorlyInitializedNet(depth=5)\n",
    "x = torch.randn(32, 100)\n",
    "y = torch.randint(0, 10, (32,))\n",
    "\n",
    "# Forward + backward\n",
    "output = model(x)\n",
    "loss = F.cross_entropy(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Collect gradient norms\n",
    "grad_norms = []\n",
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_norms.append(param.grad.norm().item())\n",
    "\n",
    "print(\"‚ö†Ô∏è  Exploding Gradients:\")\n",
    "print(f\"   Max gradient norm: {max(grad_norms):.2e}\")\n",
    "print(f\"   Min gradient norm: {min(grad_norms):.2e}\")\n",
    "print(f\"   Range: {max(grad_norms)/min(grad_norms):.2e}\")\n",
    "\n",
    "if max(grad_norms) > 1e3:\n",
    "    print(\"\\n‚ùå DANGER: Gradients exploding!\")\n",
    "    print(\"   Solution: Gradient clipping or better initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gradient Clipping\n",
    "\n",
    "### Norm Clipping\n",
    "\n",
    "$$\\text{if } ||g|| > \\text{threshold}: \\quad g = \\frac{g}{||g||} \\times \\text{threshold}$$\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping example\n",
    "\n",
    "model = PoorlyInitializedNet(depth=5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training step WITH gradient clipping\n",
    "output = model(x)\n",
    "loss = F.cross_entropy(output, y)\n",
    "loss.backward()\n",
    "\n",
    "# Gradient norms BEFORE clipping\n",
    "total_norm_before = 0.0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm_before += param_norm.item() ** 2\n",
    "total_norm_before = total_norm_before ** 0.5\n",
    "\n",
    "# CLIP gradients\n",
    "max_norm = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "# Gradient norms AFTER clipping\n",
    "total_norm_after = 0.0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm_after += param_norm.item() ** 2\n",
    "total_norm_after = total_norm_after ** 0.5\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"‚úÇÔ∏è  Gradient Clipping:\")\n",
    "print(f\"   Before clipping: {total_norm_before:.4f}\")\n",
    "print(f\"   After clipping: {total_norm_after:.4f}\")\n",
    "print(f\"   Threshold: {max_norm}\")\n",
    "print(\"\\n‚úÖ Gradients clipped to prevent explosion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. INITIALIZATION STRATEGIES\n",
    "\n",
    "## 3.1 Why Initialization Matters\n",
    "\n",
    "### Goal\n",
    "\n",
    "Maintain **variance** of activations and gradients across layers\n",
    "\n",
    "### Bad Initialization\n",
    "- Activations too small ‚Üí vanishing gradients\n",
    "- Activations too large ‚Üí exploding gradients\n",
    "- All neurons do same thing ‚Üí symmetry\n",
    "\n",
    "## 3.2 Zero Initialization (WRONG)\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD\n",
    "nn.init.zeros_(layer.weight)\n",
    "```\n",
    "\n",
    "### Problems\n",
    "1. All neurons compute same function\n",
    "2. All gradients are identical\n",
    "3. No learning (symmetry breaking fails)\n",
    "\n",
    "## 3.3 Xavier/Glorot Initialization\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in} + n_{out}}\\right)$$\n",
    "\n",
    "Or uniform:\n",
    "$$W \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "### When to Use\n",
    "- **Tanh activation**\n",
    "- **Sigmoid activation**\n",
    "- Linear layers\n",
    "\n",
    "### Derivation\n",
    "Assumes activation function symmetric around zero (tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier initialization\n",
    "\n",
    "layer = nn.Linear(100, 100)\n",
    "\n",
    "# Xavier/Glorot init\n",
    "nn.init.xavier_uniform_(layer.weight)\n",
    "print(\"‚úÖ Xavier initialization\")\n",
    "print(f\"   Mean: {layer.weight.mean().item():.6f}\")\n",
    "print(f\"   Std: {layer.weight.std().item():.6f}\")\n",
    "print(f\"   Theoretical std: {(2/(100+100))**0.5:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 He Initialization\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)$$\n",
    "\n",
    "### When to Use\n",
    "- **ReLU activation**\n",
    "- **Leaky ReLU**\n",
    "- Any activation that \"kills\" half the neurons\n",
    "\n",
    "### Why Different?\n",
    "ReLU zeroes out half the activations ‚Üí need larger variance to compensate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Xavier vs He initialization\n",
    "\n",
    "def test_initialization(init_fn, activation, depth=10, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Test initialization by measuring activation variance across layers\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(depth):\n",
    "        linear = nn.Linear(100, 100, bias=False)\n",
    "        init_fn(linear.weight)\n",
    "        layers.append(linear)\n",
    "        layers.append(activation())\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    # Forward pass\n",
    "    x = torch.randn(n_samples, 100)\n",
    "    \n",
    "    # Track activation variance per layer\n",
    "    variances = []\n",
    "    with torch.no_grad():\n",
    "        for layer in model:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                variances.append(x.var().item())\n",
    "    \n",
    "    return variances\n",
    "\n",
    "# Test different combinations\n",
    "configs = [\n",
    "    ('Xavier + Tanh', nn.init.xavier_uniform_, nn.Tanh),\n",
    "    ('Xavier + ReLU', nn.init.xavier_uniform_, nn.ReLU),\n",
    "    ('He + ReLU', nn.init.kaiming_uniform_, nn.ReLU),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i, (name, init_fn, activation) in enumerate(configs):\n",
    "    variances = test_initialization(init_fn, activation, depth=10)\n",
    "    plt.plot(variances, marker='o', label=name, linewidth=2, markersize=6)\n",
    "\n",
    "plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Ideal (var=1)')\n",
    "plt.xlabel('Layer Index', fontsize=12)\n",
    "plt.ylabel('Activation Variance', fontsize=12)\n",
    "plt.title('Activation Variance Across Layers', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(\"   Xavier + Tanh: Variance stable (designed for tanh)\")\n",
    "print(\"   Xavier + ReLU: Variance decreases (not optimal for ReLU)\")\n",
    "print(\"   He + ReLU: Variance stable (designed for ReLU)\")\n",
    "print(\"\\nüí° Key: Match initialization to activation function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. GRADIENT FLOW ANALYSIS\n",
    "\n",
    "## 4.1 Monitor Gradient Norms Per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete gradient monitoring system\n",
    "\n",
    "class GradientMonitor:\n",
    "    \"\"\"\n",
    "    Monitor gradient flow during training\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradient_norms = {}\n",
    "        \n",
    "    def record_gradients(self):\n",
    "        \"\"\"Record gradient norms for all layers\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if name not in self.gradient_norms:\n",
    "                    self.gradient_norms[name] = []\n",
    "                \n",
    "                norm = param.grad.norm().item()\n",
    "                self.gradient_norms[name].append(norm)\n",
    "    \n",
    "    def plot_gradient_flow(self, iteration=None):\n",
    "        \"\"\"Plot gradient norms per layer\"\"\"\n",
    "        if iteration is None:\n",
    "            iteration = -1  # Last iteration\n",
    "        \n",
    "        layer_names = []\n",
    "        grad_norms = []\n",
    "        \n",
    "        for name, norms in self.gradient_norms.items():\n",
    "            if 'weight' in name:\n",
    "                layer_names.append(name.split('.')[0])\n",
    "                grad_norms.append(norms[iteration])\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(range(len(grad_norms)), grad_norms)\n",
    "        plt.xlabel('Layer', fontsize=12)\n",
    "        plt.ylabel('Gradient Norm', fontsize=12)\n",
    "        plt.title(f'Gradient Flow (Iteration {iteration})', fontsize=14, fontweight='bold')\n",
    "        plt.yscale('log')\n",
    "        plt.xticks(range(len(layer_names)), layer_names, rotation=45)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_gradient_history(self, layer_name=None):\n",
    "        \"\"\"Plot gradient history over time\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        if layer_name:\n",
    "            # Plot specific layer\n",
    "            norms = self.gradient_norms[layer_name]\n",
    "            plt.plot(norms, linewidth=2)\n",
    "            plt.title(f'Gradient History: {layer_name}', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            # Plot all layers\n",
    "            for name, norms in self.gradient_norms.items():\n",
    "                if 'weight' in name:\n",
    "                    plt.plot(norms, alpha=0.7, label=name.split('.')[0])\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.title('Gradient History (All Layers)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.xlabel('Iteration', fontsize=12)\n",
    "        plt.ylabel('Gradient Norm', fontsize=12)\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ GradientMonitor class defined!\")\n",
    "print(\"\\nüìä Usage:\")\n",
    "print(\"   monitor = GradientMonitor(model)\")\n",
    "print(\"   # In training loop:\")\n",
    "print(\"   loss.backward()\")\n",
    "print(\"   monitor.record_gradients()\")\n",
    "print(\"   optimizer.step()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compare Different Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Compare gradient flow in different architectures\n",
    "\n",
    "class PlainDeepNet(nn.Module):\n",
    "    \"\"\"Plain deep network (prone to gradient issues)\"\"\"\n",
    "    def __init__(self, depth=20):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(100, 100)]\n",
    "        for _ in range(depth-1):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(100, 100))\n",
    "        layers.append(nn.Linear(100, 10))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with skip connection\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim, dim)\n",
    "        self.linear2 = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.linear1(x))\n",
    "        out = self.linear2(out)\n",
    "        out += identity  # Skip connection!\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"Deep residual network\"\"\"\n",
    "    def __init__(self, depth=10):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(100, 100)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(100) for _ in range(depth)])\n",
    "        self.output_layer = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Compare gradient flow\n",
    "models = {\n",
    "    'Plain (20 layers)': PlainDeepNet(depth=20),\n",
    "    'ResNet (10 blocks)': ResNet(depth=10)\n",
    "}\n",
    "\n",
    "x = torch.randn(32, 100)\n",
    "y = torch.randint(0, 10, (32,))\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    # Forward + backward\n",
    "    output = model(x)\n",
    "    loss = F.cross_entropy(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradients\n",
    "    grad_norms = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None and len(param.shape) > 1:  # Only weight matrices\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, 2, idx+1)\n",
    "    plt.bar(range(len(grad_norms)), grad_norms)\n",
    "    plt.xlabel('Layer Index', fontsize=12)\n",
    "    plt.ylabel('Gradient Norm', fontsize=12)\n",
    "    plt.title(name, fontsize=13, fontweight='bold')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(\"   Plain Network: Gradient vanishes in early layers\")\n",
    "print(\"   ResNet: Gradients flow better (thanks to skip connections)\")\n",
    "print(\"\\n‚úÖ Skip connections help gradient flow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì T·ªïng k·∫øt FILE 2: Backpropagation & Gradient Flow\n",
    "\n",
    "## ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "\n",
    "### 1. Backpropagation Basics\n",
    "- **Chain rule**: Gradient = product of local derivatives\n",
    "- **Computational graph**: Forward and backward pass\n",
    "- **PyTorch autograd**: Automatic differentiation\n",
    "\n",
    "### 2. Gradient Issues\n",
    "- **Vanishing gradients**: Product of small derivatives\n",
    "- **Exploding gradients**: Product of large derivatives\n",
    "- **Causes**: Activation functions, initialization, depth\n",
    "- **Solutions**: Proper init, normalization, skip connections\n",
    "\n",
    "### 3. Initialization Strategies\n",
    "- **Zero init**: NEVER use (breaks symmetry)\n",
    "- **Xavier/Glorot**: For tanh/sigmoid\n",
    "- **He initialization**: For ReLU\n",
    "- **Match init to activation function**\n",
    "\n",
    "### 4. Gradient Flow Analysis\n",
    "- **Monitor gradient norms**: Essential for debugging\n",
    "- **Layer-wise analysis**: Identify problematic layers\n",
    "- **Skip connections**: Help gradient flow\n",
    "- **Gradient clipping**: Prevent explosion\n",
    "\n",
    "## üöÄ Key Takeaways\n",
    "\n",
    "1. **Gradient = product** ‚Üí Prone to vanishing/exploding\n",
    "2. **Sigmoid/tanh** ‚Üí Vanishing gradients\n",
    "3. **ReLU** ‚Üí Better gradient flow\n",
    "4. **Proper initialization** crucial for deep networks\n",
    "5. **He init for ReLU**, Xavier for tanh\n",
    "6. **Skip connections** solve gradient flow issues\n",
    "7. **Monitor gradients** during training\n",
    "8. **Gradient clipping** for RNNs and unstable training\n",
    "\n",
    "## üìù Next: FILE 3\n",
    "\n",
    "- Regularization Techniques\n",
    "- Dropout\n",
    "- Weight Decay\n",
    "- Label Smoothing\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c m·ª´ng b·∫°n ƒë√£ ho√†n th√†nh FILE 2! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
