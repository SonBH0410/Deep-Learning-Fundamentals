{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìó Phase 2 ‚Äì Representation: 6Ô∏è‚É£ Activation Functions\n",
    "\n",
    "## Gi·∫£ng vi√™n: Deep Learning v·ªõi PyTorch\n",
    "\n",
    "### üéØ V·ª™A ƒê·ª¶ ‚Äì KH√îNG SA ƒê√Ä\n",
    "\n",
    "---\n",
    "\n",
    "## M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Sau khi ho√†n th√†nh notebook n√†y, b·∫°n s·∫Ω:\n",
    "- ‚úÖ Hi·ªÉu **vai tr√≤** c·ªßa non-linearity trong neural networks\n",
    "- ‚úÖ N·∫Øm v·ªØng c√°c activation functions ph·ªï bi·∫øn: **Sigmoid, Tanh, ReLU, Leaky ReLU, GELU, Swish**\n",
    "- ‚úÖ Hi·ªÉu **Dead ReLU problem** v√† c√°ch gi·∫£i quy·∫øt\n",
    "- ‚úÖ Hi·ªÉu t·∫°i sao **GELU** ƒë∆∞·ª£c d√πng trong Transformers\n",
    "- ‚úÖ So s√°nh **gradient flow** gi·ªØa c√°c activations\n",
    "- ‚úÖ Th·ª±c h√†nh experiments ƒë·ªÉ ch·ªçn activation ph√π h·ª£p\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.1 Activation Overview\n",
    "\n",
    "### üéØ Vai tr√≤ c·ªßa Non-linearity\n",
    "\n",
    "**Activation function** l√† h√†m phi tuy·∫øn ƒë∆∞·ª£c √°p d·ª•ng sau m·ªói linear transformation:\n",
    "\n",
    "$$\n",
    "h = \\sigma(Wx + b)\n",
    "$$\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- $W, b$: weights v√† bias (linear)\n",
    "- $\\sigma$: activation function (**non-linear**)\n",
    "\n",
    "### ‚ö†Ô∏è Linear Networks Limitation\n",
    "\n",
    "**N·∫øu kh√¥ng c√≥ activation (ho·∫∑c d√πng linear activation)**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1 &= W_1 x \\\\\n",
    "h_2 &= W_2 h_1 = W_2 W_1 x \\\\\n",
    "h_3 &= W_3 h_2 = W_3 W_2 W_1 x = W_{combined} x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**: \n",
    "- Nhi·ªÅu layers = 1 linear transformation duy nh·∫•t!\n",
    "- Kh√¥ng th·ªÉ h·ªçc **non-linear patterns**\n",
    "- Network s√¢u = v√¥ d·ª•ng!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Linear network kh√¥ng th·ªÉ h·ªçc XOR\n",
    "def demonstrate_linear_limitation():\n",
    "    \"\"\"\n",
    "    Demonstrate r·∫±ng linear network kh√¥ng th·ªÉ h·ªçc XOR problem\n",
    "    \"\"\"\n",
    "    # XOR dataset\n",
    "    X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "    y = torch.tensor([0., 1., 1., 0.])  # XOR output\n",
    "    \n",
    "    # Linear network (NO activation)\n",
    "    linear_model = nn.Sequential(\n",
    "        nn.Linear(2, 10),\n",
    "        nn.Linear(10, 10),\n",
    "        nn.Linear(10, 1)\n",
    "    )\n",
    "    \n",
    "    # Non-linear network (WITH ReLU)\n",
    "    nonlinear_model = nn.Sequential(\n",
    "        nn.Linear(2, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10, 1)\n",
    "    )\n",
    "    \n",
    "    # Training function\n",
    "    def train(model, X, y, epochs=1000):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X).squeeze()\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    # Train both\n",
    "    print(\"üèãÔ∏è  Training Linear vs Non-linear networks on XOR...\\n\")\n",
    "    linear_losses = train(linear_model, X, y)\n",
    "    nonlinear_losses = train(nonlinear_model, X, y)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('üéØ XOR Problem: Linear vs Non-linear', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0].plot(linear_losses, label='Linear (NO activation)', color='red', linewidth=2)\n",
    "    axes[0].plot(nonlinear_losses, label='Non-linear (ReLU)', color='green', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Predictions\n",
    "    with torch.no_grad():\n",
    "        linear_pred = linear_model(X).squeeze()\n",
    "        nonlinear_pred = nonlinear_model(X).squeeze()\n",
    "    \n",
    "    x_labels = ['(0,0)', '(0,1)', '(1,0)', '(1,1)']\n",
    "    x_pos = np.arange(len(x_labels))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[1].bar(x_pos - width, y.numpy(), width, label='True', color='blue', alpha=0.7)\n",
    "    axes[1].bar(x_pos, linear_pred.numpy(), width, label='Linear', color='red', alpha=0.7)\n",
    "    axes[1].bar(x_pos + width, nonlinear_pred.numpy(), width, label='Non-linear', color='green', alpha=0.7)\n",
    "    axes[1].set_xlabel('Input')\n",
    "    axes[1].set_ylabel('Output')\n",
    "    axes[1].set_title('Predictions')\n",
    "    axes[1].set_xticks(x_pos)\n",
    "    axes[1].set_xticklabels(x_labels)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Final Loss:\")\n",
    "    print(f\"   Linear: {linear_losses[-1]:.6f}\")\n",
    "    print(f\"   Non-linear: {nonlinear_losses[-1]:.6f}\")\n",
    "    print(\"\\n‚úÖ Non-linearity l√† THI·∫æT Y·∫æU ƒë·ªÉ h·ªçc complex patterns!\")\n",
    "\n",
    "demonstrate_linear_limitation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.2 Common Activation Functions\n",
    "\n",
    "### üìê Sigmoid\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm**:\n",
    "- Output: $(0, 1)$\n",
    "- Smooth, differentiable\n",
    "- D√πng cho binary classification (output layer)\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**:\n",
    "- ‚ö†Ô∏è **Vanishing gradient**: gradient ‚Üí 0 khi $|x|$ l·ªõn\n",
    "- ‚ö†Ô∏è Not zero-centered\n",
    "- ‚ö†Ô∏è Computationally expensive (exp)\n",
    "\n",
    "### üìê Tanh\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) - 1\n",
    "$$\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm**:\n",
    "- Output: $(-1, 1)$\n",
    "- Zero-centered (t·ªët h∆°n Sigmoid)\n",
    "- D√πng trong RNN, LSTM\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**:\n",
    "- ‚ö†Ô∏è V·∫´n c√≥ **vanishing gradient**\n",
    "- ‚ö†Ô∏è Expensive computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Sigmoid v√† Tanh\n",
    "def plot_activation_and_gradient(activation_fn, name, x_range=(-5, 5)):\n",
    "    \"\"\"\n",
    "    Plot activation function v√† gradient c·ªßa n√≥\n",
    "    \"\"\"\n",
    "    x = torch.linspace(x_range[0], x_range[1], 1000, requires_grad=True)\n",
    "    y = activation_fn(x)\n",
    "    \n",
    "    # Compute gradient\n",
    "    y.sum().backward()\n",
    "    grad = x.grad\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    fig.suptitle(f'{name} Activation Function', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Activation\n",
    "    ax1.plot(x.detach().numpy(), y.detach().numpy(), linewidth=2, color='blue')\n",
    "    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel(f'{name}(x)')\n",
    "    ax1.set_title('Activation Function')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient\n",
    "    ax2.plot(x.detach().numpy(), grad.numpy(), linewidth=2, color='red')\n",
    "    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel(f\"d{name}/dx\")\n",
    "    ax2.set_title('Gradient')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Sigmoid & Tanh Visualization\\n\")\n",
    "\n",
    "# Sigmoid\n",
    "plot_activation_and_gradient(torch.sigmoid, 'Sigmoid')\n",
    "print(\"‚ö†Ô∏è  Sigmoid gradient ‚Üí 0 khi |x| > 5 (Vanishing Gradient!)\\n\")\n",
    "\n",
    "# Tanh\n",
    "plot_activation_and_gradient(torch.tanh, 'Tanh')\n",
    "print(\"‚ö†Ô∏è  Tanh c≈©ng c√≥ vanishing gradient problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê ReLU (Rectified Linear Unit)\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**:\n",
    "- ‚úÖ Kh√¥ng c√≥ vanishing gradient (·ªü $x > 0$)\n",
    "- ‚úÖ Computationally efficient\n",
    "- ‚úÖ Sparse activation (nhi·ªÅu neurons = 0)\n",
    "- ‚úÖ **ƒê∆∞·ª£c d√πng r·ªông r√£i nh·∫•t!**\n",
    "\n",
    "**Nh∆∞·ª£c ƒëi·ªÉm**:\n",
    "- ‚ö†Ô∏è **Dead ReLU**: neurons c√≥ th·ªÉ \"ch·∫øt\" (output = 0 m√£i m√£i)\n",
    "- ‚ö†Ô∏è Not zero-centered\n",
    "- ‚ö†Ô∏è Unbounded (c√≥ th·ªÉ explode)\n",
    "\n",
    "### üìê Leaky ReLU\n",
    "\n",
    "$$\n",
    "\\text{LeakyReLU}(x) = \\max(\\alpha x, x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Th∆∞·ªùng $\\alpha = 0.01$ ho·∫∑c $0.1$\n",
    "\n",
    "**∆Øu ƒëi·ªÉm**:\n",
    "- ‚úÖ Gi·∫£i quy·∫øt Dead ReLU problem\n",
    "- ‚úÖ V·∫´n efficient\n",
    "\n",
    "**Variants**:\n",
    "- **PReLU** (Parametric ReLU): $\\alpha$ l√† learnable parameter\n",
    "- **ELU** (Exponential Linear Unit): smooth ·ªü negative region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: ReLU family\n",
    "def plot_relu_family():\n",
    "    \"\"\"\n",
    "    Compare ReLU variants\n",
    "    \"\"\"\n",
    "    x = torch.linspace(-3, 3, 1000)\n",
    "    \n",
    "    relu = F.relu(x)\n",
    "    leaky_relu = F.leaky_relu(x, negative_slope=0.1)\n",
    "    elu = F.elu(x)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('üî• ReLU Family', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Activations\n",
    "    axes[0].plot(x.numpy(), relu.numpy(), label='ReLU', linewidth=2)\n",
    "    axes[0].plot(x.numpy(), leaky_relu.numpy(), label='Leaky ReLU (Œ±=0.1)', linewidth=2)\n",
    "    axes[0].plot(x.numpy(), elu.numpy(), label='ELU', linewidth=2)\n",
    "    axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('f(x)')\n",
    "    axes[0].set_title('Activation Functions')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradients\n",
    "    x_grad = torch.linspace(-3, 3, 1000, requires_grad=True)\n",
    "    \n",
    "    for act_fn, name, color in [(F.relu, 'ReLU', 'C0'), \n",
    "                                  (lambda x: F.leaky_relu(x, 0.1), 'Leaky ReLU', 'C1'),\n",
    "                                  (F.elu, 'ELU', 'C2')]:\n",
    "        x_grad.grad = None\n",
    "        y = act_fn(x_grad)\n",
    "        y.sum().backward()\n",
    "        axes[1].plot(x_grad.detach().numpy(), x_grad.grad.numpy(), \n",
    "                    label=name, linewidth=2, color=color)\n",
    "    \n",
    "    axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylabel('df/dx')\n",
    "    axes[1].set_title('Gradients')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(-0.5, 1.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_relu_family()\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"   - ReLU: gradient = 0 when x < 0 (Dead ReLU risk)\")\n",
    "print(\"   - Leaky ReLU: small gradient when x < 0 (solves Dead ReLU)\")\n",
    "print(\"   - ELU: smooth, better gradient flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê GELU (Gaussian Error Linear Unit)\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "Trong ƒë√≥ $\\Phi(x)$ l√† CDF c·ªßa Gaussian distribution.\n",
    "\n",
    "**Approximation** (faster):\n",
    "$$\n",
    "\\text{GELU}(x) \\approx 0.5x\\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right]\\right)\n",
    "$$\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm**:\n",
    "- ‚úÖ Smooth (differentiable everywhere)\n",
    "- ‚úÖ Non-monotonic (c√≥ curvature)\n",
    "- ‚úÖ **ƒê∆∞·ª£c d√πng trong BERT, GPT, Transformers**\n",
    "- ‚úÖ Better gradient properties than ReLU\n",
    "\n",
    "### üìê Swish (SiLU)\n",
    "\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**ƒê·∫∑c ƒëi·ªÉm**:\n",
    "- ‚úÖ Self-gated\n",
    "- ‚úÖ Smooth, non-monotonic\n",
    "- ‚úÖ Discovered by Google AutoML\n",
    "- ‚úÖ T·ªët cho deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: GELU v√† Swish\n",
    "def plot_modern_activations():\n",
    "    \"\"\"\n",
    "    Visualize GELU v√† Swish\n",
    "    \"\"\"\n",
    "    x = torch.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Compute activations\n",
    "    relu = F.relu(x)\n",
    "    gelu = F.gelu(x)\n",
    "    silu = F.silu(x)  # Swish\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('üöÄ Modern Activation Functions', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Activation comparison\n",
    "    axes[0, 0].plot(x.numpy(), relu.numpy(), label='ReLU', linewidth=2, linestyle='--')\n",
    "    axes[0, 0].plot(x.numpy(), gelu.numpy(), label='GELU', linewidth=2)\n",
    "    axes[0, 0].plot(x.numpy(), silu.numpy(), label='Swish/SiLU', linewidth=2)\n",
    "    axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('f(x)')\n",
    "    axes[0, 0].set_title('Activation Functions')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradients\n",
    "    x_grad = torch.linspace(-5, 5, 1000, requires_grad=True)\n",
    "    \n",
    "    for act_fn, name, color in [(F.relu, 'ReLU', 'C0'), \n",
    "                                  (F.gelu, 'GELU', 'C1'),\n",
    "                                  (F.silu, 'Swish', 'C2')]:\n",
    "        x_grad.grad = None\n",
    "        y = act_fn(x_grad)\n",
    "        y.sum().backward()\n",
    "        axes[0, 1].plot(x_grad.detach().numpy(), x_grad.grad.numpy(), \n",
    "                       label=name, linewidth=2, color=color)\n",
    "    \n",
    "    axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[0, 1].set_xlabel('x')\n",
    "    axes[0, 1].set_ylabel('df/dx')\n",
    "    axes[0, 1].set_title('Gradients')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Zoom near zero for GELU\n",
    "    x_zoom = torch.linspace(-2, 2, 1000)\n",
    "    gelu_zoom = F.gelu(x_zoom)\n",
    "    relu_zoom = F.relu(x_zoom)\n",
    "    axes[1, 0].plot(x_zoom.numpy(), relu_zoom.numpy(), label='ReLU', linewidth=2, linestyle='--')\n",
    "    axes[1, 0].plot(x_zoom.numpy(), gelu_zoom.numpy(), label='GELU', linewidth=2)\n",
    "    axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('f(x)')\n",
    "    axes[1, 0].set_title('Zoom: GELU vs ReLU near zero')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Negative values behavior\n",
    "    x_neg = torch.linspace(-5, 0, 1000)\n",
    "    axes[1, 1].plot(x_neg.numpy(), F.relu(x_neg).numpy(), label='ReLU', linewidth=2)\n",
    "    axes[1, 1].plot(x_neg.numpy(), F.gelu(x_neg).numpy(), label='GELU', linewidth=2)\n",
    "    axes[1, 1].plot(x_neg.numpy(), F.silu(x_neg).numpy(), label='Swish', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('f(x)')\n",
    "    axes[1, 1].set_title('Behavior at Negative Values')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_modern_activations()\n",
    "\n",
    "print(\"\\n‚ú® Key Insights:\")\n",
    "print(\"   - GELU: Smooth, non-zero gradient ·ªü negative region\")\n",
    "print(\"   - Swish: Self-gating, smooth everywhere\")\n",
    "print(\"   - C·∫£ 2 ƒë·ªÅu t·ªët h∆°n ReLU cho deep networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.3 Activation & Optimization\n",
    "\n",
    "### üíÄ Dead ReLU Problem\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a**: M·ªôt neuron \"ch·∫øt\" khi output = 0 for ALL inputs.\n",
    "\n",
    "**Nguy√™n nh√¢n**:\n",
    "1. Weights update khi·∫øn $Wx + b < 0$ lu√¥n\n",
    "2. Gradient = 0 ‚Üí kh√¥ng update ƒë∆∞·ª£c n·ªØa\n",
    "3. Neuron \"ch·∫øt\" vƒ©nh vi·ªÖn\n",
    "\n",
    "**Gi·∫£i ph√°p**:\n",
    "- ‚úÖ D√πng Leaky ReLU / PReLU / ELU\n",
    "- ‚úÖ Careful weight initialization\n",
    "- ‚úÖ Lower learning rate\n",
    "- ‚úÖ Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Dead ReLU problem\n",
    "def demonstrate_dead_relu():\n",
    "    \"\"\"\n",
    "    Demonstrate Dead ReLU problem\n",
    "    \"\"\"\n",
    "    # Create model with ReLU\n",
    "    torch.manual_seed(42)\n",
    "    model_relu = nn.Sequential(\n",
    "        nn.Linear(10, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 1)\n",
    "    )\n",
    "    \n",
    "    # Create model with Leaky ReLU\n",
    "    torch.manual_seed(42)\n",
    "    model_leaky = nn.Sequential(\n",
    "        nn.Linear(10, 100),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Linear(100, 100),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.Linear(100, 1)\n",
    "    )\n",
    "    \n",
    "    # Synthetic data\n",
    "    X = torch.randn(1000, 10)\n",
    "    y = torch.randn(1000, 1)\n",
    "    \n",
    "    # Training function that tracks dead neurons\n",
    "    def train_and_track_dead_neurons(model, X, y, epochs=50):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # High LR to trigger dead ReLU\n",
    "        criterion = nn.MSELoss()\n",
    "        dead_neurons_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Count dead neurons in hidden layers\n",
    "            with torch.no_grad():\n",
    "                activations = model[1](model[0](X))\n",
    "                dead = (activations.abs().sum(dim=0) == 0).sum().item()\n",
    "                dead_neurons_history.append(dead)\n",
    "        \n",
    "        return dead_neurons_history\n",
    "    \n",
    "    print(\"üî¨ Tracking Dead Neurons...\\n\")\n",
    "    dead_relu = train_and_track_dead_neurons(model_relu, X, y)\n",
    "    dead_leaky = train_and_track_dead_neurons(model_leaky, X, y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dead_relu, label='ReLU', linewidth=2, color='red')\n",
    "    plt.plot(dead_leaky, label='Leaky ReLU', linewidth=2, color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Number of Dead Neurons')\n",
    "    plt.title('üíÄ Dead ReLU Problem', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final dead neurons (ReLU): {dead_relu[-1]}/100\")\n",
    "    print(f\"Final dead neurons (Leaky ReLU): {dead_leaky[-1]}/100\")\n",
    "    print(\"\\n‚úÖ Leaky ReLU gi·∫£m ƒë√°ng k·ªÉ Dead Neuron problem!\")\n",
    "\n",
    "demonstrate_dead_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåä Smooth vs Non-smooth Activations\n",
    "\n",
    "**Non-smooth** (ReLU, Leaky ReLU):\n",
    "- Kh√¥ng differentiable t·∫°i x=0\n",
    "- Sharp transitions\n",
    "- Faster computation\n",
    "\n",
    "**Smooth** (GELU, Swish, Sigmoid, Tanh):\n",
    "- Differentiable everywhere\n",
    "- Better gradient flow\n",
    "- More stable optimization\n",
    "\n",
    "### ü§ñ T·∫°i sao GELU trong Transformers?\n",
    "\n",
    "1. **Smooth gradient**: Better optimization cho deep networks\n",
    "2. **Non-monotonic**: Richer expressivity\n",
    "3. **Probabilistic interpretation**: Gates inputs by their value\n",
    "4. **Empirically better**: Proven trong BERT, GPT\n",
    "\n",
    "**BERT/GPT architecture**:\n",
    "```python\n",
    "FFN = nn.Sequential(\n",
    "    nn.Linear(d_model, 4 * d_model),\n",
    "    nn.GELU(),  # ‚Üê GELU here!\n",
    "    nn.Linear(4 * d_model, d_model)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.4 Practical Experiments\n",
    "\n",
    "Ch√∫ng ta s·∫Ω compare c√°c activations tr√™n:\n",
    "- üéØ Convergence speed\n",
    "- üìä Final accuracy\n",
    "- üåä Gradient flow stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Dataset v√† model architecture\n",
    "def create_dataset(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Create a non-linear classification dataset\n",
    "    \"\"\"\n",
    "    X = torch.randn(n_samples, 20)\n",
    "    # Non-linear combination\n",
    "    y = ((X[:, :5].pow(2).sum(dim=1) > 5) & \n",
    "         (X[:, 5:10].sum(dim=1) > 0)).long()\n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP v·ªõi pluggable activation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, output_dim=2, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Activation selection\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid()\n",
    "        }\n",
    "        self.activation = activations[activation]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset()\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {len(dataset)} samples\")\n",
    "print(f\"‚úÖ Class distribution: {dataset[:][1].bincount()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function v·ªõi detailed tracking\n",
    "def train_with_tracking(model, dataloader, epochs=30, lr=0.001, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train v√† track multiple metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'grad_norm': [],\n",
    "        'activation_std': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        grad_norms = []\n",
    "        activation_stds = []\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward v·ªõi activation tracking\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Track gradient norm\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_norm += p.grad.data.norm(2).item() ** 2\n",
    "            grad_norms.append(total_norm ** 0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Stats\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(epoch_loss / len(dataloader))\n",
    "        history['accuracy'].append(100. * correct / total)\n",
    "        history['grad_norm'].append(np.mean(grad_norms))\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"üèãÔ∏è  Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Compare all activations\n",
    "print(\"üöÄ Running Comprehensive Activation Comparison...\\n\")\n",
    "\n",
    "activations_to_test = ['relu', 'leaky_relu', 'gelu', 'silu', 'tanh', 'sigmoid']\n",
    "results = {}\n",
    "\n",
    "for act_name in tqdm(activations_to_test, desc=\"Training models\"):\n",
    "    model = FlexibleMLP(activation=act_name)\n",
    "    history = train_with_tracking(model, dataloader, epochs=30, device=device)\n",
    "    results[act_name] = history\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìä Comprehensive Activation Function Comparison', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5']\n",
    "\n",
    "# Loss curves\n",
    "for (act_name, history), color in zip(results.items(), colors):\n",
    "    axes[0, 0].plot(history['loss'], label=act_name.upper(), \n",
    "                   linewidth=2, color=color)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Accuracy curves\n",
    "for (act_name, history), color in zip(results.items(), colors):\n",
    "    axes[0, 1].plot(history['accuracy'], label=act_name.upper(),\n",
    "                   linewidth=2, color=color)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Training Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "for (act_name, history), color in zip(results.items(), colors):\n",
    "    axes[1, 0].plot(history['grad_norm'], label=act_name.upper(),\n",
    "                   linewidth=2, color=color, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Gradient Norm')\n",
    "axes[1, 0].set_title('Gradient Flow Stability')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# Final performance comparison\n",
    "final_accs = [results[act]['accuracy'][-1] for act in activations_to_test]\n",
    "bars = axes[1, 1].bar(range(len(activations_to_test)), final_accs, color=colors)\n",
    "axes[1, 1].set_xticks(range(len(activations_to_test)))\n",
    "axes[1, 1].set_xticklabels([a.upper() for a in activations_to_test], rotation=45)\n",
    "axes[1, 1].set_ylabel('Final Accuracy (%)')\n",
    "axes[1, 1].set_title('Final Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, final_accs):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{acc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nüìä Final Performance Summary\\n\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Activation':<15} {'Final Acc (%)':<15} {'Final Loss':<15} {'Avg Grad Norm'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for act_name in activations_to_test:\n",
    "    history = results[act_name]\n",
    "    final_acc = history['accuracy'][-1]\n",
    "    final_loss = history['loss'][-1]\n",
    "    avg_grad = np.mean(history['grad_norm'][-5:])  # Last 5 epochs\n",
    "    \n",
    "    print(f\"{act_name.upper():<15} {final_acc:<15.2f} {final_loss:<15.4f} {avg_grad:.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Find best\n",
    "best_act = max(activations_to_test, key=lambda a: results[a]['accuracy'][-1])\n",
    "print(f\"\\nüèÜ Best Activation: {best_act.upper()}\")\n",
    "print(f\"   Accuracy: {results[best_act]['accuracy'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö K·∫øt lu·∫≠n & Best Practices\n",
    "\n",
    "### üéØ Khi n√†o d√πng activation n√†o?\n",
    "\n",
    "| Use Case | Recommended | L√Ω do |\n",
    "|----------|-------------|-------|\n",
    "| **CNN (Computer Vision)** | ReLU / Leaky ReLU | Fast, proven, works well |\n",
    "| **Transformers / NLP** | GELU | Smooth, better gradient, proven in BERT/GPT |\n",
    "| **RNN / LSTM** | Tanh | Traditional choice, works well |\n",
    "| **Deep Networks (>50 layers)** | GELU / Swish | Better gradient flow |\n",
    "| **Binary Classification Output** | Sigmoid | Output in (0,1) |\n",
    "| **Multi-class Output** | Softmax | Probability distribution |\n",
    "| **Regression Output** | None (Linear) | No constraint needed |\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "1. **Default Choice**: Start v·ªõi **ReLU** (CNN) ho·∫∑c **GELU** (Transformers)\n",
    "2. **Dead ReLU**: N·∫øu g·∫∑p ‚Üí switch sang **Leaky ReLU** ho·∫∑c **ELU**\n",
    "3. **Very Deep Networks**: Th·ª≠ **GELU** ho·∫∑c **Swish**\n",
    "4. **Gradient Issues**: Avoid Sigmoid/Tanh trong hidden layers\n",
    "5. **Combination**: C√≥ th·ªÉ d√πng activations kh√°c nhau ·ªü layers kh√°c nhau\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistakes:\n",
    "\n",
    "- ‚ùå D√πng Sigmoid/Tanh cho hidden layers trong deep networks\n",
    "- ‚ùå Qu√™n ƒë·∫∑t activation sau Linear layers\n",
    "- ‚ùå D√πng activation ·ªü output layer khi kh√¥ng c·∫ßn\n",
    "- ‚ùå Kh√¥ng test multiple activations khi optimize\n",
    "\n",
    "### üî¨ Experiment Tips:\n",
    "\n",
    "1. Lu√¥n test **√≠t nh·∫•t 2-3 activations** kh√°c nhau\n",
    "2. Monitor **gradient norm** ƒë·ªÉ detect vanishing/exploding\n",
    "3. Check **dead neuron percentage** v·ªõi ReLU\n",
    "4. Compare **convergence speed**, kh√¥ng ch·ªâ final accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## üéì B√†i t·∫≠p th·ª±c h√†nh\n",
    "\n",
    "1. **Implement Mish activation** t·ª´ scratch: $\\text{Mish}(x) = x \\cdot \\tanh(\\ln(1 + e^x))$\n",
    "2. **Compare activations tr√™n CIFAR-10**: Test ReLU vs GELU tr√™n ResNet\n",
    "3. **Visualize activation distributions**: Plot histogram c·ªßa activations qua training\n",
    "4. **Dead neuron tracking**: Monitor percentage of dead neurons over time\n",
    "5. **Custom activation**: Design v√† test activation function c·ªßa b·∫°n!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "1. [GELU Paper](https://arxiv.org/abs/1606.08415) - Gaussian Error Linear Units\n",
    "2. [Swish Paper](https://arxiv.org/abs/1710.05941) - Searching for Activation Functions\n",
    "3. [ReLU Deep Dive](https://arxiv.org/abs/1803.08375) - Understanding ReLU\n",
    "4. [Activation Survey](https://arxiv.org/abs/2109.14545) - Comprehensive comparison\n",
    "\n",
    "---\n",
    "\n",
    "### üôè C·∫£m ∆°n b·∫°n ƒë√£ h·ªçc!\n",
    "\n",
    "**Normalization + Activation = Foundation c·ªßa Modern Deep Learning!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
