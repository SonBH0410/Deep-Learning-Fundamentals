{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò PYTORCH PHASE 1 - FILE 3: REGULARIZATION\n",
    "\n",
    "**Core Concepts:** Overfitting, Dropout, Weight Decay, Label Smoothing\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- ‚úÖ Hi·ªÉu overfitting & generalization\n",
    "- ‚úÖ Master Dropout technique\n",
    "- ‚úÖ Understand Weight Decay (L2 regularization)\n",
    "- ‚úÖ Apply Label Smoothing\n",
    "- ‚úÖ Practical regularization strategies\n",
    "\n",
    "**Th·ªùi l∆∞·ª£ng:** 2 tu·∫ßn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö M·ª•c L·ª•c\n",
    "\n",
    "### 1. OVERFITTING & GENERALIZATION\n",
    "1.1 Bias-Variance Tradeoff\n",
    "1.2 Train vs Validation Gap\n",
    "1.3 Implicit vs Explicit Regularization\n",
    "\n",
    "### 2. DROPOUT\n",
    "2.1 Dropout Intuition\n",
    "2.2 Train vs Test Behavior\n",
    "2.3 Dropout Rate Effects\n",
    "2.4 When Dropout Hurts\n",
    "\n",
    "### 3. WEIGHT DECAY\n",
    "3.1 L2 Regularization\n",
    "3.2 L2 vs Weight Decay in Adam\n",
    "3.3 Practical Tuning\n",
    "\n",
    "### 4. LABEL SMOOTHING\n",
    "4.1 Over-confidence Problem\n",
    "4.2 Effect on Calibration\n",
    "4.3 Accuracy vs Robustness\n",
    "\n",
    "### 5. PRACTICAL EXPERIMENTS\n",
    "5.1 Same Model With/Without Regularization\n",
    "5.2 Dropout Rate Sweep\n",
    "5.3 Weight Decay Sweep\n",
    "5.4 Label Smoothing Impact\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. OVERFITTING & GENERALIZATION\n",
    "\n",
    "## 1.1 Bias-Variance Tradeoff\n",
    "\n",
    "### Definitions\n",
    "\n",
    "**Bias**: Error from wrong assumptions\n",
    "- High bias = underfitting\n",
    "- Model too simple\n",
    "\n",
    "**Variance**: Error from sensitivity to training data\n",
    "- High variance = overfitting\n",
    "- Model too complex\n",
    "\n",
    "### Total Error\n",
    "\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "### Tradeoff\n",
    "\n",
    "| Model Complexity | Bias | Variance | Total Error |\n",
    "|------------------|------|----------|-------------|\n",
    "| Too simple | High | Low | High |\n",
    "| Just right | Low | Low | Low |\n",
    "| Too complex | Low | High | High |\n",
    "\n",
    "## 1.2 Train vs Validation Gap\n",
    "\n",
    "### Overfitting Signs\n",
    "\n",
    "- Train accuracy >> Val accuracy\n",
    "- Train loss << Val loss\n",
    "- Gap increases over time\n",
    "\n",
    "### Solutions\n",
    "\n",
    "1. **More data**\n",
    "2. **Data augmentation**\n",
    "3. **Regularization** (Dropout, Weight Decay)\n",
    "4. **Early stopping**\n",
    "5. **Simpler model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting\n",
    "\n",
    "# Create small dataset (prone to overfitting)\n",
    "torch.manual_seed(42)\n",
    "X_train = torch.randn(50, 10)\n",
    "y_train = torch.randint(0, 2, (50,))\n",
    "X_val = torch.randn(200, 10)\n",
    "y_val = torch.randint(0, 2, (200,))\n",
    "\n",
    "# Large model (prone to overfitting)\n",
    "class LargeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "model = LargeModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(200):\n",
    "    # Train\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train.to(device))\n",
    "    loss = criterion(outputs, y_train.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_acc = (outputs.argmax(1) == y_train.to(device)).float().mean().item()\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val.to(device))\n",
    "        val_loss = criterion(val_outputs, y_val.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_acc = (val_outputs.argmax(1) == y_val.to(device)).float().mean().item()\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Overfitting: Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(train_accs, label='Train Acc', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Overfitting: Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚ö†Ô∏è  OVERFITTING DETECTED:\")\n",
    "print(f\"   Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"   Train Acc: {train_accs[-1]:.2%}, Val Acc: {val_accs[-1]:.2%}\")\n",
    "print(f\"   Gap: {train_accs[-1] - val_accs[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. DROPOUT\n",
    "\n",
    "## 2.1 Dropout Intuition\n",
    "\n",
    "### What is Dropout?\n",
    "\n",
    "During training: **Randomly drop** (set to 0) some neurons with probability $p$\n",
    "\n",
    "$$\\text{output} = \\begin{cases}\n",
    "0 & \\text{with probability } p \\\\\n",
    "\\frac{x}{1-p} & \\text{with probability } 1-p\n",
    "\\end{cases}$$\n",
    "\n",
    "### Why it Works?\n",
    "\n",
    "1. **Model averaging**: Training ensemble of subnetworks\n",
    "2. **Prevents co-adaptation**: Neurons can't rely on specific others\n",
    "3. **Adds noise**: Acts as regularization\n",
    "\n",
    "### Train vs Test\n",
    "\n",
    "- **Training**: Dropout active, scale outputs by $\\frac{1}{1-p}$\n",
    "- **Testing**: Dropout OFF, use all neurons\n",
    "\n",
    "## 2.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout demonstration\n",
    "\n",
    "# Create model WITH dropout\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Test dropout behavior\n",
    "model = ModelWithDropout(dropout_rate=0.5)\n",
    "x = torch.randn(5, 10)\n",
    "\n",
    "# Training mode (dropout active)\n",
    "model.train()\n",
    "output_train1 = model(x)\n",
    "output_train2 = model(x)\n",
    "\n",
    "print(\"üé≤ Dropout Behavior:\")\n",
    "print(\"\\nüìä Training mode (dropout ACTIVE):\")\n",
    "print(f\"   Output 1: {output_train1[0].detach().numpy()}\")\n",
    "print(f\"   Output 2: {output_train2[0].detach().numpy()}\")\n",
    "print(f\"   Different? {not torch.allclose(output_train1, output_train2)}\")\n",
    "\n",
    "# Test mode (dropout inactive)\n",
    "model.eval()\n",
    "output_test1 = model(x)\n",
    "output_test2 = model(x)\n",
    "\n",
    "print(\"\\nüìä Test mode (dropout INACTIVE):\")\n",
    "print(f\"   Output 1: {output_test1[0].detach().numpy()}\")\n",
    "print(f\"   Output 2: {output_test2[0].detach().numpy()}\")\n",
    "print(f\"   Same? {torch.allclose(output_test1, output_test2)}\")\n",
    "\n",
    "print(\"\\nüí° Key points:\")\n",
    "print(\"   - Training: Outputs DIFFERENT (dropout active)\")\n",
    "print(\"   - Testing: Outputs SAME (dropout inactive)\")\n",
    "print(\"   - Always use model.train() / model.eval()!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dropout Rate Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different dropout rates\n",
    "\n",
    "def train_model_with_dropout(dropout_rate, epochs=100):\n",
    "    \"\"\"Train model v·ªõi dropout rate c·ª• th·ªÉ\"\"\"\n",
    "    model = ModelWithDropout(dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Test different dropout rates\n",
    "dropout_rates = [0.0, 0.2, 0.5, 0.8]\n",
    "results = {}\n",
    "\n",
    "print(\"üîÑ Training v·ªõi different dropout rates...\\n\")\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    train_losses, val_losses = train_model_with_dropout(rate)\n",
    "    results[rate] = {'train': train_losses, 'val': val_losses}\n",
    "    print(f\"‚úÖ Dropout={rate}: Val Loss={val_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    plt.plot(results[rate]['val'], label=f'Dropout={rate}', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.title('Effect of Dropout Rate', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"   Dropout=0.0: Overfits (no regularization)\")\n",
    "print(\"   Dropout=0.2-0.5: Good regularization\")\n",
    "print(\"   Dropout=0.8: Too much (underfits)\")\n",
    "print(\"\\nüí° Typical: 0.2-0.5 for hidden layers, 0.5 for input layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 When Dropout Hurts\n",
    "\n",
    "### Situations Where Dropout is BAD\n",
    "\n",
    "1. **Small datasets**: Not enough data to benefit\n",
    "2. **Already regularized**: BatchNorm + Dropout can hurt\n",
    "3. **Convolutional layers**: Spatial dropout better\n",
    "4. **Recurrent layers**: Use specific dropout variants\n",
    "5. **Output layer**: Never apply dropout here!\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Use dropout=0.2-0.5 on fully connected layers\n",
    "- Higher dropout on input layer (0.5)\n",
    "- Lower dropout on hidden layers (0.2-0.3)\n",
    "- Always `model.eval()` during inference\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Apply dropout to every layer\n",
    "- Use very high dropout (>0.7)\n",
    "- Forget to switch train/eval modes\n",
    "- Apply to output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. WEIGHT DECAY\n",
    "\n",
    "## 3.1 L2 Regularization\n",
    "\n",
    "### Formula\n",
    "\n",
    "Add penalty term to loss:\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{task}} + \\frac{\\lambda}{2} \\sum_i w_i^2$$\n",
    "\n",
    "### Gradient\n",
    "\n",
    "$$\\nabla L_{\\text{total}} = \\nabla L_{\\text{task}} + \\lambda w$$\n",
    "\n",
    "### Effect\n",
    "\n",
    "- Encourages smaller weights\n",
    "- Smoother decision boundaries\n",
    "- Prevents overfitting\n",
    "\n",
    "## 3.2 Weight Decay vs L2\n",
    "\n",
    "### In SGD: Same thing\n",
    "\n",
    "```python\n",
    "# L2 regularization\n",
    "loss = task_loss + lambda * (w ** 2).sum()\n",
    "\n",
    "# Weight decay\n",
    "optimizer = SGD(params, lr=lr, weight_decay=lambda)\n",
    "```\n",
    "\n",
    "### In Adam: DIFFERENT!\n",
    "\n",
    "- **L2**: Regularization term gets adapted by Adam\n",
    "- **Weight Decay**: Decoupled, not adapted\n",
    "\n",
    "‚Üí Use **AdamW** for proper weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weight decay values\n",
    "\n",
    "def train_with_weight_decay(weight_decay, epochs=100):\n",
    "    \"\"\"Train v·ªõi weight decay\"\"\"\n",
    "    model = LargeModel().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    return val_losses\n",
    "\n",
    "# Test different weight decay values\n",
    "weight_decays = [0.0, 0.001, 0.01, 0.1]\n",
    "results = {}\n",
    "\n",
    "print(\"üîÑ Training v·ªõi different weight decay...\\n\")\n",
    "\n",
    "for wd in weight_decays:\n",
    "    val_losses = train_with_weight_decay(wd)\n",
    "    results[wd] = val_losses\n",
    "    print(f\"‚úÖ Weight Decay={wd}: Val Loss={val_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for wd in weight_decays:\n",
    "    plt.plot(results[wd], label=f'WD={wd}', linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.title('Effect of Weight Decay', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"   WD=0.0: Overfits\")\n",
    "print(\"   WD=0.001-0.01: Good regularization\")\n",
    "print(\"   WD=0.1: Too strong (underfits)\")\n",
    "print(\"\\nüí° Typical: 0.0001-0.01 depending on model size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. LABEL SMOOTHING\n",
    "\n",
    "## 4.1 Over-confidence Problem\n",
    "\n",
    "### Standard Cross Entropy\n",
    "\n",
    "Targets: **Hard labels** (one-hot)\n",
    "$$y = [0, 0, 1, 0, 0, ...]$$\n",
    "\n",
    "Problem: Model becomes overconfident\n",
    "$$\\text{softmax}(\\text{logits}) = [0.001, 0.001, 0.997, 0.001, ...]$$\n",
    "\n",
    "### Label Smoothing\n",
    "\n",
    "Targets: **Soft labels**\n",
    "$$y_{\\text{smooth}} = (1-\\epsilon) \\cdot y + \\frac{\\epsilon}{K}$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon$: smoothing parameter (typically 0.1)\n",
    "- $K$: number of classes\n",
    "\n",
    "Example v·ªõi $\\epsilon=0.1, K=10$:\n",
    "$$y = [0.01, 0.01, 0.91, 0.01, ...]$$\n",
    "\n",
    "## 4.2 Benefits\n",
    "\n",
    "- ‚úÖ Better calibration\n",
    "- ‚úÖ More robust predictions\n",
    "- ‚úÖ Slight regularization effect\n",
    "- ‚úÖ Prevents overconfidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label smoothing implementation\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross entropy v·ªõi label smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs: Model predictions (logits)\n",
    "            targets: Ground truth labels (long tensor)\n",
    "        \"\"\"\n",
    "        n_classes = outputs.size(-1)\n",
    "        \n",
    "        # Convert targets to one-hot\n",
    "        one_hot = torch.zeros_like(outputs).scatter(1, targets.unsqueeze(1), 1)\n",
    "        \n",
    "        # Apply label smoothing\n",
    "        smooth_labels = one_hot * (1 - self.epsilon) + self.epsilon / n_classes\n",
    "        \n",
    "        # Compute loss\n",
    "        log_probs = F.log_softmax(outputs, dim=-1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=-1).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Compare standard vs label smoothing\n",
    "def train_with_label_smoothing(use_smoothing, epochs=100):\n",
    "    model = LargeModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    if use_smoothing:\n",
    "        criterion = LabelSmoothingCrossEntropy(epsilon=0.1)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.to(device))\n",
    "            val_loss = criterion(val_outputs, y_val.to(device))\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    return val_losses\n",
    "\n",
    "# Train both\n",
    "print(\"üîÑ Training with standard CE...\")\n",
    "losses_standard = train_with_label_smoothing(False)\n",
    "\n",
    "print(\"üîÑ Training with label smoothing...\")\n",
    "losses_smoothing = train_with_label_smoothing(True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(losses_standard, label='Standard CE', linewidth=2)\n",
    "plt.plot(losses_smoothing, label='Label Smoothing (Œµ=0.1)', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.title('Label Smoothing Effect', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(f\"   Standard CE: {losses_standard[-1]:.4f}\")\n",
    "print(f\"   Label Smoothing: {losses_smoothing[-1]:.4f}\")\n",
    "print(\"\\n‚úÖ Label smoothing provides slight regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì T·ªïng k·∫øt FILE 3: Regularization\n",
    "\n",
    "## ‚úÖ Nh·ªØng g√¨ ƒë√£ h·ªçc\n",
    "\n",
    "### 1. Overfitting & Generalization\n",
    "- **Bias-variance tradeoff**: Balance model complexity\n",
    "- **Train-val gap**: Sign of overfitting\n",
    "- **Solutions**: Regularization, more data, early stopping\n",
    "\n",
    "### 2. Dropout\n",
    "- **Mechanism**: Randomly drop neurons during training\n",
    "- **Benefits**: Model averaging, prevents co-adaptation\n",
    "- **Best rates**: 0.2-0.5 hidden, 0.5 input\n",
    "- **Critical**: Use `model.train()` and `model.eval()`\n",
    "\n",
    "### 3. Weight Decay\n",
    "- **L2 regularization**: Penalty on large weights\n",
    "- **Weight decay vs L2**: Different in Adam!\n",
    "- **Use AdamW**: Proper decoupled weight decay\n",
    "- **Typical values**: 0.0001-0.01\n",
    "\n",
    "### 4. Label Smoothing\n",
    "- **Problem**: Overconfidence\n",
    "- **Solution**: Soft labels with $\\epsilon=0.1$\n",
    "- **Benefits**: Better calibration, robustness\n",
    "\n",
    "## üöÄ Key Takeaways\n",
    "\n",
    "1. **Overfitting** = high variance problem\n",
    "2. **Dropout** effective regularizer (0.2-0.5)\n",
    "3. **Weight decay** complementary to dropout\n",
    "4. **AdamW** better than Adam + L2\n",
    "5. **Label smoothing** prevents overconfidence\n",
    "6. **Combine regularizations** for best results\n",
    "\n",
    "## üìù Next Files\n",
    "\n",
    "- FILE 4: Embedding\n",
    "- FILE 5: Normalization\n",
    "- FILE 6: Activation Functions\n",
    "\n",
    "---\n",
    "\n",
    "**Ch√∫c m·ª´ng b·∫°n ƒë√£ ho√†n th√†nh FILE 3! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
