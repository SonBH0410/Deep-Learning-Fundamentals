{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìó Phase 2 ‚Äì Representation: 5Ô∏è‚É£ Normalization\n",
    "\n",
    "## Gi·∫£ng vi√™n: Deep Learning v·ªõi PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "## M·ª•c ti√™u h·ªçc t·∫≠p\n",
    "\n",
    "Sau khi ho√†n th√†nh notebook n√†y, b·∫°n s·∫Ω:\n",
    "- ‚úÖ Hi·ªÉu **t·∫°i sao** normalization ho·∫°t ƒë·ªông (Internal Covariate Shift, gradient flow)\n",
    "- ‚úÖ N·∫Øm v·ªØng **Batch Normalization**: train vs inference, running statistics\n",
    "- ‚úÖ Hi·ªÉu **Layer Normalization** v√† t·∫°i sao n√≥ t·ªët cho Transformers\n",
    "- ‚úÖ Kh√°m ph√° **RMSNorm**: hi·ªáu qu·∫£ v√† trade-offs\n",
    "- ‚úÖ Th·ª±c h√†nh so s√°nh BN/LN/RMSNorm qua experiments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style cho ƒë·∫πp\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Seed ƒë·ªÉ reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.1 T·∫°i sao Normalization ho·∫°t ƒë·ªông?\n",
    "\n",
    "### üéØ Internal Covariate Shift (Quan ƒëi·ªÉm l·ªãch s·ª≠)\n",
    "\n",
    "**ƒê·ªãnh nghƒ©a**: S·ª± thay ƒë·ªïi ph√¢n ph·ªëi c·ªßa input trong m·ªói layer khi training.\n",
    "\n",
    "**V·∫•n ƒë·ªÅ**:\n",
    "- Layer sau ph·∫£i \"h·ªçc l·∫°i\" khi ph√¢n ph·ªëi input thay ƒë·ªïi\n",
    "- L√†m ch·∫≠m qu√° tr√¨nh training\n",
    "- C·∫ßn learning rate nh·ªè ƒë·ªÉ ·ªïn ƒë·ªãnh\n",
    "\n",
    "### üéØ Smoothing Optimization Landscape\n",
    "\n",
    "**Nghi√™n c·ª©u g·∫ßn ƒë√¢y** (Santurkar et al., 2018):\n",
    "- BatchNorm l√†m m∆∞·ª£t loss landscape\n",
    "- Gradient tr·ªü n√™n predictable h∆°n\n",
    "- Cho ph√©p learning rate l·ªõn h∆°n\n",
    "\n",
    "### üéØ Stabilizing Gradient Flow\n",
    "\n",
    "- NgƒÉn gradient vanishing/exploding\n",
    "- Gi·ªØ gradient trong kho·∫£ng h·ª£p l√Ω\n",
    "- Training s√¢u h∆°n, nhanh h∆°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: V·∫Ω ph√¢n ph·ªëi activation qua c√°c layer\n",
    "def visualize_activation_distribution(model, x, title=\"Activation Distribution\"):\n",
    "    \"\"\"\n",
    "    Visualize how activation distributions change across layers\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    # Forward pass v√† l∆∞u activations\n",
    "    with torch.no_grad():\n",
    "        a = x\n",
    "        for layer in model:\n",
    "            a = layer(a)\n",
    "            activations.append(a.cpu().numpy().flatten())\n",
    "    \n",
    "    # V·∫Ω histogram\n",
    "    fig, axes = plt.subplots(1, len(activations), figsize=(15, 3))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, (ax, act) in enumerate(zip(axes, activations)):\n",
    "        ax.hist(act, bins=50, alpha=0.7, color=f'C{idx}')\n",
    "        ax.set_title(f'Layer {idx+1}\\nŒº={act.mean():.3f}\\nœÉ={act.std():.3f}')\n",
    "        ax.set_xlabel('Activation value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demo: So s√°nh network v·ªõi v√† kh√¥ng c√≥ normalization\n",
    "print(\"üî¨ Demo: Internal Covariate Shift\\n\")\n",
    "\n",
    "# T·∫°o sample data\n",
    "x_sample = torch.randn(1000, 100)\n",
    "\n",
    "# Network KH√îNG c√≥ normalization\n",
    "model_no_norm = nn.Sequential(\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Network C√ì normalization\n",
    "model_with_norm = nn.Sequential(\n",
    "    nn.Linear(100, 100),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "visualize_activation_distribution(\n",
    "    [model_no_norm[0], model_no_norm[2], model_no_norm[4]], \n",
    "    x_sample,\n",
    "    \"‚ùå KH√îNG c√≥ Normalization - Ph√¢n ph·ªëi b·ªã shift\"\n",
    ")\n",
    "\n",
    "visualize_activation_distribution(\n",
    "    [model_with_norm[0:3], model_with_norm[3:6], model_with_norm[6:9]], \n",
    "    x_sample,\n",
    "    \"‚úÖ C√ì Normalization - Ph√¢n ph·ªëi ·ªïn ƒë·ªãnh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Nh·∫≠n x√©t:\n",
    "\n",
    "- **Kh√¥ng c√≥ Norm**: Activations b·ªã shift, variance thay ƒë·ªïi qua layers\n",
    "- **C√≥ Norm**: Activations ·ªïn ƒë·ªãnh, mean‚âà0, std‚âà1 ·ªü m·ªói layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.2 Batch Normalization (BN)\n",
    "\n",
    "### üìê C√¥ng th·ª©c to√°n h·ªçc\n",
    "\n",
    "**Training mode**:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_B &= \\frac{1}{m}\\sum_{i=1}^{m} x_i \\quad \\text{(mean c·ªßa batch)} \\\\\n",
    "\\sigma_B^2 &= \\frac{1}{m}\\sum_{i=1}^{m} (x_i - \\mu_B)^2 \\quad \\text{(variance c·ªßa batch)} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(normalize)} \\\\\n",
    "y_i &= \\gamma \\hat{x}_i + \\beta \\quad \\text{(scale & shift - learnable)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Inference mode**:\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_{running}}{\\sqrt{\\sigma_{running}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "### üîÑ Train vs Inference\n",
    "\n",
    "| Mode | Mean & Variance | C·∫≠p nh·∫≠t |\n",
    "|------|----------------|----------|\n",
    "| **Train** | T√≠nh t·ª´ batch hi·ªán t·∫°i | C·∫≠p nh·∫≠t running statistics |\n",
    "| **Inference** | D√πng running statistics | Kh√¥ng c·∫≠p nh·∫≠t |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: BatchNorm t·ª´ scratch ƒë·ªÉ hi·ªÉu r√µ mechanism\n",
    "class BatchNorm1dFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch Normalization implementation from scratch ƒë·ªÉ hi·ªÉu r√µ c∆° ch·∫ø\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "        \n",
    "        # Running statistics (kh√¥ng h·ªçc, ch·ªâ track)\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features)\n",
    "        \n",
    "        if self.training:\n",
    "            # TRAINING MODE: t√≠nh mean & var t·ª´ batch\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0, unbiased=False)\n",
    "            \n",
    "            # Normalize\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t running statistics\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + \\\n",
    "                                   self.momentum * batch_mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + \\\n",
    "                                  self.momentum * batch_var\n",
    "                self.num_batches_tracked += 1\n",
    "        else:\n",
    "            # INFERENCE MODE: d√πng running statistics\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "        \n",
    "        # Scale & shift\n",
    "        out = self.gamma * x_normalized + self.beta\n",
    "        return out\n",
    "\n",
    "print(\"üß™ Demo: BatchNorm Train vs Inference\\n\")\n",
    "\n",
    "# T·∫°o data\n",
    "x_train = torch.randn(32, 10) * 2 + 5  # mean‚âà5, std‚âà2\n",
    "\n",
    "# Initialize BN layer\n",
    "bn_custom = BatchNorm1dFromScratch(10)\n",
    "bn_pytorch = nn.BatchNorm1d(10)\n",
    "\n",
    "# Training mode\n",
    "bn_custom.train()\n",
    "bn_pytorch.train()\n",
    "out_custom_train = bn_custom(x_train)\n",
    "out_pytorch_train = bn_pytorch(x_train)\n",
    "\n",
    "print(\"üìä TRAINING MODE:\")\n",
    "print(f\"Input - Mean: {x_train.mean():.3f}, Std: {x_train.std():.3f}\")\n",
    "print(f\"Output (Custom) - Mean: {out_custom_train.mean():.3f}, Std: {out_custom_train.std():.3f}\")\n",
    "print(f\"Output (PyTorch) - Mean: {out_pytorch_train.mean():.3f}, Std: {out_pytorch_train.std():.3f}\")\n",
    "print(f\"Running Mean (Custom): {bn_custom.running_mean.mean():.3f}\")\n",
    "\n",
    "# Inference mode v·ªõi data kh√°c\n",
    "x_test = torch.randn(16, 10) * 3 + 10  # mean‚âà10, std‚âà3 (kh√°c h·∫≥n!)\n",
    "bn_custom.eval()\n",
    "bn_pytorch.eval()\n",
    "out_custom_test = bn_custom(x_test)\n",
    "out_pytorch_test = bn_pytorch(x_test)\n",
    "\n",
    "print(\"\\nüìä INFERENCE MODE:\")\n",
    "print(f\"Input - Mean: {x_test.mean():.3f}, Std: {x_test.std():.3f}\")\n",
    "print(f\"Output (Custom) - Mean: {out_custom_test.mean():.3f}, Std: {out_custom_test.std():.3f}\")\n",
    "print(f\"Output (PyTorch) - Mean: {out_pytorch_test.mean():.3f}, Std: {out_pytorch_test.std():.3f}\")\n",
    "print(f\"\\n‚úÖ Inference d√πng running stats ‚Üí output kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi test batch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Batch Size Dependency\n",
    "\n",
    "BatchNorm **ph·ª• thu·ªôc v√†o batch size**:\n",
    "- Batch nh·ªè ‚Üí statistics kh√¥ng ƒë√°ng tin\n",
    "- Batch size kh√°c nhau train/test ‚Üí v·∫•n ƒë·ªÅ!\n",
    "- Th∆∞·ªùng c·∫ßn batch size ‚â• 16-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: ·∫¢nh h∆∞·ªüng c·ªßa batch size l√™n BN\n",
    "def test_batch_size_effect(batch_sizes=[2, 8, 32, 128]):\n",
    "    \"\"\"\n",
    "    Test hi·ªáu ·ª©ng c·ªßa batch size l√™n BatchNorm\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    x_full = torch.randn(1000, 50)\n",
    "    \n",
    "    for bs in batch_sizes:\n",
    "        bn = nn.BatchNorm1d(50)\n",
    "        bn.train()\n",
    "        \n",
    "        # Process theo batch\n",
    "        outputs = []\n",
    "        for i in range(0, len(x_full), bs):\n",
    "            batch = x_full[i:i+bs]\n",
    "            if len(batch) == bs:  # Ch·ªâ l·∫•y batch ƒë·ªß size\n",
    "                out = bn(batch)\n",
    "                outputs.append(out)\n",
    "        \n",
    "        all_outputs = torch.cat(outputs)\n",
    "        results[bs] = {\n",
    "            'mean': all_outputs.mean().item(),\n",
    "            'std': all_outputs.std().item()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üî¨ Experiment: ·∫¢nh h∆∞·ªüng c·ªßa Batch Size\\n\")\n",
    "results = test_batch_size_effect()\n",
    "\n",
    "for bs, stats in results.items():\n",
    "    print(f\"Batch Size {bs:3d}: Mean={stats['mean']:6.3f}, Std={stats['std']:.3f}\")\n",
    "\n",
    "print(\"\\nüìå Nh·∫≠n x√©t: Batch size nh·ªè ‚Üí statistics kh√¥ng ·ªïn ƒë·ªãnh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è BatchNorm trong CNN vs MLP\n",
    "\n",
    "**CNN**: `BatchNorm2d`\n",
    "- Normalize theo (N, H, W) - gi·ªØ nguy√™n spatial structure\n",
    "- Input shape: `(N, C, H, W)`\n",
    "- Statistics shape: `(C,)` - m·ªói channel c√≥ ri√™ng mean/var\n",
    "\n",
    "**MLP**: `BatchNorm1d`\n",
    "- Normalize theo batch dimension\n",
    "- Input shape: `(N, features)`\n",
    "- Statistics shape: `(features,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: BatchNorm2d cho CNN\n",
    "print(\"üñºÔ∏è  Demo: BatchNorm2d trong CNN\\n\")\n",
    "\n",
    "# T·∫°o fake image data\n",
    "images = torch.randn(8, 3, 32, 32)  # (batch, channels, height, width)\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "\n",
    "# BatchNorm2d\n",
    "bn2d = nn.BatchNorm2d(3)  # 3 channels\n",
    "bn2d.train()\n",
    "output = bn2d(images)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nGamma shape (learnable): {bn2d.weight.shape}\")\n",
    "print(f\"Beta shape (learnable): {bn2d.bias.shape}\")\n",
    "print(f\"Running mean shape: {bn2d.running_mean.shape}\")\n",
    "print(f\"Running var shape: {bn2d.running_var.shape}\")\n",
    "\n",
    "# Verify normalization per channel\n",
    "print(\"\\nüìä Statistics per channel:\")\n",
    "for c in range(3):\n",
    "    channel_data = output[:, c, :, :]\n",
    "    print(f\"Channel {c}: Mean={channel_data.mean():.3f}, Std={channel_data.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.3 Layer Normalization (LN)\n",
    "\n",
    "### üìê C√¥ng th·ª©c\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &= \\frac{1}{D}\\sum_{i=1}^{D} x_i \\quad \\text{(mean per sample)} \\\\\n",
    "\\sigma^2 &= \\frac{1}{D}\\sum_{i=1}^{D} (x_i - \\mu)^2 \\quad \\text{(variance per sample)} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "y_i &= \\gamma \\hat{x}_i + \\beta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### üîë Kh√°c bi·ªát ch√≠nh v·ªõi BatchNorm:\n",
    "\n",
    "| Aspect | BatchNorm | LayerNorm |\n",
    "|--------|-----------|----------|\n",
    "| Normalize theo | **Batch** dimension | **Feature** dimension |\n",
    "| Ph·ª• thu·ªôc batch size | ‚úÖ C√≥ | ‚ùå Kh√¥ng |\n",
    "| Train = Inference | ‚ùå Kh√°c | ‚úÖ Gi·ªëng |\n",
    "| Running statistics | ‚úÖ C√≥ | ‚ùå Kh√¥ng |\n",
    "| T·ªët cho | CNN, MLP | **Transformers, RNN** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: BatchNorm vs LayerNorm\n",
    "def visualize_norm_difference():\n",
    "    \"\"\"\n",
    "    Visualize s·ª± kh√°c bi·ªát gi·ªØa BatchNorm v√† LayerNorm\n",
    "    \"\"\"\n",
    "    # T·∫°o data: (batch_size=4, features=6)\n",
    "    x = torch.tensor([\n",
    "        [1., 2., 3., 4., 5., 6.],\n",
    "        [2., 4., 6., 8., 10., 12.],\n",
    "        [3., 6., 9., 12., 15., 18.],\n",
    "        [4., 8., 12., 16., 20., 24.]\n",
    "    ])\n",
    "    \n",
    "    # Apply normalization\n",
    "    bn = nn.BatchNorm1d(6)\n",
    "    ln = nn.LayerNorm(6)\n",
    "    \n",
    "    bn.eval()\n",
    "    ln.eval()\n",
    "    \n",
    "    x_bn = bn(x)\n",
    "    x_ln = ln(x)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Original\n",
    "    im0 = axes[0].imshow(x.numpy(), cmap='viridis', aspect='auto')\n",
    "    axes[0].set_title('Original Data', fontweight='bold')\n",
    "    axes[0].set_xlabel('Features')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "    plt.colorbar(im0, ax=axes[0])\n",
    "    \n",
    "    # BatchNorm\n",
    "    im1 = axes[1].imshow(x_bn.detach().numpy(), cmap='viridis', aspect='auto')\n",
    "    axes[1].set_title('BatchNorm\\n(normalize theo c·ªôt)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Features')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "    plt.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    # LayerNorm\n",
    "    im2 = axes[2].imshow(x_ln.detach().numpy(), cmap='viridis', aspect='auto')\n",
    "    axes[2].set_title('LayerNorm\\n(normalize theo h√†ng)', fontweight='bold')\n",
    "    axes[2].set_xlabel('Features')\n",
    "    axes[2].set_ylabel('Samples')\n",
    "    plt.colorbar(im2, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"üìä Statistics Verification:\\n\")\n",
    "    print(\"BatchNorm - Mean per feature (should be ‚âà0):\")\n",
    "    print(x_bn.mean(dim=0).numpy().round(3))\n",
    "    print(\"\\nLayerNorm - Mean per sample (should be ‚âà0):\")\n",
    "    print(x_ln.mean(dim=1).numpy().round(3))\n",
    "\n",
    "print(\"üé® Visualization: BatchNorm vs LayerNorm\\n\")\n",
    "visualize_norm_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ T·∫°i sao LayerNorm t·ªët cho Transformers?\n",
    "\n",
    "1. **Sequence length kh√°c nhau**: M·ªói sample c√≥ th·ªÉ c√≥ length kh√°c\n",
    "2. **Batch size nh·ªè**: Transformers th∆∞·ªùng train v·ªõi batch nh·ªè (do memory)\n",
    "3. **Train = Inference**: Kh√¥ng c·∫ßn running statistics\n",
    "4. **Per-token normalization**: M·ªói token ƒë∆∞·ª£c normalize ƒë·ªôc l·∫≠p\n",
    "\n",
    "**Trong Transformer**:\n",
    "```python\n",
    "# Input shape: (batch, seq_len, d_model)\n",
    "# LayerNorm normalize theo d_model dimension\n",
    "ln = nn.LayerNorm(d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: LayerNorm trong Transformer-style architecture\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Transformer block v·ªõi LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, nhead=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention + residual + LayerNorm\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # FFN + residual + LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "print(\"ü§ñ Demo: LayerNorm trong Transformer\\n\")\n",
    "\n",
    "# T·∫°o fake sequence data v·ªõi varying lengths\n",
    "batch_size = 4\n",
    "seq_lengths = [10, 15, 8, 12]  # Kh√°c nhau!\n",
    "d_model = 512\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(seq_lengths)\n",
    "x = torch.randn(batch_size, max_len, d_model)\n",
    "\n",
    "# Create model\n",
    "model = SimpleTransformerBlock(d_model=d_model)\n",
    "model.eval()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\n‚úÖ LayerNorm ho·∫°t ƒë·ªông t·ªët v·ªõi sequence lengths kh√°c nhau!\")\n",
    "print(f\"‚úÖ Kh√¥ng c·∫ßn batch statistics ‚Üí inference ·ªïn ƒë·ªãnh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.4 RMSNorm (Root Mean Square Normalization)\n",
    "\n",
    "### üìê C√¥ng th·ª©c\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{RMS} &= \\sqrt{\\frac{1}{D}\\sum_{i=1}^{D} x_i^2} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i}{\\text{RMS} + \\epsilon} \\cdot \\gamma\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### üîë Kh√°c bi·ªát v·ªõi LayerNorm:\n",
    "\n",
    "| Aspect | LayerNorm | RMSNorm |\n",
    "|--------|-----------|----------|\n",
    "| Mean centering | ‚úÖ C√≥ (`x - Œº`) | ‚ùå **KH√îNG** |\n",
    "| Variance normalization | ‚úÖ C√≥ | ‚úÖ C√≥ (via RMS) |\n",
    "| Learnable bias Œ≤ | ‚úÖ C√≥ | ‚ùå Kh√¥ng |\n",
    "| Computation | 2 passes | **1 pass** |\n",
    "| Speed | Slower | **~15% faster** |\n",
    "\n",
    "### üí° T·∫°i sao b·ªè mean?\n",
    "\n",
    "- **Empirical finding**: Mean centering kh√¥ng quan tr·ªçng l·∫Øm!\n",
    "- **Efficiency**: Gi·∫£m computation\n",
    "- **ƒê∆∞·ª£c d√πng trong**: LLaMA, GPT-NeoX, T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: RMSNorm from scratch\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    Paper: https://arxiv.org/abs/1910.07467\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute RMS\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        \n",
    "        # Normalize and scale\n",
    "        x_normalized = x / rms\n",
    "        return self.gamma * x_normalized\n",
    "\n",
    "print(\"üî¨ Implementation: RMSNorm\\n\")\n",
    "\n",
    "# Test data\n",
    "x_test = torch.randn(4, 8, 512)  # (batch, seq, features)\n",
    "\n",
    "# Initialize\n",
    "ln = nn.LayerNorm(512)\n",
    "rms = RMSNorm(512)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    out_ln = ln(x_test)\n",
    "    out_rms = rms(x_test)\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"\\nLayerNorm output:\")\n",
    "print(f\"  Mean: {out_ln.mean():.6f} (should be ‚âà0)\")\n",
    "print(f\"  Std: {out_ln.std():.6f}\")\n",
    "print(f\"\\nRMSNorm output:\")\n",
    "print(f\"  Mean: {out_rms.mean():.6f} (NOT zero!)\")\n",
    "print(f\"  Std: {out_rms.std():.6f}\")\n",
    "print(f\"\\nüìå RMSNorm KH√îNG center v·ªÅ 0 ‚Üí faster computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Trade-offs\n",
    "\n",
    "**Advantages**:\n",
    "- ‚ö° Faster (~15% speedup)\n",
    "- üíæ Less memory (no mean tracking)\n",
    "- üéØ Simpler gradient computation\n",
    "\n",
    "**Potential Issues**:\n",
    "- ‚ö†Ô∏è Kh√¥ng center v·ªÅ 0 ‚Üí c√≥ th·ªÉ ·∫£nh h∆∞·ªüng m·ªôt s·ªë architectures\n",
    "- ‚ö†Ô∏è √çt ƒë∆∞·ª£c research h∆°n LayerNorm\n",
    "- ‚ö†Ô∏è Ph·∫£i tune hyperparameters l·∫°i khi switch t·ª´ LN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.5 Practical Experiments: So s√°nh BN / LN / RMSNorm\n",
    "\n",
    "Ch√∫ng ta s·∫Ω train c√πng 1 model v·ªõi 3 lo·∫°i normalization kh√°c nhau v√† so s√°nh:\n",
    "- ‚ö° Convergence speed\n",
    "- üìä Batch size sensitivity\n",
    "- üìà Gradient norm stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o synthetic dataset\n",
    "def create_classification_dataset(n_samples=10000, n_features=50, n_classes=10):\n",
    "    \"\"\"\n",
    "    T·∫°o synthetic classification dataset\n",
    "    \"\"\"\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    # T·∫°o labels v·ªõi some structure\n",
    "    W = torch.randn(n_features, n_classes)\n",
    "    logits = X @ W\n",
    "    y = logits.argmax(dim=1)\n",
    "    \n",
    "    return TensorDataset(X, y)\n",
    "\n",
    "# Define models v·ªõi c√°c normalization kh√°c nhau\n",
    "class MLPWithNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP v·ªõi pluggable normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=50, hidden_dim=128, output_dim=10, norm_type='bn'):\n",
    "        super().__init__()\n",
    "        self.norm_type = norm_type\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Normalization layers\n",
    "        if norm_type == 'bn':\n",
    "            self.norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        elif norm_type == 'ln':\n",
    "            self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "            self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        elif norm_type == 'rms':\n",
    "            self.norm1 = RMSNorm(hidden_dim)\n",
    "            self.norm2 = RMSNorm(hidden_dim)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.fc1(x)))\n",
    "        x = F.relu(self.norm2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"üèóÔ∏è  Setup: Creating models and dataset...\\n\")\n",
    "dataset = create_classification_dataset()\n",
    "print(f\"‚úÖ Dataset created: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function v·ªõi gradient tracking\n",
    "def train_model(model, dataloader, epochs=10, lr=0.001, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train model v√† track metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'grad_norm': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        grad_norms = []\n",
    "        \n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Track gradient norm\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    total_norm += p.grad.data.norm(2).item() ** 2\n",
    "            grad_norms.append(total_norm ** 0.5)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100. * correct / total\n",
    "        avg_grad_norm = np.mean(grad_norms)\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        history['grad_norm'].append(avg_grad_norm)\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"üéØ Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Convergence speed v·ªõi batch size = 32\n",
    "print(\"üöÄ Experiment 1: Convergence Speed\\n\")\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train c√°c models\n",
    "results = {}\n",
    "norm_types = ['none', 'bn', 'ln', 'rms']\n",
    "colors = ['gray', 'blue', 'green', 'red']\n",
    "\n",
    "for norm_type in tqdm(norm_types, desc=\"Training models\"):\n",
    "    model = MLPWithNorm(norm_type=norm_type)\n",
    "    history = train_model(model, dataloader, epochs=20, device=device)\n",
    "    results[norm_type] = history\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('üìä Convergence Comparison (Batch Size = 32)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss\n",
    "for norm_type, color in zip(norm_types, colors):\n",
    "    axes[0].plot(results[norm_type]['loss'], label=norm_type.upper(), \n",
    "                color=color, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "for norm_type, color in zip(norm_types, colors):\n",
    "    axes[1].plot(results[norm_type]['accuracy'], label=norm_type.upper(),\n",
    "                color=color, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm\n",
    "for norm_type, color in zip(norm_types, colors):\n",
    "    axes[2].plot(results[norm_type]['grad_norm'], label=norm_type.upper(),\n",
    "                color=color, linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Gradient Norm')\n",
    "axes[2].set_title('Gradient Stability')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Observations:\")\n",
    "print(\"   - NONE: Slower convergence, unstable gradients\")\n",
    "print(\"   - BN/LN/RMS: Faster convergence, stable gradients\")\n",
    "print(\"   - RMS: Slightly faster than LN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Batch size sensitivity\n",
    "print(\"üî¨ Experiment 2: Batch Size Sensitivity\\n\")\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "batch_results = {bs: {} for bs in batch_sizes}\n",
    "\n",
    "for bs in tqdm(batch_sizes, desc=\"Testing batch sizes\"):\n",
    "    dataloader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    for norm_type in ['bn', 'ln', 'rms']:\n",
    "        model = MLPWithNorm(norm_type=norm_type)\n",
    "        history = train_model(model, dataloader, epochs=10, device=device)\n",
    "        batch_results[bs][norm_type] = history['accuracy'][-1]  # Final accuracy\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('üìä Batch Size Sensitivity', fontsize=16, fontweight='bold')\n",
    "\n",
    "for norm_type, color in zip(['bn', 'ln', 'rms'], ['blue', 'green', 'red']):\n",
    "    accuracies = [batch_results[bs][norm_type] for bs in batch_sizes]\n",
    "    ax.plot(batch_sizes, accuracies, marker='o', label=norm_type.upper(),\n",
    "           color=color, linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Batch Size', fontsize=12)\n",
    "ax.set_ylabel('Final Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Effect of Batch Size on Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Observations:\")\n",
    "print(\"   - BN: Sensitive to batch size (worse with small batches)\")\n",
    "print(\"   - LN/RMS: More robust across batch sizes\")\n",
    "print(\"   - RMS: Consistently good performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö T·ªïng k·∫øt: Khi n√†o d√πng g√¨?\n",
    "\n",
    "| Use Case | Recommended | L√Ω do |\n",
    "|----------|-------------|-------|\n",
    "| **CNN (Computer Vision)** | BatchNorm2d | Spatial structure, batch statistics reliable |\n",
    "| **Transformers / NLP** | LayerNorm | Variable length, small batches |\n",
    "| **RNN / LSTM** | LayerNorm | Temporal data, sequence-to-sequence |\n",
    "| **Large Language Models** | RMSNorm | Efficiency, proven in LLaMA/GPT |\n",
    "| **Small batch training** | LayerNorm / RMSNorm | Kh√¥ng ph·ª• thu·ªôc batch |\n",
    "| **Inference optimization** | RMSNorm | Fastest, simplest |\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "1. **BatchNorm**:\n",
    "   - Batch size ‚â• 16-32\n",
    "   - `.train()` v√† `.eval()` quan tr·ªçng!\n",
    "   - C·∫©n th·∫≠n v·ªõi distributed training\n",
    "\n",
    "2. **LayerNorm**:\n",
    "   - Safe default cho most cases\n",
    "   - ƒê·∫∑c bi·ªát t·ªët cho Transformers\n",
    "   - Kh√¥ng c·∫ßn worry v·ªÅ batch size\n",
    "\n",
    "3. **RMSNorm**:\n",
    "   - Th·ª≠ khi c·∫ßn optimize speed\n",
    "   - Tune hyperparameters l·∫°i\n",
    "   - Monitor training stability\n",
    "\n",
    "---\n",
    "\n",
    "## üéì B√†i t·∫≠p th·ª±c h√†nh\n",
    "\n",
    "1. **Implement GroupNorm** t·ª´ scratch (normalize theo groups of channels)\n",
    "2. **Compare BN vs SyncBN** trong distributed training\n",
    "3. **Implement InstanceNorm** cho style transfer\n",
    "4. **Test normalization** v·ªõi different activation functions\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ References\n",
    "\n",
    "1. [Batch Normalization Paper](https://arxiv.org/abs/1502.03167)\n",
    "2. [Layer Normalization Paper](https://arxiv.org/abs/1607.06450)\n",
    "3. [RMSNorm Paper](https://arxiv.org/abs/1910.07467)\n",
    "4. [How Does Batch Normalization Help?](https://arxiv.org/abs/1805.11604)\n",
    "\n",
    "---\n",
    "\n",
    "### üôè C·∫£m ∆°n b·∫°n ƒë√£ h·ªçc!\n",
    "\n",
    "Next: **6Ô∏è‚É£ Activation Functions** üî•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
