{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù NLP Fundamentals & HuggingFace\n",
    "\n",
    "**M·ª•c ti√™u:** Essential NLP techniques v√† HuggingFace ecosystem\n",
    "\n",
    "**N·ªôi dung:**\n",
    "- Text preprocessing (NLTK, spaCy)\n",
    "- Tokenization methods\n",
    "- Text vectorization (TF-IDF, embeddings)\n",
    "- HuggingFace Transformers basics\n",
    "- Fine-tuning patterns\n",
    "- Common NLP tasks\n",
    "\n",
    "**Level:** Intermediate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (uncomment if needed)\n",
    "# !pip install transformers datasets tokenizers nltk spacy scikit-learn\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Text Preprocessing\n",
    "\n",
    "### Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Basic text cleaning\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example\n",
    "sample = \"Check out https://example.com! @user #NLP is AMAZING!!! üòä\"\n",
    "cleaned = clean_text(sample)\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "text = \"The cats are running faster than the dogs were running\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w not in stop_words]\n",
    "print(f\"No stopwords: {filtered}\")\n",
    "\n",
    "# Stemming (chop word endings)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in filtered]\n",
    "print(f\"Stemmed: {stemmed}\")\n",
    "\n",
    "# Lemmatization (find root form)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w, pos='v') for w in filtered]\n",
    "print(f\"Lemmatized: {lemmatized}\")\n",
    "\n",
    "print(\"\\nüí° Stemming vs Lemmatization:\")\n",
    "print(\"   Stemming: Fast, crude (running ‚Üí run)\")\n",
    "print(\"   Lemmatization: Slow, accurate (better ‚Üí good)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Text Vectorization\n",
    "\n",
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Machine learning is awesome\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Natural language processing uses machine learning\"\n",
    "]\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(f\"Vocabulary: {vectorizer.get_feature_names_out()}\")\n",
    "print(f\"\\nTF-IDF Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Matrix (dense):\\n{tfidf_matrix.toarray()}\")\n",
    "\n",
    "# Get top words for each document\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for i, doc in enumerate(docs):\n",
    "    scores = list(zip(feature_names, tfidf_matrix[i].toarray()[0]))\n",
    "    top_words = sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"\\nDoc {i} top words: {top_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. HuggingFace Transformers\n",
    "\n",
    "### Basic Pipeline API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"üîç Sentiment Analysis:\")\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this product! It's amazing!\")\n",
    "print(f\"  Result: {result}\")\n",
    "\n",
    "# Text Generation\n",
    "print(\"\\n‚úçÔ∏è  Text Generation:\")\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(\"Artificial intelligence will\", max_length=30, num_return_sequences=1)\n",
    "print(f\"  Generated: {result[0]['generated_text']}\")\n",
    "\n",
    "# Named Entity Recognition\n",
    "print(\"\\nüè∑Ô∏è  Named Entity Recognition:\")\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner(\"Elon Musk founded SpaceX in California.\")\n",
    "for entity in result:\n",
    "    print(f\"  {entity['word']}: {entity['entity_group']} (score: {entity['score']:.2f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline API = Quick inference without writing code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nOutput shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  (batch_size, sequence_length, hidden_size)\")\n",
    "\n",
    "# Extract [CLS] token embedding (sentence representation)\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "print(f\"\\n[CLS] embedding shape: {cls_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Tokenization Deep Dive\n",
    "\n",
    "### WordPiece vs BPE vs Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different tokenizers\n",
    "models = [\n",
    "    (\"bert-base-uncased\", \"WordPiece\"),\n",
    "    (\"gpt2\", \"BPE\"),\n",
    "    (\"albert-base-v2\", \"Unigram\")\n",
    "]\n",
    "\n",
    "text = \"unhappiness\"\n",
    "\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "for model_name, method in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"{method} ({model_name}):\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  IDs: {tokenizer.convert_tokens_to_ids(tokens)}\\n\")\n",
    "\n",
    "print(\"üí° Observations:\")\n",
    "print(\"   - Different methods split words differently\")\n",
    "print(\"   - Subword tokenization handles OOV words\")\n",
    "print(\"   - ## prefix (WordPiece) = continuation token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Special tokens:\")\n",
    "print(f\"  PAD: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
    "print(f\"  CLS: {tokenizer.cls_token} ({tokenizer.cls_token_id})\")\n",
    "print(f\"  SEP: {tokenizer.sep_token} ({tokenizer.sep_token_id})\")\n",
    "print(f\"  UNK: {tokenizer.unk_token} ({tokenizer.unk_token_id})\")\n",
    "print(f\"  MASK: {tokenizer.mask_token} ({tokenizer.mask_token_id})\")\n",
    "\n",
    "# Example with special tokens\n",
    "text = \"Hello world\"\n",
    "encoded = tokenizer(text, add_special_tokens=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "\n",
    "print(f\"\\nText: '{text}'\")\n",
    "print(f\"Tokens with special: {tokens}\")\n",
    "print(\"  [CLS] = Start of sequence\")\n",
    "print(\"  [SEP] = End of sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Fine-tuning Example\n",
    "\n",
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this movie!\",\n",
    "        \"Terrible experience, never again.\",\n",
    "        \"Absolutely fantastic product!\",\n",
    "        \"Worst purchase ever.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete! Ready to train:\")\n",
    "print(\"   trainer.train()\")\n",
    "print(\"\\nüí° Trainer API handles:\")\n",
    "print(\"   - Training loop\")\n",
    "print(\"   - Evaluation\")\n",
    "print(\"   - Checkpointing\")\n",
    "print(\"   - Logging\")\n",
    "print(\"   - Mixed precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Common NLP Tasks\n",
    "\n",
    "### Task Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_tasks = \"\"\"\n",
    "üéØ Common NLP Tasks:\n",
    "\n",
    "1. Text Classification\n",
    "   - Sentiment analysis\n",
    "   - Spam detection\n",
    "   - Topic classification\n",
    "   Model: BERT, DistilBERT, RoBERTa\n",
    "\n",
    "2. Named Entity Recognition (NER)\n",
    "   - Extract person, location, organization\n",
    "   Model: BERT, RoBERTa\n",
    "\n",
    "3. Question Answering\n",
    "   - Extract answer from context\n",
    "   Model: BERT, ALBERT, RoBERTa\n",
    "\n",
    "4. Text Generation\n",
    "   - Story generation, completion\n",
    "   Model: GPT-2, GPT-3, T5\n",
    "\n",
    "5. Summarization\n",
    "   - Abstractive or extractive\n",
    "   Model: BART, T5, Pegasus\n",
    "\n",
    "6. Translation\n",
    "   - Machine translation\n",
    "   Model: MarianMT, T5, mBART\n",
    "\n",
    "7. Embedding/Similarity\n",
    "   - Semantic search, clustering\n",
    "   Model: Sentence-BERT, SimCSE\n",
    "\"\"\"\n",
    "\n",
    "print(nlp_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_guide = \"\"\"\n",
    "üìä Model Selection:\n",
    "\n",
    "| Task | Fast & Light | Balanced | High Accuracy |\n",
    "|------|--------------|----------|---------------|\n",
    "| Classification | DistilBERT | BERT-base | RoBERTa-large |\n",
    "| NER | DistilBERT | BERT-base | RoBERTa-large |\n",
    "| QA | DistilBERT | BERT-base | ALBERT-xxlarge |\n",
    "| Generation | DistilGPT2 | GPT-2 | GPT-3 (API) |\n",
    "| Summarization | DistilBART | BART | PEGASUS-large |\n",
    "| Embeddings | MiniLM | SBERT | MPNet |\n",
    "\n",
    "üí° Trade-offs:\n",
    "   Speed: DistilBERT (40% faster) vs BERT\n",
    "   Size: TinyBERT (7.5MB) vs BERT-base (440MB)\n",
    "   Accuracy: Large models +2-3% vs base\n",
    "\"\"\"\n",
    "\n",
    "print(model_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices\n",
    "\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_practices = \"\"\"\n",
    "‚úÖ Text Preprocessing Best Practices:\n",
    "\n",
    "1. For Traditional ML (TF-IDF, CountVectorizer):\n",
    "   ‚úì Lowercase\n",
    "   ‚úì Remove punctuation\n",
    "   ‚úì Remove stopwords\n",
    "   ‚úì Stemming/Lemmatization\n",
    "\n",
    "2. For Transformers (BERT, GPT):\n",
    "   ‚úì Keep original text (case-sensitive models exist)\n",
    "   ‚úì Keep punctuation\n",
    "   ‚úó DON'T remove stopwords\n",
    "   ‚úó DON'T stem/lemmatize\n",
    "   ‚Üí Tokenizer handles it!\n",
    "\n",
    "3. Data Augmentation:\n",
    "   - Back-translation\n",
    "   - Synonym replacement\n",
    "   - Random insertion/deletion\n",
    "   - Paraphrasing\n",
    "\n",
    "4. Handling Imbalanced Data:\n",
    "   - Oversampling minority class\n",
    "   - Class weights in loss\n",
    "   - Focal loss\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### NLP Pipeline\n",
    "\n",
    "```python\n",
    "# Classical ML\n",
    "text ‚Üí clean ‚Üí tokenize ‚Üí vectorize (TF-IDF) ‚Üí ML model\n",
    "\n",
    "# Transformers\n",
    "text ‚Üí tokenizer ‚Üí model ‚Üí output\n",
    "```\n",
    "\n",
    "### HuggingFace Workflow\n",
    "\n",
    "```python\n",
    "# 1. Quick inference\n",
    "pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = pipeline(text)\n",
    "\n",
    "# 2. Custom model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 3. Fine-tuning\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### Essential Concepts\n",
    "\n",
    "1. **Tokenization**: Text ‚Üí IDs\n",
    "   - WordPiece (BERT)\n",
    "   - BPE (GPT-2)\n",
    "   - Handles OOV with subwords\n",
    "\n",
    "2. **Special Tokens**:\n",
    "   - [CLS]: Sentence representation\n",
    "   - [SEP]: Separator\n",
    "   - [PAD]: Padding\n",
    "   - [MASK]: Masked token\n",
    "\n",
    "3. **Transfer Learning**:\n",
    "   - Pretrained models learn language\n",
    "   - Fine-tune on task-specific data\n",
    "   - Much better than training from scratch\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Need | Use |\n",
    "|------|-----|\n",
    "| Quick inference | `pipeline()` |\n",
    "| Custom model | `AutoModel` |\n",
    "| Fine-tuning | `Trainer` API |\n",
    "| Feature extraction | Model embeddings |\n",
    "| Text generation | GPT-2, GPT-3 |\n",
    "| Classification | BERT, RoBERTa |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Series Complete!**\n",
    "\n",
    "B·∫°n ƒë√£ c√≥ ƒë·∫ßy ƒë·ªß fundamentals cho ML/DL:\n",
    "- ‚úÖ Libraries (NumPy, Pandas, Matplotlib, Scikit-learn, OpenCV)\n",
    "- ‚úÖ Deep Learning (PyTorch Advanced, Transformers)\n",
    "- ‚úÖ Computer Vision (timm)\n",
    "- ‚úÖ NLP (HuggingFace)\n",
    "- ‚úÖ Deployment (Docker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
